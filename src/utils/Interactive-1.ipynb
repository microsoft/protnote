{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to protein_functions_310 (Python 3.10.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import json\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gzip\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "import random\n",
    "import numpy as np\n",
    "import wget\n",
    "import hashlib\n",
    "import math\n",
    "import re\n",
    "import blosum as bl\n",
    "from typing import Union,List,Set,Literal\n",
    "import transformers\n",
    "from collections import OrderedDict\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from pynvml import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetMemoryInfo\n",
    "\n",
    "def log_gpu_memory_usage(logger, device_id):\n",
    "    # Initialize NVML and get handle for the device\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(device_id)\n",
    "\n",
    "    # Get memory information using NVML\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    total_memory = info.total\n",
    "    used_memory = info.used\n",
    "    memory_percent = used_memory / total_memory * 100\n",
    "\n",
    "    # Reset peak memory stats\n",
    "    torch.cuda.reset_peak_memory_stats(device_id)\n",
    "\n",
    "    # Log memory usage information\n",
    "    logger.info(\n",
    "        f\"GPU memory occupied: {used_memory // 1024 ** 2} MB ({memory_percent:.2f}% of total memory {total_memory // 1024 ** 2} MB). \"\n",
    "        f\"Device {device_id} [Name: {torch.cuda.get_device_name(device_id)}]\")\n",
    "\n",
    "\n",
    "\n",
    "def convert_float16_to_float32(df):\n",
    "    float16_cols = df.select_dtypes(include='float16').columns\n",
    "    df[float16_cols] = df[float16_cols].astype('float32')\n",
    "    return df\n",
    "\n",
    "\n",
    "def hash_alphanumeric_sequence_id(s: str):\n",
    "    return int(hashlib.md5(s.encode()).hexdigest(), 16)\n",
    "\n",
    "\n",
    "def read_fasta(data_path: str, sep=\" \"):\n",
    "    \"\"\"\n",
    "    Reads a FASTA file and returns a list of tuples containing sequences, ids, and labels.\n",
    "    \"\"\"\n",
    "    sequences_with_ids_and_labels = []\n",
    "\n",
    "    for record in SeqIO.parse(data_path, \"fasta\"):\n",
    "        sequence = str(record.seq)\n",
    "        components = record.description.split(sep)\n",
    "        # labels[0] contains the sequence ID, and the rest of the labels are GO terms. \n",
    "        sequence_id = components[0]\n",
    "        labels = components[1:]\n",
    "        \n",
    "        # Return a tuple of sequence, sequence_id, and labels\n",
    "        sequences_with_ids_and_labels.append((sequence, sequence_id, labels))\n",
    "    return sequences_with_ids_and_labels\n",
    "\n",
    "\n",
    "def read_yaml(data_path: str):\n",
    "    with open(data_path, \"r\") as file:\n",
    "        data = yaml.safe_load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_json(data_path: str):\n",
    "    with open(data_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_json(data, data_path: str):\n",
    "    with open(data_path, \"w\") as file:\n",
    "        json.dump(data, file)\n",
    "\n",
    "\n",
    "def get_vocab_mappings(vocabulary):\n",
    "    assert len(vocabulary) == len(set(vocabulary)\n",
    "                                  ), \"items in vocabulary must be unique\"\n",
    "    term2int = {term: idx for idx, term in enumerate(vocabulary)}\n",
    "    int2term = {idx: term for term, idx in term2int.items()}\n",
    "    return term2int, int2term\n",
    "\n",
    "def generate_vocabularies(file_path: str)->dict:\n",
    "    \"\"\"\n",
    "    Generate vocabularies based on the provided data path.\n",
    "    path must be .fasta file\n",
    "    \"\"\"\n",
    "    vocabs = {'amino_acid_vocab':set(),\n",
    "              'label_vocab':set(),\n",
    "              'sequence_id_vocab':set()\n",
    "            }\n",
    "    \n",
    "    if isinstance(file_path,str):\n",
    "        data = read_fasta(file_path)\n",
    "    else:\n",
    "        raise TypeError(\"File not supported, vocabularies can only be generated from .fasta files.\")\n",
    "\n",
    "    for sequence, sequence_id, labels in data:\n",
    "        vocabs['sequence_id_vocab'].add(sequence_id)\n",
    "        vocabs['label_vocab'].update(labels)\n",
    "        vocabs['amino_acid_vocab'].update(list(sequence))\n",
    "    \n",
    "    for vocab_type in vocabs.keys():\n",
    "        vocabs[vocab_type] = sorted(list(vocabs[vocab_type]))\n",
    " \n",
    "    return vocabs\n",
    "\n",
    "def save_to_pickle(item, file_path: str):\n",
    "    with open(file_path, \"wb\") as p:\n",
    "        pickle.dump(item, p)\n",
    "        \n",
    "\n",
    "def save_to_fasta(sequence_id_labels_tuples, output_file):\n",
    "    \"\"\"\n",
    "    Save a list of tuples in the form (sequence, [labels]) to a FASTA file.\n",
    "\n",
    "    :param sequence_label_tuples: List of tuples containing sequences and labels\n",
    "    :param output_file: Path to the output FASTA file\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for _, (sequence, id, labels,) in enumerate(sequence_id_labels_tuples):\n",
    "        # Create a description from labels, joined by space\n",
    "        description = \" \".join(labels)\n",
    "       \n",
    "        record = SeqRecord(Seq(sequence), id=id, description=description)\n",
    "        records.append(record)\n",
    "\n",
    "    # Write the SeqRecord objects to a FASTA file\n",
    "    with open(output_file, \"w\") as output_handle:\n",
    "        SeqIO.write(records, output_handle, \"fasta\")\n",
    "        print(\"Saved FASTA file to \" + output_file)\n",
    "\n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as p:\n",
    "        item = pickle.load(p)\n",
    "    return item\n",
    "\n",
    "\n",
    "def download_and_unzip(url, output_file):\n",
    "    \"\"\"\n",
    "    Download a file from a given link and unzip it.\n",
    "\n",
    "    Args:\n",
    "        link (str): The URL to download the file from.\n",
    "        filename (str): The absolute path to save the downloaded file.\n",
    "    \"\"\"\n",
    "    filename = output_file + '.gz'\n",
    "\n",
    "    # Download the file from the web\n",
    "    zip_name = wget.download(url)\n",
    "    \n",
    "    # Move the file to data/swissprot\n",
    "    os.rename(zip_name, filename)\n",
    "\n",
    "    # Unzip the downloaded file\n",
    "    with gzip.open(zip_name, 'rb') as f_in:\n",
    "        with open(output_file, 'wb') as f_out:\n",
    "            f_out.write(f_in.read())\n",
    "\n",
    "    print(\n",
    "        f\"File {output_file + '.gz'} has been downloaded and unzipped to {output_file}.\")\n",
    "\n",
    "\n",
    "def seed_everything(seed: int, device: str):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    transformers.set_seed(seed)\n",
    "\n",
    "\n",
    "def load_gz_json(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        with gzip.GzipFile(fileobj=f, mode=\"rb\") as gzip_file:\n",
    "            return json.load(gzip_file)\n",
    "\n",
    "\n",
    "def ensure_list(value):\n",
    "    # Case 1: If the value is already a list\n",
    "    if isinstance(value, list):\n",
    "        return value\n",
    "    # Case 2: If the value is NaN\n",
    "    elif value is math.nan or (isinstance(value, float) and math.isnan(value)):\n",
    "        return []\n",
    "    # Case 3: For all other cases (including strings)\n",
    "    else:\n",
    "        return [value]\n",
    "\n",
    "def remove_obsolete_from_string(text):\n",
    "    pattern= r'(?i)\\bobsolete\\.?\\s*'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "\n",
    "class Blossum62Mutations:\n",
    "    def __init__(self,amino_acid_vocabulary:Union[Set,List]=None):\n",
    "\n",
    "        if amino_acid_vocabulary is None:\n",
    "            self.amino_acid_vocabulary = set(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I',\n",
    "                             'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "        else:\n",
    "            self.amino_acid_vocabulary = set(amino_acid_vocabulary)\n",
    "\n",
    "        # Load the BLOSUM62 matrix and convert to defaultdict using dictionary comprehension\n",
    "        blosum62 = bl.BLOSUM(62)\n",
    "        self.blosum62 = defaultdict(dict, {aa1: {aa2: blosum62[aa1][aa2] for aa2 in blosum62.keys()} for aa1 in blosum62.keys()})\n",
    "\n",
    "    def get_aa_scores(self,amino_acid: str,mutation_type:Literal['conservative','non-conservative']):\n",
    "\n",
    "        # Get the substitutions for the amino acid, ensuring only amino acids within the vocabulary are considered\n",
    "        substitutions = self.blosum62[amino_acid]\n",
    "        multiplier = -1 if mutation_type=='non-conservative'else 1\n",
    "        substitutions = {aa: score*multiplier for aa, score in substitutions.items() if aa in self.amino_acid_vocabulary}\n",
    "        amino_acids, scores = zip(*substitutions.items())\n",
    "        return amino_acids, scores\n",
    "\n",
    "    def get_most_conservative_mutation():\n",
    "        pass\n",
    "    def get_most_non_conservative_mutation():\n",
    "        pass\n",
    "\n",
    "    def sample_aa(self, amino_acid: str,mutation_type:Literal['conservative','non-conservative']) -> str:\n",
    "        \"\"\"\n",
    "        Sample an amino acid based on the BLOSUM62 substitution matrix, favoring mutations based on mutation_type selected. \n",
    "        Args:\n",
    "            amino_acid (str): The amino acid to find a substitution for.\n",
    "        Returns:\n",
    "            str: The substituted amino acid.\n",
    "        \"\"\"\n",
    "\n",
    "        amino_acids, scores = self.get_aa_scores(amino_acid=amino_acid,\n",
    "                                                 mutation_type=mutation_type)\n",
    "        \n",
    "        # Use only non-negative scores\n",
    "        probabilities = [max(0, score) for score in scores]\n",
    "        total = sum(probabilities)\n",
    "        \n",
    "        # If all scores are negative, do not change the amino acid\n",
    "        if total == 0:\n",
    "            return amino_acid\n",
    "        else:\n",
    "            # Normalize the scores to sum to 1 and sample from the distribution\n",
    "            probabilities = [p / total for p in probabilities]\n",
    "            return random.choices(amino_acids, weights=probabilities, k=1)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
