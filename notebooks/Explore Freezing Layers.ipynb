{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "import loralib as lora\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# Load test dataset\n",
    "GO_ANNOTATIONS_PATH = \"/home/samirchar/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH)\n",
    "\n",
    "# Get first 1000 labels as a list\n",
    "text = go_annotations.iloc[:100, 0].tolist()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 47 MB.\n",
      "GPU memory occupied: 2290 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "import torch\n",
    "\n",
    "print_gpu_utilization()\n",
    "checkpoint = 'microsoft/biogpt'\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint,\n",
    "    # torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print_gpu_utilization()\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"do_eval\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "# Tokenize the go_annotations list\n",
    "tokenized_data = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Create random labels for the new dataset\n",
    "random_labels = np.random.randint(0, 1, (len(text)))\n",
    "\n",
    "# Create the dataset from the tokenized data and random labels\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"], \n",
    "    \"labels\": random_labels\n",
    "})\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "dummy_dataset.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def apply_lora_biogpt_attention(layer,rank,device,in_features= 1024, out_features= 1024):\n",
    "    layer.self_attn.q_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank)  \n",
    "    layer.self_attn.v_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank)\n",
    "    layer.self_attn.k_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank)\n",
    "    layer.self_attn.out_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank)\n",
    "    layer=layer.to(device)\n",
    "    \n",
    "def biogpt_train_last_n_layers(model,n,lora_params=None):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if n>0:\n",
    "        max_layer_num = len(model.layers)-1\n",
    "        for param_name,param in model.named_parameters():\n",
    "            layer_num = re.search(r'layers\\.(\\d+)', param_name)\n",
    "            if layer_num:\n",
    "                number = int(layer_num.group(1))\n",
    "                if number>max_layer_num-n:\n",
    "                    param.requires_grad = True\n",
    "                    if lora_params is not None:\n",
    "\n",
    "                        apply_lora_biogpt_attention(**{**lora_params,\n",
    "                                                     'layer':model.layers[number]}\n",
    "                                                     )\n",
    "        \n",
    "        if lora_params is not None:\n",
    "            lora.mark_only_lora_as_trainable(model)\n",
    "        \n",
    "        #Always train last layer norm.\n",
    "        for param in model.layer_norm.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.23.self_attn.k_proj.lora_A\n",
      "layers.23.self_attn.k_proj.lora_B\n",
      "layers.23.self_attn.v_proj.lora_A\n",
      "layers.23.self_attn.v_proj.lora_B\n",
      "layers.23.self_attn.q_proj.lora_A\n",
      "layers.23.self_attn.q_proj.lora_B\n",
      "layers.23.self_attn.out_proj.lora_A\n",
      "layers.23.self_attn.out_proj.lora_B\n",
      "layer_norm.weight\n",
      "layer_norm.bias\n",
      "trainable params: 67584 || all params: 346828800 || trainable%: 0.02\n"
     ]
    }
   ],
   "source": [
    "biogpt_train_last_n_layers(model,1,lora_params={'rank':8,'in_features':1024,'out_features':1024,'device':'cuda:0'})\n",
    "\n",
    "for param_name,param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param_name)\n",
    "print_trainable_parameters(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_embeddings(last_hidden_states,attention_mask,method):\n",
    "    '''\n",
    "    '''\n",
    "    sequence_length = attention_mask.sum(dim=1, keepdim=True) #includind SOS token\n",
    "    last_token_indices = sequence_length - 1\n",
    "\n",
    " \n",
    "    if method=='mean':\n",
    "        #Account for SOS token\n",
    "        adjusted_attention_mask = attention_mask.clone()\n",
    "        adjusted_attention_mask[:,0]=0\n",
    " \n",
    "        # Mask the last_hidden_state tensor and compute the sum\n",
    "        sum_hidden_states = (last_hidden_states *\n",
    "                                adjusted_attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    " \n",
    "        # Compute the mean of the last hidden state\n",
    "        sequence_embedding = sum_hidden_states / (sequence_length-1) #subtract -1 for SOS token\n",
    " \n",
    "    elif method == 'last_token':\n",
    "        last_token_indices = last_token_indices\\\n",
    "            .unsqueeze(-1)\\\n",
    "            .expand(-1, -1, last_hidden_states.size(-1))\n",
    " \n",
    "        sequence_embedding = last_hidden_states.gather(1, last_token_indices).squeeze()\n",
    "    elif method == 'all':\n",
    " \n",
    "        sequence_embedding = last_hidden_states\n",
    "    \n",
    "    \n",
    " \n",
    " \n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-12-09 21:31:58 592209:592209 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-12-09 21:33:36 592209:592209 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-12-09 21:33:36 592209:592209 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "\n",
    "    outs = []\n",
    "    for _ in range(10):\n",
    "        out = model(input_ids = tokenized_data['input_ids'].to('cuda:0'),attention_mask = tokenized_data['attention_mask'].to('cuda:0')).last_hidden_state\n",
    "        sequence_embedding = pool_embeddings(out,tokenized_data['attention_mask'].to('cuda:0'),method='last_token')\n",
    "        outs.append(sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            cudaStreamSynchronize        95.65%       20.302s        95.65%       20.302s     676.743ms           0 b           0 b           0 b           0 b            30  \n",
      "                                         cudaFree         3.43%     728.124ms         3.43%     728.124ms     364.062ms           0 b           0 b           0 b           0 b             2  \n",
      "                                      aten::addmm         0.24%      50.993ms         3.74%     793.921ms     551.334us           0 b           0 b     421.88 Gb     420.48 Gb          1440  \n",
      "                                 cudaLaunchKernel         0.14%      30.087ms         0.14%      30.087ms       5.808us           0 b           0 b           0 b           0 b          5180  \n",
      "                                       cudaMalloc         0.07%      14.075ms         0.07%      14.075ms     502.679us           0 b           0 b           0 b           0 b            28  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 21.226s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                            cudaStreamSynchronize        72.84%        2.145s        72.84%        2.145s     715.110ms           0 b           0 b           0 b           0 b             3  \n",
      "                                         cudaFree        24.41%     718.970ms        24.41%     718.970ms     359.485ms           0 b           0 b           0 b           0 b             2  \n",
      "                                         aten::to         0.98%      28.903ms        74.05%        2.181s     198.282ms           0 b           0 b     127.17 Mb           0 b            11  \n",
      "                                      aten::addmm         0.42%      12.407ms        24.90%     733.406ms       5.093ms           0 b           0 b      42.20 Gb      42.05 Gb           144  \n",
      "                                       cudaMalloc         0.38%      11.055ms         0.38%      11.055ms     460.625us           0 b           0 b           0 b           0 b            24  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.945s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5600207872"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4864001560460185"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5600617472/1606418432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5600617472"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "820224"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5600617472 - 5599797248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 17398 MB.\n"
     ]
    }
   ],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding = pool_embeddings(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'),method='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scorer = nn.Linear(sequence_embedding.shape[-1],1, bias=True).to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding_ = (sequence_embedding*attn_weights.unsqueeze(-1)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-24.2735, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_embedding_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "\n",
    "#Masked scored for softmax\n",
    "raw_attn_scores = raw_attn_scores.masked_fill(tokenized_data['attention_mask'].to('cuda:0')==0,float('-inf'))\n",
    "\n",
    "#softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding___ = torch.bmm(attn_weights.unsqueeze(1),sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-4.6718e-06, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sequence_embedding_-sequence_embedding___.squeeze(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-24.2735, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_embedding___.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0223, 0.0329, 0.0190, 0.0210, 0.0325, 0.0150, 0.0556, 0.0831, 0.3534,\n",
       "        0.0090, 0.0100, 0.0238, 0.0070, 0.0072, 0.0189, 0.0219, 0.0217, 0.0133,\n",
       "        0.0118, 0.0154, 0.0192, 0.0197, 0.0158, 0.0951, 0.0076, 0.0084, 0.0092,\n",
       "        0.0301], device='cuda:0', grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(raw_attn_scores[0][:28],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3822,  0.7712,  0.2224,  ..., -0.7430, -0.7430, -0.7430],\n",
       "        [ 0.3822, -0.0762,  0.4468,  ..., -0.3420, -0.3420, -0.3420]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(raw_attn_scores+softmax_mask,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3425,  0.1925,  0.4544,  ..., -0.0675,  0.1730,  0.8976],\n",
       "        [-0.6936, -0.4513,  0.6367,  ..., -1.5723,  1.7188,  0.1801]],\n",
       "       device='cuda:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mean_hidden_states(last_hidden_states, attention_mask):\n",
    "    \"\"\"Compute the mean of the last hidden state for only the relevant tokens.\"\"\"\n",
    "    # Compute the number of relevant tokens for each sequence\n",
    "    num_relevant_tokens = attention_mask.sum(dim=1, keepdim=True)\n",
    "    # Mask the last_hidden_state tensor and compute the sum\n",
    "    sum_hidden_states = (last_hidden_states *\n",
    "                         attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    "    # Compute the mean of the last hidden state\n",
    "    return sum_hidden_states / num_relevant_tokens\n",
    "\n",
    "compute_mean_hidden_states(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512, 2])"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(out.last_hidden_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9007, -0.7836],\n",
       "        [-0.5462, -1.2105]], device='cuda:0', grad_fn=<IndexBackward0>)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output = score(out.last_hidden_state)\n",
    "idxs = (torch.ne(tokenized_data['input_ids'].to('cuda:0'), model.config.pad_token_id).sum(-1) - 1).to('cuda:0')\n",
    "linear_output[torch.arange(2, device='cuda:0'), idxs.squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda: True\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The model did not return a loss from the inputs, only the following keys: last_hidden_state,past_key_values. For reference, the inputs it received are input_ids,attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/Explore Freezing Layers.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     per_device_train_batch_size\u001b[39m=\u001b[39mbatch_size, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     remove_unused_columns\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdefault_args\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model\u001b[39m=\u001b[39mtest_model, args\u001b[39m=\u001b[39mtraining_args, train_dataset\u001b[39m=\u001b[39mdummy_dataset)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m result \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m print_summary(result)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/transformers/trainer.py:1555\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1555\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1556\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1557\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1558\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1559\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1560\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/transformers/trainer.py:1837\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1834\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[1;32m   1836\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1837\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1839\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1840\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1841\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1842\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1843\u001b[0m ):\n\u001b[1;32m   1844\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/transformers/trainer.py:2682\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2679\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m   2681\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2682\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[1;32m   2684\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   2685\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/transformers/trainer.py:2724\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2722\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2723\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m outputs:\n\u001b[0;32m-> 2724\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2725\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe model did not return a loss from the inputs, only the following keys: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2726\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(outputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m. For reference, the inputs it received are \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2727\u001b[0m         )\n\u001b[1;32m   2728\u001b[0m     \u001b[39m# We don't use .loss here since the model may return tuples instead of ModelOutput.\u001b[39;00m\n\u001b[1;32m   2729\u001b[0m     loss \u001b[39m=\u001b[39m outputs[\u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mdict\u001b[39m) \u001b[39melse\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: The model did not return a loss from the inputs, only the following keys: last_hidden_state,past_key_values. For reference, the inputs it received are input_ids,attention_mask."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "test_model = model\n",
    "\n",
    "# Check if model is on cuda\n",
    "print(f\"Model on cuda: {next(test_model.parameters()).is_cuda}\")\n",
    "\n",
    "batch_size = 40\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    **default_args\n",
    ")\n",
    "trainer = Trainer(model=test_model, args=training_args, train_dataset=dummy_dataset)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train full model: memory 62150 MB\n",
    "\n",
    "train last layer: memory 16900 MB\n",
    "\n",
    "train last 2 layers: memory 18820  MB\n",
    "\n",
    "train last 3 layers: memory 20620  MB\n",
    "\n",
    "train last 4 layers: memory 22582  MB\n",
    "\n",
    "train full model + lora: memory 54080 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LoRa:\n",
    "\n",
    "Model on cuda: True\n",
    "{'train_runtime': 14.3185, 'train_samples_per_second': 69.84, 'train_steps_per_second': 1.746, 'train_loss': 1.344638671875, 'epoch': 1.0}\n",
    "Time: 14.32\n",
    "Samples/second: 69.84\n",
    "GPU memory occupied: 55019 MB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
