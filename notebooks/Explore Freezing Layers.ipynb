{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "import loralib as lora\n",
    "import torch\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "GO_ANNOTATIONS_PATH = \"/home/samirchar/ProteinFunctions/data/annotations/go_annotations_2019_07_01_updated.pkl\"\n",
    "\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH)\n",
    "from src.utils.data import read_json\n",
    "embeddings_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings_last_token.pt\" \n",
    "embeddings_idx_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings_last_token_index.pt\" \n",
    "\n",
    "embeddings = torch.load(embeddings_path)\n",
    "embeddings_index = torch.load(embeddings_idx_path)\n",
    "\n",
    "# Get some labels\n",
    "text = go_annotations['label'].tolist()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GO:0000002</td>\n",
       "      <td>name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GO:0000002</td>\n",
       "      <td>label</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185239</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185240</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185241</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185242</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185243</th>\n",
       "      <td>GO:2001317</td>\n",
       "      <td>synonym_exact</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185244 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id description_type\n",
       "0       GO:0000001             name\n",
       "1       GO:0000001            label\n",
       "2       GO:0000001    synonym_exact\n",
       "3       GO:0000002             name\n",
       "4       GO:0000002            label\n",
       "...            ...              ...\n",
       "185239  GO:2001317    synonym_exact\n",
       "185240  GO:2001317    synonym_exact\n",
       "185241  GO:2001317    synonym_exact\n",
       "185242  GO:2001317    synonym_exact\n",
       "185243  GO:2001317    synonym_exact\n",
       "\n",
       "[185244 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[-1.2802,  0.4720,  2.4675,  ..., -1.8398, -1.0484,  0.5001]\n",
    "[-0.5067,  0.1378, -0.5433,  ..., -0.1567,  0.1737, -0.3209]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The chemical reactions and pathways resulting in the formation of kojic acid.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "go_annotations.loc['GO:2001317']['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import read_fasta\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[text[47400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text =[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 48427 MB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 48427 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification,AutoModel\n",
    "\n",
    "print_gpu_utilization()\n",
    "checkpoint = 'microsoft/biogpt'\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint,\n",
    "    # torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print_gpu_utilization()\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"do_eval\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "# Tokenize the go_annotations list\n",
    "tokenized_data = tokenizer(text, padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Create random labels for the new dataset\n",
    "random_labels = np.random.randint(0, 1, (len(text)))\n",
    "\n",
    "# Create the dataset from the tokenized data and random labels\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"], \n",
    "    \"labels\": random_labels\n",
    "})\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "dummy_dataset.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re \n",
    "def apply_lora_biogpt_attention(layer,rank,alpha,device,in_features= 1024, out_features= 1024):\n",
    "    # layer.self_attn.q_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)  \n",
    "    # layer.self_attn.v_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    # layer.self_attn.k_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    layer.self_attn.out_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    # layer.fc1 = lora.Linear(\n",
    "    #     in_features, out_features*4, r=rank,lora_alpha=alpha)\n",
    "    # layer.fc2 = lora.Linear(\n",
    "    #     in_features*4, out_features, r=rank,lora_alpha=alpha)\n",
    "    \n",
    "\n",
    "    layer=layer.to(device)\n",
    "    \n",
    "def biogpt_train_last_n_layers(model,n,lora_params=None):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if n>0:\n",
    "        max_layer_num = len(model.layers)-1\n",
    "        for param_name,param in model.named_parameters():\n",
    "            layer_num = re.search(r'layers\\.(\\d+)', param_name)\n",
    "            if layer_num:\n",
    "                number = int(layer_num.group(1))\n",
    "                if number>max_layer_num-n:\n",
    "                    param.requires_grad = True\n",
    "                    if lora_params is not None:\n",
    "\n",
    "                        apply_lora_biogpt_attention(**{**lora_params,\n",
    "                                                     'layer':model.layers[number]}\n",
    "                                                     )\n",
    "        \n",
    "        if lora_params is not None:\n",
    "            lora.mark_only_lora_as_trainable(model)\n",
    "        \n",
    "        #Always train last layer norm.\n",
    "        for param in model.layer_norm.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BioGptModel(\n",
       "  (embed_tokens): Embedding(42384, 1024, padding_idx=1)\n",
       "  (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x BioGptDecoderLayer(\n",
       "      (self_attn): BioGptAttention(\n",
       "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (activation_fn): GELUActivation()\n",
       "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 346763264 || trainable%: 0.00\n"
     ]
    }
   ],
   "source": [
    "biogpt_train_last_n_layers(model,\n",
    "                           0,\n",
    "                           lora_params={'rank':8,'alpha':4,'in_features':1024,'out_features':1024,'device':'cuda:0'}\n",
    "                           )\n",
    "\n",
    "for param_name,param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param_name)\n",
    "print_trainable_parameters(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_embeddings(last_hidden_states,attention_mask,method):\n",
    "    '''\n",
    "    '''\n",
    "    sequence_length = attention_mask.sum(dim=1, keepdim=True) #includind SOS token\n",
    "    last_token_indices = sequence_length - 1\n",
    "\n",
    " \n",
    "    if method=='mean':\n",
    "        #Account for SOS token\n",
    "        adjusted_attention_mask = attention_mask.clone()\n",
    "        adjusted_attention_mask[:,0]=0\n",
    " \n",
    "        # Mask the last_hidden_state tensor and compute the sum\n",
    "        sum_hidden_states = (last_hidden_states *\n",
    "                                adjusted_attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    " \n",
    "        # Compute the mean of the last hidden state\n",
    "        sequence_embedding = sum_hidden_states / (sequence_length-1) #subtract -1 for SOS token\n",
    " \n",
    "    elif method == 'last_token':\n",
    "        last_token_indices = last_token_indices\\\n",
    "            .unsqueeze(-1)\\\n",
    "            .expand(-1, -1, last_hidden_states.size(-1))\n",
    " \n",
    "        sequence_embedding = last_hidden_states.gather(1, last_token_indices).squeeze()\n",
    "    elif method == 'all':\n",
    " \n",
    "        sequence_embedding = last_hidden_states\n",
    "    \n",
    "    \n",
    " \n",
    " \n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scorer = torch.nn.Linear(1024,1, bias=True).to('cuda:0')\n",
    "\n",
    "def additive_attention(hidden_states,attention_mask):\n",
    "    raw_attn_scores = raw_attn_scorer(hidden_states).squeeze(-1)\n",
    "    \n",
    "    #Masked scored for softmax\n",
    "    raw_attn_scores = raw_attn_scores.masked_fill(attention_mask==0,float('-inf'))\n",
    "\n",
    "    #Normalized attention weights\n",
    "    attn_weights = torch.softmax(raw_attn_scores,dim=-1)\n",
    "\n",
    "    #Get final label embedding\n",
    "    return torch.bmm(attn_weights.unsqueeze(1),hidden_states).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_joint_embeddings(P_e, L_e, num_sequences,num_labels):\n",
    "\n",
    "    sequence_embedding_dim = P_e.shape[1]\n",
    "    label_embedding_dim = L_e.shape[1]\n",
    "\n",
    "    # Use broadcasting so we don't have to expand the tensor dimensions\n",
    "    joint_embeddings = torch.cat([\n",
    "        P_e[:, None, :].expand(\n",
    "            num_sequences, num_labels, sequence_embedding_dim),\n",
    "        L_e[None, :, :].expand(\n",
    "            num_sequences, num_labels, label_embedding_dim)\n",
    "    ], dim=2).reshape(-1, sequence_embedding_dim + label_embedding_dim)\n",
    "\n",
    "    return joint_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_f = torch.rand((32,1100)).to('cuda:0')\n",
    "\n",
    "#L_e = torch.rand((32102,1024))\n",
    "L_f = additive_attention(label_embeddings,masks)\n",
    "W_p = MLP(1100,[1024]*1,bias=False,norm_layer=torch.nn.BatchNorm1d).to('cuda:0')\n",
    "W_l = MLP(1024,[1024]*1,bias=False,norm_layer=torch.nn.BatchNorm1d).to('cuda:0')\n",
    "\n",
    "L_e = W_l(L_f)\n",
    "P_e = W_p(P_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint=_get_joint_embeddings(P_e.to('cuda:0'), L_e, 32,32102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint.shape,P_e.shape,L_e.shape,P_f.shape,L_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data['input_ids']=torch.randint(0,10000,(1000,60)).to('cuda:0')\n",
    "tokenized_data['attention_mask']=torch.randint(0,1,(1000,60)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([47400]),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(go_annotations.index=='GO:2001317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,    18,   919,  1263,     8,   885,   946,    10,     6,   381,\n",
       "             5, 15527,  1777,  1047,   156,     4,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['input_ids'][[47400]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3408222720\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "bs = 1000\n",
    "embeddings = []\n",
    "\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    with autocast(), torch.set_grad_enabled(False):\n",
    "\n",
    "        embeddings.append(\n",
    "            pool_embeddings(\n",
    "                model(\n",
    "                    input_ids = tokenized_data['input_ids'][:bs,:].to('cuda:0'),\n",
    "                    attention_mask = tokenized_data['attention_mask'][:bs,:].to('cuda:0')).last_hidden_state,\n",
    "                    tokenized_data['attention_mask'][:bs,:].to('cuda:0'),\n",
    "                    method='last_token'\n",
    "                    )\n",
    "                )   \n",
    "    print(torch.cuda.memory_allocated('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-44.6542, device='cuda:0', grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "81.2/2744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(13235055616-1387511808)/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "import torch\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "\n",
    "    outs = []\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for i in range(13):\n",
    "            print(i)\n",
    "            out = model(input_ids = tokenized_data['input_ids'].to('cuda:0'),attention_mask = tokenized_data['attention_mask'].to('cuda:0')).last_hidden_state\n",
    "            sequence_embedding = pool_embeddings(out,tokenized_data['attention_mask'].to('cuda:0'),method='last_token')\n",
    "            outs.append(sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()\n",
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cuda_memory_usage', row_limit=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5600617472/1606418432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5600617472 - 5599797248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding = pool_embeddings(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'),method='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scorer = nn.Linear(sequence_embedding.shape[-1],1, bias=True).to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding_ = (sequence_embedding*attn_weights.unsqueeze(-1)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "\n",
    "#Masked scored for softmax\n",
    "raw_attn_scores = raw_attn_scores.masked_fill(tokenized_data['attention_mask'].to('cuda:0')==0,float('-inf'))\n",
    "\n",
    "#softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding___ = torch.bmm(attn_weights.unsqueeze(1),sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sequence_embedding_-sequence_embedding___.squeeze(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding___.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(raw_attn_scores[0][:28],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(raw_attn_scores+softmax_mask,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_hidden_states(last_hidden_states, attention_mask):\n",
    "    \"\"\"Compute the mean of the last hidden state for only the relevant tokens.\"\"\"\n",
    "    # Compute the number of relevant tokens for each sequence\n",
    "    num_relevant_tokens = attention_mask.sum(dim=1, keepdim=True)\n",
    "    # Mask the last_hidden_state tensor and compute the sum\n",
    "    sum_hidden_states = (last_hidden_states *\n",
    "                         attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    "    # Compute the mean of the last hidden state\n",
    "    return sum_hidden_states / num_relevant_tokens\n",
    "\n",
    "compute_mean_hidden_states(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(out.last_hidden_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output = score(out.last_hidden_state)\n",
    "idxs = (torch.ne(tokenized_data['input_ids'].to('cuda:0'), model.config.pad_token_id).sum(-1) - 1).to('cuda:0')\n",
    "linear_output[torch.arange(2, device='cuda:0'), idxs.squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "test_model = model\n",
    "\n",
    "# Check if model is on cuda\n",
    "print(f\"Model on cuda: {next(test_model.parameters()).is_cuda}\")\n",
    "\n",
    "batch_size = 40\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    **default_args\n",
    ")\n",
    "trainer = Trainer(model=test_model, args=training_args, train_dataset=dummy_dataset)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train full model: memory 62150 MB\n",
    "\n",
    "train last layer: memory 16900 MB\n",
    "\n",
    "train last 2 layers: memory 18820  MB\n",
    "\n",
    "train last 3 layers: memory 20620  MB\n",
    "\n",
    "train last 4 layers: memory 22582  MB\n",
    "\n",
    "train full model + lora: memory 54080 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LoRa:\n",
    "\n",
    "Model on cuda: True\n",
    "{'train_runtime': 14.3185, 'train_samples_per_second': 69.84, 'train_steps_per_second': 1.746, 'train_loss': 1.344638671875, 'epoch': 1.0}\n",
    "Time: 14.32\n",
    "Samples/second: 69.84\n",
    "GPU memory occupied: 55019 MB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
