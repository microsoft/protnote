{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# Load test dataset\n",
    "GO_ANNOTATIONS_PATH = \"/home/samirchar/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH)\n",
    "\n",
    "# Get first 1000 labels as a list\n",
    "text = go_annotations.iloc[:1000, 0].tolist()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 5131 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BioGptForSequenceClassification were not initialized from the model checkpoint at microsoft/biogpt and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 7370 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "print_gpu_utilization()\n",
    "checkpoint = 'microsoft/biogpt'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    # torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print_gpu_utilization()\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"do_eval\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/Explore Freezing Layers.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Tokenize the go_annotations list\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m tokenized_data \u001b[39m=\u001b[39m tokenizer(text, padding\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax_length\u001b[39m\u001b[39m\"\u001b[39m, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_length\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create random labels for the new dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/Explore%20Freezing%20Layers.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m random_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, (\u001b[39mlen\u001b[39m(text)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# Tokenize the go_annotations list\n",
    "tokenized_data = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Create random labels for the new dataset\n",
    "random_labels = np.random.randint(0, 1, (len(text)))\n",
    "\n",
    "# Create the dataset from the tokenized data and random labels\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"], \n",
    "    \"labels\": random_labels\n",
    "})\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "dummy_dataset.set_format(\"pt\")\n",
    "\n",
    "print_trainable_parameters(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def biogpt_train_last_n_layers(model,n):\n",
    "    if n>0:\n",
    "        max_layer_num = len(model.biogpt.layers)-1\n",
    "        for param_name,param in model.named_parameters():\n",
    "            layer_num = re.search(r'biogpt.layers\\.(\\d+)', param_name)\n",
    "\n",
    "            if layer_num:\n",
    "                number = int(layer_num.group(1))\n",
    "                if number>max_layer_num-n:\n",
    "                    param.requires_grad = True\n",
    "\n",
    "        for param in model.biogpt.layer_norm.parameters():\n",
    "            param.requires_grad = True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda: True\n",
      "{'train_runtime': 7.7045, 'train_samples_per_second': 129.795, 'train_steps_per_second': 3.245, 'train_loss': 0.029159345626831056, 'epoch': 1.0}\n",
      "Time: 7.70\n",
      "Samples/second: 129.79\n",
      "GPU memory occupied: 22582 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "# freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "biogpt_train_last_n_layers(model,4)\n",
    "\n",
    "\n",
    "test_model = model\n",
    "\n",
    "# Check if model is on cuda\n",
    "print(f\"Model on cuda: {next(test_model.parameters()).is_cuda}\")\n",
    "\n",
    "batch_size = 40\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    **default_args\n",
    ")\n",
    "trainer = Trainer(model=test_model, args=training_args, train_dataset=dummy_dataset)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train full model: memory 62150 MB\n",
    "\n",
    "train last layer: memory 16900 MB\n",
    "\n",
    "train last 2 layers: memory 18820  MB\n",
    "\n",
    "train last 3 layers: memory 20620  MB\n",
    "\n",
    "train last 4 layers: memory 22582  MB\n",
    "\n",
    "train full model + lora: memory 54080 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LoRa:\n",
    "\n",
    "Model on cuda: True\n",
    "{'train_runtime': 14.3185, 'train_samples_per_second': 69.84, 'train_steps_per_second': 1.746, 'train_loss': 1.344638671875, 'epoch': 1.0}\n",
    "Time: 14.32\n",
    "Samples/second: 69.84\n",
    "GPU memory occupied: 55019 MB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
