{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import sys\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "\n",
    "# Load test dataset\n",
    "GO_ANNOTATIONS_PATH = \"/home/samirchar/ProteinFunctions/data/annotations/go_annotations_2019_07_01_updated.pkl\"\n",
    "\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH)\n",
    "from src.utils.data import read_json\n",
    "embeddings_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_E5_multiling_inst_label_embeddings_mean.pt\" \n",
    "embeddings_idx_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_E5_multiling_inst_label_embeddings_mean_index.pt\" \n",
    "\n",
    "embeddings = torch.load(embeddings_path)\n",
    "embeddings_index = torch.load(embeddings_idx_path)\n",
    "\n",
    "# Get some labels\n",
    "text = go_annotations['label'].tolist()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Instruct: Identify the main categories, themes, or topics described in the following Gene Ontology (GO) term, which is used to detail a protein's function\\nQuery: patulin biosynthetic process\"]\n",
      "tensor([ 0.7899,  0.3719, -0.3108,  ..., -0.2164, -0.3004,  0.4498]) tensor(-8.3121)\n"
     ]
    }
   ],
   "source": [
    "embeddings_path =\"/home/samirchar/ProteinFunctions/data/embeddings/2024_E5_multiling_inst_frozen_label_embeddings_mean.pt\" \n",
    "embeddings_idx_path =\"/home/samirchar/ProteinFunctions/data/embeddings/2024_E5_multiling_inst_frozen_label_embeddings_mean_index.pt\" \n",
    "embeddings = torch.load(embeddings_path)\n",
    "embeddings_index = torch.load(embeddings_idx_path)\n",
    "text=embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')]['description'].tolist()\n",
    "text_embeddings = embeddings[embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')].index[0]]\n",
    "print(text)\n",
    "print(text_embeddings,text_embeddings.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([9304]), tensor([770]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(embeddings == 0.7906093597412109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7906)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[9304,770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Instruct: Identify the main categories, themes, or topics described in the following Gene Ontology (GO) term, which is used to detail a protein's function\\nQuery: patulin biosynthetic process\"]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Instruct: Identify the main categories, themes, or topics described in the following Gene Ontology (GO) term, which is used to detail a protein's function\\nQuery: patulin biosynthetic process\"]\n",
      "tensor([ 0.7931,  0.3759, -0.3119,  ..., -0.2140, -0.3011,  0.4528]) tensor(-8.3237)\n"
     ]
    }
   ],
   "source": [
    "embeddings_path =\"/home/samirchar/ProteinFunctions/data/embeddings/2024_E5_multiling_inst_sos_false_frozen_label_embeddings_mean.pt\" \n",
    "embeddings_idx_path =\"/home/samirchar/ProteinFunctions/data/embeddings/2024_E5_multiling_inst_sos_false_frozen_label_embeddings_mean_index.pt\" \n",
    "embeddings = torch.load(embeddings_path)\n",
    "embeddings_index = torch.load(embeddings_idx_path)\n",
    "text=embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')]['description'].tolist()\n",
    "text_embeddings = embeddings[embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')].index[0]]\n",
    "print(text)\n",
    "print(text_embeddings,text_embeddings.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Instruct: Identify the main categories, themes, or topics described in the following Gene Ontology (GO) term, which is used to detail a protein's function\\nQuery: patulin biosynthetic process\"]\n",
      "tensor([ 0.7906,  0.3704, -0.3123,  ..., -0.2163, -0.2990,  0.4475]) tensor(-8.3131)\n"
     ]
    }
   ],
   "source": [
    "embeddings_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_E5_multiling_inst_label_embeddings_mean.pt\" \n",
    "embeddings_idx_path =\"/home/samirchar/ProteinFunctions/data/embeddings/frozen_E5_multiling_inst_label_embeddings_mean_index.pt\" \n",
    "embeddings = torch.load(embeddings_path)\n",
    "embeddings_index = torch.load(embeddings_idx_path)\n",
    "text=embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')]['description'].tolist()\n",
    "text_embeddings = embeddings[embeddings_index[(embeddings_index['id']=='GO:0140723')&(embeddings_index['description_type']=='name')].index[0]]\n",
    "print(text)\n",
    "print(text_embeddings,text_embeddings.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 10993, 188642]), tensor([770,   0]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(embeddings == 0.7906093597412109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7906,  0.3704, -0.3123,  ..., -0.2163, -0.2990,  0.4475])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[188642]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.7899,  0.3719, -0.3108,  ..., -0.2164, -0.3004,  0.4498]),\n",
       " tensor(-0.0081))"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embeddings,text_embeddings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import read_fasta\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer,AutoModel\n",
    "\n",
    "checkpoint = 'intfloat/multilingual-e5-large-instruct'\n",
    "model = AutoModel.from_pretrained(\n",
    "    checkpoint,\n",
    "    # torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"do_eval\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "# Tokenize the go_annotations list\n",
    "tokenized_data = tokenizer(text, padding=\"longest\", truncation=True, max_length=510, return_tensors=\"pt\")\n",
    "\n",
    "# Create random labels for the new dataset\n",
    "random_labels = np.random.randint(0, 1, (len(text)))\n",
    "\n",
    "# Create the dataset from the tokenized data and random labels\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"], \n",
    "    \"labels\": random_labels\n",
    "})\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "dummy_dataset.set_format(\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re \n",
    "def apply_lora_biogpt_attention(layer,rank,alpha,device,in_features= 1024, out_features= 1024):\n",
    "    # layer.self_attn.q_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)  \n",
    "    # layer.self_attn.v_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    # layer.self_attn.k_proj = lora.Linear(\n",
    "    #     in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    layer.self_attn.out_proj = lora.Linear(\n",
    "        in_features, out_features, r=rank,lora_alpha=alpha)\n",
    "    # layer.fc1 = lora.Linear(\n",
    "    #     in_features, out_features*4, r=rank,lora_alpha=alpha)\n",
    "    # layer.fc2 = lora.Linear(\n",
    "    #     in_features*4, out_features, r=rank,lora_alpha=alpha)\n",
    "    \n",
    "\n",
    "    layer=layer.to(device)\n",
    "    \n",
    "def biogpt_train_last_n_layers(model,n,lora_params=None):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if n>0:\n",
    "        max_layer_num = len(model.layers)-1\n",
    "        for param_name,param in model.named_parameters():\n",
    "            layer_num = re.search(r'layers\\.(\\d+)', param_name)\n",
    "            if layer_num:\n",
    "                number = int(layer_num.group(1))\n",
    "                if number>max_layer_num-n:\n",
    "                    param.requires_grad = True\n",
    "                    if lora_params is not None:\n",
    "\n",
    "                        apply_lora_biogpt_attention(**{**lora_params,\n",
    "                                                     'layer':model.layers[number]}\n",
    "                                                     )\n",
    "        \n",
    "        if lora_params is not None:\n",
    "            lora.mark_only_lora_as_trainable(model)\n",
    "        \n",
    "        #Always train last layer norm.\n",
    "        for param in model.layer_norm.parameters():\n",
    "            param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 346763264 || trainable%: 0.00\n"
     ]
    }
   ],
   "source": [
    "biogpt_train_last_n_layers(model,\n",
    "                           0,\n",
    "                           lora_params={'rank':8,'alpha':4,'in_features':1024,'out_features':1024,'device':'cuda:0'}\n",
    "                           )\n",
    "\n",
    "for param_name,param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param_name)\n",
    "print_trainable_parameters(model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pool_embeddings(last_hidden_states,attention_mask,method,account_for_sos = True):\n",
    "    '''\n",
    "    '''\n",
    "    sequence_length_raw = attention_mask.sum(dim=1, keepdim=True) #includind SOS token\n",
    "\n",
    "    sequence_length = sequence_length_raw - 1*account_for_sos\n",
    "\n",
    " \n",
    "    if method=='mean':\n",
    "        #Account for SOS token\n",
    "        adjusted_attention_mask = attention_mask.clone()\n",
    "        if account_for_sos:\n",
    "            adjusted_attention_mask[:,0]=0\n",
    " \n",
    "        # Mask the last_hidden_state tensor and compute the sum\n",
    "        sum_hidden_states = (last_hidden_states *\n",
    "                                adjusted_attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    " \n",
    "        # Compute the mean of the last hidden state\n",
    "        sequence_embedding = sum_hidden_states / (sequence_length) \n",
    " \n",
    "    elif method == 'last_token':\n",
    "        last_token_indices = (sequence_length_raw - 1)\\\n",
    "            .unsqueeze(-1)\\\n",
    "            .expand(-1, -1, last_hidden_states.size(-1))\n",
    " \n",
    "        sequence_embedding = last_hidden_states.gather(1, last_token_indices).squeeze()\n",
    "    elif method == 'all':\n",
    " \n",
    "        sequence_embedding = last_hidden_states\n",
    " \n",
    "    return sequence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scorer = torch.nn.Linear(1024,1, bias=True).to('cuda:0')\n",
    "\n",
    "def additive_attention(hidden_states,attention_mask):\n",
    "    raw_attn_scores = raw_attn_scorer(hidden_states).squeeze(-1)\n",
    "    \n",
    "    #Masked scored for softmax\n",
    "    raw_attn_scores = raw_attn_scores.masked_fill(attention_mask==0,float('-inf'))\n",
    "\n",
    "    #Normalized attention weights\n",
    "    attn_weights = torch.softmax(raw_attn_scores,dim=-1)\n",
    "\n",
    "    #Get final label embedding\n",
    "    return torch.bmm(attn_weights.unsqueeze(1),hidden_states).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_joint_embeddings(P_e, L_e, num_sequences,num_labels):\n",
    "\n",
    "    sequence_embedding_dim = P_e.shape[1]\n",
    "    label_embedding_dim = L_e.shape[1]\n",
    "\n",
    "    # Use broadcasting so we don't have to expand the tensor dimensions\n",
    "    joint_embeddings = torch.cat([\n",
    "        P_e[:, None, :].expand(\n",
    "            num_sequences, num_labels, sequence_embedding_dim),\n",
    "        L_e[None, :, :].expand(\n",
    "            num_sequences, num_labels, label_embedding_dim)\n",
    "    ], dim=2).reshape(-1, sequence_embedding_dim + label_embedding_dim)\n",
    "\n",
    "    return joint_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_f = torch.rand((32,1100)).to('cuda:0')\n",
    "\n",
    "#L_e = torch.rand((32102,1024))\n",
    "L_f = additive_attention(label_embeddings,masks)\n",
    "W_p = MLP(1100,[1024]*1,bias=False,norm_layer=torch.nn.BatchNorm1d).to('cuda:0')\n",
    "W_l = MLP(1024,[1024]*1,bias=False,norm_layer=torch.nn.BatchNorm1d).to('cuda:0')\n",
    "\n",
    "L_e = W_l(L_f)\n",
    "P_e = W_p(P_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint=_get_joint_embeddings(P_e.to('cuda:0'), L_e, 32,32102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint.shape,P_e.shape,L_e.shape,P_f.shape,L_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data['input_ids']=torch.randint(0,10000,(1000,60)).to('cuda:0')\n",
    "tokenized_data['attention_mask']=torch.randint(0,1,(1000,60)).to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[     0,    360,  36716,     12,  71257,  40383,     70,   5201,  39283,\n",
       "              7,      4,   2856,     90,      4,    707,  28451,      7, 151552,\n",
       "             23,     70,  25632,  46980,   2161,     18,  25443,     15,  19930,\n",
       "             16,  13579,      4,   3129,     83,  11814,     47,  22443,     10,\n",
       "          21308,     25,      7,  32354,  58836,     53,     12,   2340,  53311,\n",
       "           3530,  12654,   2347,   9523,   9433,      2]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Instruct: Identify the main categories, themes, or topics described in the following Gene Ontology (GO) term, which is used to detail a protein's function\\nQuery: patulin biosynthetic process\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2239570944"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([47400]),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(go_annotations.index=='GO:2001317')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2248168448\n"
     ]
    }
   ],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "import torch\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "bs = 1000\n",
    "embeddings = []\n",
    "model.eval()\n",
    "for i in range(1):\n",
    "    print(i)\n",
    "    with autocast(), torch.set_grad_enabled(False):\n",
    "\n",
    "        embeddings.append(\n",
    "            pool_embeddings(\n",
    "                model(\n",
    "                    input_ids = tokenized_data['input_ids'][:bs,:].to('cuda:0'),\n",
    "                    attention_mask = tokenized_data['attention_mask'][:bs,:].to('cuda:0')).last_hidden_state,\n",
    "                    tokenized_data['attention_mask'][:bs,:].to('cuda:0'),\n",
    "                    method='mean',\n",
    "                    account_for_sos=False\n",
    "                    ),\n",
    "                    \n",
    "                )   \n",
    "    print(torch.cuda.memory_allocated('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor([[ 0.7928,  0.3759, -0.3129,  ..., -0.2123, -0.3003,  0.4511]],\n",
       "         device='cuda:0')],\n",
       " tensor(-8.3214, device='cuda:0'))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings,embeddings[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "81.2/2744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(13235055616-1387511808)/1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.autograd.profiler as profiler\n",
    "import torch\n",
    "with profiler.profile(with_stack=True, profile_memory=True) as prof:\n",
    "\n",
    "    outs = []\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for i in range(13):\n",
    "            print(i)\n",
    "            out = model(input_ids = tokenized_data['input_ids'].to('cuda:0'),attention_mask = tokenized_data['attention_mask'].to('cuda:0')).last_hidden_state\n",
    "            sequence_embedding = pool_embeddings(out,tokenized_data['attention_mask'].to('cuda:0'),method='last_token')\n",
    "            outs.append(sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()\n",
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cuda_memory_usage', row_limit=50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prof.key_averages(group_by_stack_n=5).table(sort_by='self_cpu_time_total', row_limit=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5600617472/1606418432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5600617472 - 5599797248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding = pool_embeddings(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'),method='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scorer = nn.Linear(sequence_embedding.shape[-1],1, bias=True).to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding_ = (sequence_embedding*attn_weights.unsqueeze(-1)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "raw_attn_scores = raw_attn_scorer(sequence_embedding).squeeze(-1)\n",
    "\n",
    "#Masked scored for softmax\n",
    "raw_attn_scores = raw_attn_scores.masked_fill(tokenized_data['attention_mask'].to('cuda:0')==0,float('-inf'))\n",
    "\n",
    "#softmax_mask = torch.where(tokenized_data['attention_mask'].to('cuda:0')==0,-torch.inf,tokenized_data['attention_mask'].to('cuda:0'))\n",
    "attn_weights = torch.softmax(raw_attn_scores+softmax_mask,dim=-1)\n",
    "sequence_embedding___ = torch.bmm(attn_weights.unsqueeze(1),sequence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sequence_embedding_-sequence_embedding___.squeeze(1)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_embedding___.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(raw_attn_scores[0][:28],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(raw_attn_scores+softmax_mask,dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_hidden_states(last_hidden_states, attention_mask):\n",
    "    \"\"\"Compute the mean of the last hidden state for only the relevant tokens.\"\"\"\n",
    "    # Compute the number of relevant tokens for each sequence\n",
    "    num_relevant_tokens = attention_mask.sum(dim=1, keepdim=True)\n",
    "    # Mask the last_hidden_state tensor and compute the sum\n",
    "    sum_hidden_states = (last_hidden_states *\n",
    "                         attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    "    # Compute the mean of the last hidden state\n",
    "    return sum_hidden_states / num_relevant_tokens\n",
    "\n",
    "compute_mean_hidden_states(out.last_hidden_state,tokenized_data['attention_mask'].to('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(out.last_hidden_state).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output = score(out.last_hidden_state)\n",
    "idxs = (torch.ne(tokenized_data['input_ids'].to('cuda:0'), model.config.pad_token_id).sum(-1) - 1).to('cuda:0')\n",
    "linear_output[torch.arange(2, device='cuda:0'), idxs.squeeze()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "test_model = model\n",
    "\n",
    "# Check if model is on cuda\n",
    "print(f\"Model on cuda: {next(test_model.parameters()).is_cuda}\")\n",
    "\n",
    "batch_size = 40\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=False,\n",
    "    **default_args\n",
    ")\n",
    "trainer = Trainer(model=test_model, args=training_args, train_dataset=dummy_dataset)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train full model: memory 62150 MB\n",
    "\n",
    "train last layer: memory 16900 MB\n",
    "\n",
    "train last 2 layers: memory 18820  MB\n",
    "\n",
    "train last 3 layers: memory 20620  MB\n",
    "\n",
    "train last 4 layers: memory 22582  MB\n",
    "\n",
    "train full model + lora: memory 54080 MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "LoRa:\n",
    "\n",
    "Model on cuda: True\n",
    "{'train_runtime': 14.3185, 'train_samples_per_second': 69.84, 'train_steps_per_second': 1.746, 'train_loss': 1.344638671875, 'epoch': 1.0}\n",
    "Time: 14.32\n",
    "Samples/second: 69.84\n",
    "GPU memory occupied: 55019 MB."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
