{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle, read_json\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embeddings: torch.Size([6, 2])\n",
      "Labels: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example sequences and labels\n",
    "sequences = [\"SEQ1\", \"SEQ2\", \"SEQ3\"]\n",
    "labels = torch.tensor([\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Example sequence embeddings\n",
    "sequence_to_embeddings_dict = {\n",
    "    \"SEQ1\": torch.tensor([0.1, 0.2, 0.3]),\n",
    "    \"SEQ2\": torch.tensor([0.4, 0.5, 0.6]),\n",
    "    \"SEQ3\": torch.tensor([0.7, 0.8, 0.9])\n",
    "}\n",
    "\n",
    "# Example label embeddings\n",
    "label_embeddings = torch.tensor([\n",
    "    [0.01, 0.02],\n",
    "    [0.03, 0.04],\n",
    "    [0.05, 0.06], # Embeddings for all-zero columns\n",
    "    [0.07, 0.08],\n",
    "    [0.09, 0.10],  # Embeddings for all-zero columns\n",
    "    [0.11, 0.12]   # Embeddings for all-zero columns\n",
    "])\n",
    "\n",
    "print(\"Label embeddings:\", label_embeddings.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Constants\n",
    "PROTEIN_EMBEDDING_DIM = 3\n",
    "LABEL_EMBEDDING_DIM = 2\n",
    "LATENT_EMBEDDING_DIM = 2\n",
    "TEMPERATURE = 1.0\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Assuming you're using a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "label_embeddings = label_embeddings.to(device)\n",
    "sequence_to_embeddings_dict = {seq: embedding.to(device) for seq, embedding in sequence_to_embeddings_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ProTCL(protein_embedding_dim=PROTEIN_EMBEDDING_DIM, \n",
    "                  label_embedding_dim=LABEL_EMBEDDING_DIM, \n",
    "                  latent_dim=LATENT_EMBEDDING_DIM,\n",
    "                  temperature=TEMPERATURE,\n",
    "                  sequence_to_embeddings_dict=sequence_to_embeddings_dict,\n",
    "                  ordered_label_embeddings=label_embeddings).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[-0.8432,  0.5377],\n",
      "        [-0.8763,  0.4817],\n",
      "        [-0.8845,  0.4665]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[-0.7129,  0.7013],\n",
      "        [-0.7731,  0.6343],\n",
      "        [-0.7986,  0.6019]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Test the forward method\n",
    "P_e, L_e, target = model(sequences, labels)\n",
    "print(P_e)\n",
    "print(L_e)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6116, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the contrastive loss function\n",
    "loss = contrastive_loss(P_e, L_e, model.t, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle, read_json\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from ProteinFunctions.src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall\n",
    "\n",
    "# Get the root path from the environment variable\n",
    "ROOT_PATH = os.environ.get('ROOT_PATH', '.')  # Default to current directory if ROOT_PATH is not set\n",
    "\n",
    "# Load the configuration file\n",
    "config = read_json(os.path.join(ROOT_PATH, 'config.json'))\n",
    "params = config['params']\n",
    "relative_paths = config['relative_paths']\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(filename='train.log', filemode='w',\n",
    "                    format='%(asctime)s %(levelname)-4s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "# Initialize new run in W&B\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.info(\n",
    "    f\"################## {timestamp} RUNNING test_ProConNet.py ##################\")\n",
    "\n",
    "config={\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"TEMPERATURE\": 0.07,\n",
    "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
    "    \"LABEL_EMBEDDING_DIM\": 768,\n",
    "    \"LATENT_EMBEDDING_DIM\": 934,\n",
    "    \"NUM_EPOCHS\": 10,\n",
    "    \"BATCH_SIZE\": 1000,\n",
    "    \"DECISION_TH\": 0.88\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/label_embeddings.pk1\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pk1\"\n",
    "\n",
    "# Directory to save models\n",
    "OUTPUT_MODEL_DIR = \"/home/ncorley/protein/ProteinFunctions/models/ProConNet\"\n",
    "\n",
    "# Model to load\n",
    "LOAD_MODEL_PATH = \"/home/ncorley/protein/ProteinFunctions/models/ProConNet/2023-09-05_23-28-25_best_Pro_ConNet.pth\"\n",
    "\n",
    "# Test only\n",
    "TRAIN_MODEL = False\n",
    "TEST_MODEL = True\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets\n",
    "test_dataset = ProteinDataset(data_path=TEST_DATA_PATH,\n",
    "                              sequence_vocabulary_path=AMINO_ACID_VOCAB_PATH,\n",
    "                              label_vocabulary_path=GO_LABEL_VOCAB_PATH)\n",
    "\n",
    "# Define data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=10, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "\n",
    "# Load sequence embeddings\n",
    "if os.path.exists(SEQUENCE_EMBEDDING_PATH):\n",
    "    sequence_to_embeddings_dict_raw = read_pickle(SEQUENCE_EMBEDDING_PATH)[\n",
    "        ['sequence', 'embedding']].set_index('sequence')['embedding'].to_dict()\n",
    "\n",
    "    # Convert embeddings in the dictionary to tensors\n",
    "    sequence_to_embeddings_dict = {seq: torch.tensor(\n",
    "        embedding) for seq, embedding in sequence_to_embeddings_dict_raw.items()}\n",
    "else:\n",
    "    raise ValueError(\"Sequence embeddings not found.\")\n",
    "\n",
    "# Load label embeddings (in the same order as the label vocabulary)\n",
    "if os.path.exists(LABEL_EMBEDDING_PATH):\n",
    "    # Load a dictionary mapping GO IDs to embeddings\n",
    "    label_embedding_dict = read_pickle(LABEL_EMBEDDING_PATH)[\n",
    "        ['go_id', 'embedding']].set_index('go_id')['embedding'].to_dict()\n",
    "    # Create a tensor of embeddings in the correct order (i.e., the order of the label vocabulary)\n",
    "    label_embeddings = torch.stack(\n",
    "        # All label vocabularies are the same, so we can use the train dataset\n",
    "        [torch.tensor(label_embedding_dict[label]) for label in test_dataset.label_vocabulary])\n",
    "else:\n",
    "    raise ValueError(\"Label embeddings not found.\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "label_embeddings = label_embeddings.to(device)\n",
    "sequence_to_embeddings_dict = {seq: embedding.to(\n",
    "    device) for seq, embedding in sequence_to_embeddings_dict.items()}\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ProTCL(protein_embedding_dim=config['PROTEIN_EMBEDDING_DIM'],\n",
    "                  label_embedding_dim=config['LABEL_EMBEDDING_DIM'],\n",
    "                  latent_dim=config['LATENT_EMBEDDING_DIM'],\n",
    "                  temperature=config['TEMPERATURE'],\n",
    "                  sequence_to_embeddings_dict=sequence_to_embeddings_dict,\n",
    "                  ordered_label_embeddings=label_embeddings).to(device)\n",
    "\n",
    "# Load the model weights if LOAD_MODEL_PATH is provided and exists\n",
    "if LOAD_MODEL_PATH is not None and os.path.exists(LOAD_MODEL_PATH):\n",
    "    logging.info(f\"Loading model weights from {LOAD_MODEL_PATH}...\")\n",
    "    model.load_state_dict(torch.load(LOAD_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTesting complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Close the W&B run\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m: avg_test_loss, \u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m: average_f1_score,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: average_precision, \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: average_recall, \u001b[39m\"\u001b[39m\u001b[39mcoverage\u001b[39m\u001b[39m\"\u001b[39m: coverage})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreinit_wrapper\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m wandb\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou must call wandb.init() before \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    " ####### TESTING LOOP #######\n",
    "logging.info(\"Starting testing...\")\n",
    "model.eval()\n",
    "\n",
    "# Initialize metrics\n",
    "total_test_loss = 0\n",
    "at_least_one_positive_pred = torch.tensor(0, dtype=int).to(\n",
    "    device)  # seqs with at least one positive label prediction\n",
    "n = torch.tensor(0, dtype=int).to(device)\n",
    "seqwise_precision = BinaryPrecision(threshold=config['DECISION_TH'],\n",
    "                                    multidim_average='samplewise').to(device)\n",
    "seqwise_recall = BinaryRecall(threshold=config['DECISION_TH'],\n",
    "                              multidim_average='samplewise').to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        # Unpack the test batch\n",
    "        sequences, sequence_lengths, labels = test_batch\n",
    "\n",
    "        # Convert labels to floats and move to GPU, if available\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        P_e, L_e, true_labels = model(sequences, labels)\n",
    "\n",
    "        # Compute test loss for the batch\n",
    "        test_loss = contrastive_loss(P_e, L_e, model.t, true_labels)\n",
    "\n",
    "        # Accumulate the total test loss\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # Compute cosine similarities for zero-shot classification\n",
    "        logits = torch.mm(P_e, L_e.t()) * torch.exp(model.t)\n",
    "\n",
    "        # Apply sigmoid to get the probabilities for multi-label classification\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Throw error\n",
    "        # throw_error = True\n",
    "        # if throw_error:\n",
    "        #     raise ValueError(\"Error!\")\n",
    "\n",
    "        # Update metrics\n",
    "        at_least_one_positive_pred += (probabilities >\n",
    "                                       config['DECISION_TH']).any(axis=1).sum()\n",
    "        n += true_labels.size(0)\n",
    "        seqwise_precision(probabilities, true_labels)\n",
    "        seqwise_recall(probabilities, true_labels)\n",
    "\n",
    "# Compute average test loss\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "# Compute average precision, recall, coverage, and F1 score\n",
    "average_precision = seqwise_precision.compute().sum()/at_least_one_positive_pred\n",
    "average_recall = seqwise_recall.compute().mean()\n",
    "average_f1_score = 2*average_precision * \\\n",
    "    average_recall/(average_precision+average_recall)\n",
    "coverage = at_least_one_positive_pred/n\n",
    "\n",
    "logging.info(\n",
    "    f\"Test Loss: {avg_test_loss}, F1 Score: {average_f1_score}, Precision: {average_precision}, Recall: {average_recall}, Coverage: {coverage}\")\n",
    "\n",
    "logging.info(\"Testing complete.\")\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, F1 Score: {average_f1_score}, Precision: {average_precision}, Recall: {average_recall}, Coverage: {coverage}\")\n",
    "\n",
    "# Close the W&B run\n",
    "# wandb.log({\"test_loss\": avg_test_loss, \"f1_score\": average_f1_score,\n",
    "#           \"precision\": average_precision, \"recall\": average_recall, \"coverage\": coverage})\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
