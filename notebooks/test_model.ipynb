{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Test the ProteinDataset\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle, read_json\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data paths\n",
    "FULL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/full_GO.fasta'\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "SEQUENCE_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/sequence_vocab.json'\n",
    "SEQUENCE_ID_MAP_PATH = '/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_id_map.pkl'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_PubMedBERT_label_embeddings.pt\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Loaded 522607 sequences from /home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/full_GO.fasta.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Create a ProteinDataset\n",
    "paths = {\n",
    "    'data_path': FULL_DATA_PATH,\n",
    "    'amino_acid_vocabulary_path': AMINO_ACID_VOCAB_PATH,\n",
    "    'label_vocabulary_path': GO_LABEL_VOCAB_PATH,\n",
    "    'sequence_id_vocabulary_path': '/home/ncorley/protein/ProteinFunctions/data/vocabularies/sequence_id_vocab.json',\n",
    "    'sequence_id_map_path': SEQUENCE_ID_MAP_PATH,\n",
    "}\n",
    "full_datset = ProteinDataset(paths=paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 522607\n"
     ]
    }
   ],
   "source": [
    "# Get the length of the dataset\n",
    "print(f\"Length of the dataset: {len(full_datset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a data laoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "loader = DataLoader(\n",
    "            full_datset,\n",
    "            batch_size=2,\n",
    "            shuffle=True,\n",
    "            collate_fn=collate_variable_sequence_length,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ncorley/protein/ProteinFunctions/src/data/collators.py\", line 46, in collate_variable_sequence_length\n    return torch.stack(processed_sequence_ids), torch.stack(processed_sequence_onehots), torch.stack(processed_label_ids), torch.stack(processed_label_multihots), torch.stack(processed_sequence_lengths)\n                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [50] at entry 0 and [38] at entry 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb Cell 6\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load one batch from the loader\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m loader:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(batch)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Print shape for each item in batch\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     data\u001b[39m.\u001b[39mreraise()\n\u001b[1;32m   1372\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/anaconda/envs/protein_functions/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 54, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ncorley/protein/ProteinFunctions/src/data/collators.py\", line 46, in collate_variable_sequence_length\n    return torch.stack(processed_sequence_ids), torch.stack(processed_sequence_onehots), torch.stack(processed_label_ids), torch.stack(processed_label_multihots), torch.stack(processed_sequence_lengths)\n                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: stack expects each tensor to be equal size, but got [50] at entry 0 and [38] at entry 1\n"
     ]
    }
   ],
   "source": [
    "# Load one batch from the loader\n",
    "for batch in loader:\n",
    "    print(batch)\n",
    "    # Print shape for each item in batch\n",
    "    for item in batch:\n",
    "        print(item.shape)\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !export ROOT_PATH=\"/home/ncorley/protein/ProteinFunctions\"\n",
    "\n",
    "# ROOT_PATH = os.environ.get('ROOT_PATH', '.')\n",
    "# print(f\"Root path: {ROOT_PATH}\")\n",
    "from src.utils.data import read_pickle, read_json, read_yaml    \n",
    "\n",
    "ROOT_PATH = \"/home/ncorley/protein/ProteinFunctions\"\n",
    "\n",
    "# Load the configuration file\n",
    "config = read_yaml(ROOT_PATH + '/config.yaml')\n",
    "params = config['params']\n",
    "paths = config['relative_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'AMINO_ACID_VOCAB_PATH'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msrc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m ProteinDataset, create_multiple_loaders\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m common_paths \u001b[39m=\u001b[39m {\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mamino_acid_vocabulary_path\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_PATH, paths[\u001b[39m'\u001b[39m\u001b[39mAMINO_ACID_VOCAB_PATH\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlabel_vocabulary_path\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_PATH, paths[\u001b[39m'\u001b[39m\u001b[39mGO_LABEL_VOCAB_PATH\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msequence_id_vocabulary_path\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_PATH, paths[\u001b[39m'\u001b[39m\u001b[39mSEQUENCE_ID_VOCAB_PATH\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39msequence_id_map_path\u001b[39m\u001b[39m'\u001b[39m: os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(ROOT_PATH, paths[\u001b[39m'\u001b[39m\u001b[39mSEQUENCE_ID_MAP_PATH\u001b[39m\u001b[39m'\u001b[39m]),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m paths_list \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcommon_paths,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(paths_list)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'AMINO_ACID_VOCAB_PATH'"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import ProteinDataset, create_multiple_loaders\n",
    "import os\n",
    "\n",
    "common_paths = {\n",
    "    'amino_acid_vocabulary_path': os.path.join(ROOT_PATH, paths['AMINO_ACID_VOCAB_PATH']),\n",
    "    'label_vocabulary_path': os.path.join(ROOT_PATH, paths['GO_LABEL_VOCAB_PATH']),\n",
    "    'sequence_id_vocabulary_path': os.path.join(ROOT_PATH, paths['SEQUENCE_ID_VOCAB_PATH']),\n",
    "    'sequence_id_map_path': os.path.join(ROOT_PATH, paths['SEQUENCE_ID_MAP_PATH']),\n",
    "}\n",
    "\n",
    "paths_list = [\n",
    "    {\n",
    "        **common_paths,\n",
    "        'data_path': os.path.join(ROOT_PATH, data_path)\n",
    "    }\n",
    "    for data_path in [\n",
    "        paths['TRAIN_DATA_PATH'],\n",
    "        paths['VAL_DATA_PATH'],\n",
    "        paths['TEST_DATA_PATH']\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(paths_list)\n",
    "\n",
    "# Load datasets from config file paths; the same vocabulary is used for all datasets\n",
    "train_dataset, val_dataset, test_dataset = ProteinDataset.create_multiple_datasets(paths_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "train_loader, val_loader, test_loader = create_multiple_loaders(\n",
    "    [train_dataset, val_dataset, test_dataset],\n",
    "    [params['TRAIN_BATCH_SIZE'], params['VALIDATION_BATCH_SIZE'],\n",
    "        params['TEST_BATCH_SIZE']],\n",
    "    num_workers=params['NUM_WORKERS'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### EMBEDDINGS ####\n",
    "sequence_embedding_path = '/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_proteinfer_sequence_embeddings.pkl'\n",
    "\n",
    "sequence_embeddings = read_pickle(sequence_embedding_path)\n",
    "\n",
    "sequence_id_map = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_id_map.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a map from unique numeric sequence ID to sequence embedding\n",
    "numeric_id_embedding_map = {sequence_id_map[alphanumeric_id]: embedding for alphanumeric_id, embedding in sequence_embeddings.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the maximum numeric sequence ID (may be different than length)\n",
    "max_id = max(numeric_id_embedding_map.keys())\n",
    "\n",
    "# Get the embedding dimension for the sequence\n",
    "embedding_dim = params['PROTEIN_EMBEDDING_DIM']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create an embedding matrix with zeros for all IDs\n",
    "sequence_embedding_matrix = torch.zeros(max_id + 1, embedding_dim, device=device)\n",
    "\n",
    "# Fill the embedding matrix with the embeddings from the map\n",
    "for numeric_id, numpy_embedding in numeric_id_embedding_map.items():\n",
    "    tensor_embedding = torch.tensor(numpy_embedding, device=device)\n",
    "    sequence_embedding_matrix[numeric_id] = tensor_embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "sequence_embedding_layer = nn.Embedding.from_pretrained(sequence_embedding_matrix, freeze=True).to(device)  # Assuming you want to freeze the embeddings and not train them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1100])\n",
      "tensor([[ 1.0928, -0.0995, -2.6018,  ...,  3.2864,  0.9259,  0.6427],\n",
      "        [-0.1883, -0.4159, -1.2945,  ..., -0.9814,  0.5722,  0.4089],\n",
      "        [-0.8475,  1.3617,  0.5347,  ...,  2.5408, -0.4552, -0.5581]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x1a\\xf7&\\xd7\\xbb\\x9a\\xbbH\\x1c\\xc5\\xcb\\xdd\\xe7t`lgU .\\xf7\\xb0\\x9e\\xecB`\\x85*\\xa2\\n3s\\xe96\\x15ku*', b'\\xaa5\\x9c\\x94sx\\x97\\x01\\xe0\\x95\\x0c\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xcdI\\xcd\\xddVw\\xa4m']\n",
      "Bad pipe message: %s [b\"y\\xbf`C\\xc1v-6\\xab6\\xc2\\xf6\\xd0\\x11B\\xe1\\xfb\\x83\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\"]\n",
      "Bad pipe message: %s [b'd\\x7f\\xb9\\xe0u\\xe8AF-\\xca&\\x0c@\\xd50\\x9br\\xc3\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s', b\"\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\", b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'1\\xc3i@\\xa7\\xf4\\xb5\\xbd\\xd1\\xbe\\x17\\xf8\\x94\\xaf~\\xb3N4\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00', b'\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'!\\x18\\xa0\\xc4\\x1d\"\\xab1^4\\n}\\xb7\\xdcz\\x1c\\xf1Z\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x00']\n",
      "Bad pipe message: %s [b'\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x18\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b\"/k\\x13\\\\$qPQ8\\xdfS'\\xff\\x82c\\x98\\xac[\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\"]\n",
      "Bad pipe message: %s [b\"\\xa1\\xea\\x99\\x1d\\xebr\\x91\\xfc\\x84u)\\x00*\\xb5\\x1b\\xaeL\\x9c\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0\", b'\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'6m\\xca4\\xc3G\\xe0\\x82\\xe4\\xd1\\xe8\\xbb\\x96\\x02W\\x0f.:\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0']\n",
      "Bad pipe message: %s [b\"*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\"]\n"
     ]
    }
   ],
   "source": [
    "sequence_embedding_layer\n",
    "# Create a tensor with numbers 1-10\n",
    "x = torch.arange(1, 4).to(device)\n",
    "\n",
    "sequence_embeddings = sequence_embedding_layer(x)\n",
    "print(sequence_embeddings.shape)\n",
    "print(sequence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now do the same thing for the labels\n",
    "label_embedding_path = '/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_PubMedBERT_label_embeddings.pkl'\n",
    "\n",
    "# Load label embeddings\n",
    "label_embeddings = read_pickle(label_embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id_map = full_datset.label2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_numeric_id_embedding_map = {\n",
    "    label_id_map[k]: v for k, v in label_embeddings.items() if k in label_id_map\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max ID: 32101\n"
     ]
    }
   ],
   "source": [
    " # Get the maximum numeric sequence ID (may be different than length)\n",
    "max_id = max(label_numeric_id_embedding_map.keys())\n",
    "print(f\"Max ID: {max_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an embedding matrix with zeros for all IDs\n",
    "label_embedding_matrix = torch.zeros(\n",
    "    max_id + 1, params['LABEL_EMBEDDING_DIM'], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the embedding matrix with the embeddings from the map\n",
    "for numeric_id, embedding in label_numeric_id_embedding_map.items():\n",
    "    tensor_embedding = torch.tensor(embedding, device=device)\n",
    "    label_embedding_matrix[numeric_id] = tensor_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32102, 768])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_embedding_layer = nn.Embedding.from_pretrained(\n",
    "    label_embedding_matrix, freeze=True).to(device)  # Assuming you want to freeze the embeddings and not train them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3552852/4289621951.py:3: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  label_embedding_matrix = torch.tensor(embeddings, device=device)\n"
     ]
    }
   ],
   "source": [
    "embeddings = label_embeddings['embedding']\n",
    "# Convert the embeddings to a tensor\n",
    "label_embedding_matrix = torch.tensor(embeddings, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GO:0000001    [-0.18025249, -0.02392223, 0.33825943, 0.15572...\n",
       "GO:0000002    [-0.2219825, -0.051336795, 0.33668807, 0.14193...\n",
       "GO:0000003    [-0.2465049, -0.19750126, 0.16020584, 0.038120...\n",
       "GO:0000005    [-0.15026829, 0.14948216, 0.203052, 0.06520696...\n",
       "GO:0000006    [-0.07081436, -0.40411177, 0.14208032, 0.02184...\n",
       "                                    ...                        \n",
       "GO:2001313    [-0.079838775, -0.22622626, 0.097667456, 0.431...\n",
       "GO:2001314    [-0.09654722, -0.2291644, 0.187317, 0.35629278...\n",
       "GO:2001315    [-0.22437151, 0.0012431458, 0.16394448, 0.3430...\n",
       "GO:2001316    [-0.11879057, -0.1450008, 0.21268089, 0.440057...\n",
       "GO:2001317    [-0.12991917, -0.11843379, 0.18660213, 0.36462...\n",
       "Name: embedding, Length: 47401, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embeddings['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([47401, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle, read_json\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label embeddings: torch.Size([6, 2])\n",
      "Labels: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Example sequences and labels\n",
    "sequences = [\"SEQ1\", \"SEQ2\", \"SEQ3\"]\n",
    "labels = torch.tensor([\n",
    "    [1, 0, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Example sequence embeddings\n",
    "sequence_to_embeddings_dict = {\n",
    "    \"SEQ1\": torch.tensor([0.1, 0.2, 0.3]),\n",
    "    \"SEQ2\": torch.tensor([0.4, 0.5, 0.6]),\n",
    "    \"SEQ3\": torch.tensor([0.7, 0.8, 0.9])\n",
    "}\n",
    "\n",
    "# Example label embeddings\n",
    "label_embeddings = torch.tensor([\n",
    "    [0.01, 0.02],\n",
    "    [0.03, 0.04],\n",
    "    [0.05, 0.06], # Embeddings for all-zero columns\n",
    "    [0.07, 0.08],\n",
    "    [0.09, 0.10],  # Embeddings for all-zero columns\n",
    "    [0.11, 0.12]   # Embeddings for all-zero columns\n",
    "])\n",
    "\n",
    "print(\"Label embeddings:\", label_embeddings.shape)\n",
    "print(\"Labels:\", labels.shape)\n",
    "\n",
    "# Constants\n",
    "PROTEIN_EMBEDDING_DIM = 3\n",
    "LABEL_EMBEDDING_DIM = 2\n",
    "LATENT_EMBEDDING_DIM = 2\n",
    "TEMPERATURE = 1.0\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Assuming you're using a GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "label_embeddings = label_embeddings.to(device)\n",
    "sequence_to_embeddings_dict = {seq: embedding.to(device) for seq, embedding in sequence_to_embeddings_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = ProTCL(protein_embedding_dim=PROTEIN_EMBEDDING_DIM, \n",
    "                  label_embedding_dim=LABEL_EMBEDDING_DIM, \n",
    "                  latent_dim=LATENT_EMBEDDING_DIM,\n",
    "                  temperature=TEMPERATURE,\n",
    "                  sequence_to_embeddings_dict=sequence_to_embeddings_dict,\n",
    "                  ordered_label_embeddings=label_embeddings).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "tensor([[-0.8432,  0.5377],\n",
      "        [-0.8763,  0.4817],\n",
      "        [-0.8845,  0.4665]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[-0.7129,  0.7013],\n",
      "        [-0.7731,  0.6343],\n",
      "        [-0.7986,  0.6019]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor([[1., 0., 1.],\n",
      "        [0., 1., 1.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Test the forward method\n",
    "P_e, L_e, target = model(sequences, labels)\n",
    "print(P_e)\n",
    "print(L_e)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6116, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test the contrastive loss function\n",
    "loss = contrastive_loss(P_e, L_e, model.t, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle, read_json\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from ProteinFunctions.src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import datetime\n",
    "from torchmetrics.classification import BinaryPrecision, BinaryRecall\n",
    "\n",
    "# Get the root path from the environment variable\n",
    "ROOT_PATH = os.environ.get('ROOT_PATH', '.')  # Default to current directory if ROOT_PATH is not set\n",
    "\n",
    "# Load the configuration file\n",
    "config = read_json(os.path.join(ROOT_PATH, 'config.json'))\n",
    "params = config['params']\n",
    "relative_paths = config['relative_paths']\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(filename='train.log', filemode='w',\n",
    "                    format='%(asctime)s %(levelname)-4s %(message)s',\n",
    "                    level=logging.INFO,\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "# Initialize new run in W&B\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "logging.info(\n",
    "    f\"################## {timestamp} RUNNING test_ProConNet.py ##################\")\n",
    "\n",
    "config={\n",
    "    \"LEARNING_RATE\": 0.001,\n",
    "    \"TEMPERATURE\": 0.07,\n",
    "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
    "    \"LABEL_EMBEDDING_DIM\": 768,\n",
    "    \"LATENT_EMBEDDING_DIM\": 934,\n",
    "    \"NUM_EPOCHS\": 10,\n",
    "    \"BATCH_SIZE\": 1000,\n",
    "    \"DECISION_TH\": 0.88\n",
    "}\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/label_embeddings.pk1\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pk1\"\n",
    "\n",
    "# Directory to save models\n",
    "OUTPUT_MODEL_DIR = \"/home/ncorley/protein/ProteinFunctions/models/ProConNet\"\n",
    "\n",
    "# Model to load\n",
    "LOAD_MODEL_PATH = \"/home/ncorley/protein/ProteinFunctions/models/ProConNet/2023-09-05_23-28-25_best_Pro_ConNet.pth\"\n",
    "\n",
    "# Test only\n",
    "TRAIN_MODEL = False\n",
    "TEST_MODEL = True\n",
    "\n",
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load datasets\n",
    "test_dataset = ProteinDataset(data_path=TEST_DATA_PATH,\n",
    "                              sequence_vocabulary_path=AMINO_ACID_VOCAB_PATH,\n",
    "                              label_vocabulary_path=GO_LABEL_VOCAB_PATH)\n",
    "\n",
    "# Define data loaders\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset, batch_size=config.BATCH_SIZE, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=10, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)\n",
    "\n",
    "# Load sequence embeddings\n",
    "if os.path.exists(SEQUENCE_EMBEDDING_PATH):\n",
    "    sequence_to_embeddings_dict_raw = read_pickle(SEQUENCE_EMBEDDING_PATH)[\n",
    "        ['sequence', 'embedding']].set_index('sequence')['embedding'].to_dict()\n",
    "\n",
    "    # Convert embeddings in the dictionary to tensors\n",
    "    sequence_to_embeddings_dict = {seq: torch.tensor(\n",
    "        embedding) for seq, embedding in sequence_to_embeddings_dict_raw.items()}\n",
    "else:\n",
    "    raise ValueError(\"Sequence embeddings not found.\")\n",
    "\n",
    "# Load label embeddings (in the same order as the label vocabulary)\n",
    "if os.path.exists(LABEL_EMBEDDING_PATH):\n",
    "    # Load a dictionary mapping GO IDs to embeddings\n",
    "    label_embedding_dict = read_pickle(LABEL_EMBEDDING_PATH)[\n",
    "        ['go_id', 'embedding']].set_index('go_id')['embedding'].to_dict()\n",
    "    # Create a tensor of embeddings in the correct order (i.e., the order of the label vocabulary)\n",
    "    label_embeddings = torch.stack(\n",
    "        # All label vocabularies are the same, so we can use the train dataset\n",
    "        [torch.tensor(label_embedding_dict[label]) for label in test_dataset.label_vocabulary])\n",
    "else:\n",
    "    raise ValueError(\"Label embeddings not found.\")\n",
    "\n",
    "# Move tensors to GPU\n",
    "label_embeddings = label_embeddings.to(device)\n",
    "sequence_to_embeddings_dict = {seq: embedding.to(\n",
    "    device) for seq, embedding in sequence_to_embeddings_dict.items()}\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = ProTCL(protein_embedding_dim=config['PROTEIN_EMBEDDING_DIM'],\n",
    "                  label_embedding_dim=config['LABEL_EMBEDDING_DIM'],\n",
    "                  latent_dim=config['LATENT_EMBEDDING_DIM'],\n",
    "                  temperature=config['TEMPERATURE'],\n",
    "                  sequence_to_embeddings_dict=sequence_to_embeddings_dict,\n",
    "                  ordered_label_embeddings=label_embeddings).to(device)\n",
    "\n",
    "# Load the model weights if LOAD_MODEL_PATH is provided and exists\n",
    "if LOAD_MODEL_PATH is not None and os.path.exists(LOAD_MODEL_PATH):\n",
    "    logging.info(f\"Loading model weights from {LOAD_MODEL_PATH}...\")\n",
    "    model.load_state_dict(torch.load(LOAD_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1000, shuffle=True, collate_fn=collate_variable_sequence_length, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.log()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=63'>64</a>\u001b[0m logging\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mTesting complete.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# Close the W&B run\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=66'>67</a>\u001b[0m wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mtest_loss\u001b[39m\u001b[39m\"\u001b[39m: avg_test_loss, \u001b[39m\"\u001b[39m\u001b[39mf1_score\u001b[39m\u001b[39m\"\u001b[39m: average_f1_score,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=67'>68</a>\u001b[0m          \u001b[39m\"\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m\"\u001b[39m: average_precision, \u001b[39m\"\u001b[39m\u001b[39mrecall\u001b[39m\u001b[39m\"\u001b[39m: average_recall, \u001b[39m\"\u001b[39m\u001b[39mcoverage\u001b[39m\u001b[39m\"\u001b[39m: coverage})\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/test_model.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=68'>69</a>\u001b[0m wandb\u001b[39m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.11/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreinit_wrapper\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[39mraise\u001b[39;00m wandb\u001b[39m.\u001b[39mError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou must call wandb.init() before \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m()\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"
     ]
    }
   ],
   "source": [
    " ####### TESTING LOOP #######\n",
    "logging.info(\"Starting testing...\")\n",
    "model.eval()\n",
    "\n",
    "# Initialize metrics\n",
    "total_test_loss = 0\n",
    "at_least_one_positive_pred = torch.tensor(0, dtype=int).to(\n",
    "    device)  # seqs with at least one positive label prediction\n",
    "n = torch.tensor(0, dtype=int).to(device)\n",
    "seqwise_precision = BinaryPrecision(threshold=config['DECISION_TH'],\n",
    "                                    multidim_average='samplewise').to(device)\n",
    "seqwise_recall = BinaryRecall(threshold=config['DECISION_TH'],\n",
    "                              multidim_average='samplewise').to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_batch in test_loader:\n",
    "        # Unpack the test batch\n",
    "        sequences, sequence_lengths, labels = test_batch\n",
    "\n",
    "        # Convert labels to floats and move to GPU, if available\n",
    "        labels = labels.float().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        P_e, L_e, true_labels = model(sequences, labels)\n",
    "\n",
    "        # Compute test loss for the batch\n",
    "        test_loss = contrastive_loss(P_e, L_e, model.t, true_labels)\n",
    "\n",
    "        # Accumulate the total test loss\n",
    "        total_test_loss += test_loss.item()\n",
    "\n",
    "        # Compute cosine similarities for zero-shot classification\n",
    "        logits = torch.mm(P_e, L_e.t()) * torch.exp(model.t)\n",
    "\n",
    "        # Apply sigmoid to get the probabilities for multi-label classification\n",
    "        probabilities = torch.sigmoid(logits)\n",
    "\n",
    "        # Throw error\n",
    "        # throw_error = True\n",
    "        # if throw_error:\n",
    "        #     raise ValueError(\"Error!\")\n",
    "\n",
    "        # Update metrics\n",
    "        at_least_one_positive_pred += (probabilities >\n",
    "                                       config['DECISION_TH']).any(axis=1).sum()\n",
    "        n += true_labels.size(0)\n",
    "        seqwise_precision(probabilities, true_labels)\n",
    "        seqwise_recall(probabilities, true_labels)\n",
    "\n",
    "# Compute average test loss\n",
    "avg_test_loss = total_test_loss / len(test_loader)\n",
    "\n",
    "# Compute average precision, recall, coverage, and F1 score\n",
    "average_precision = seqwise_precision.compute().sum()/at_least_one_positive_pred\n",
    "average_recall = seqwise_recall.compute().mean()\n",
    "average_f1_score = 2*average_precision * \\\n",
    "    average_recall/(average_precision+average_recall)\n",
    "coverage = at_least_one_positive_pred/n\n",
    "\n",
    "logging.info(\n",
    "    f\"Test Loss: {avg_test_loss}, F1 Score: {average_f1_score}, Precision: {average_precision}, Recall: {average_recall}, Coverage: {coverage}\")\n",
    "\n",
    "logging.info(\"Testing complete.\")\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss}, F1 Score: {average_f1_score}, Precision: {average_precision}, Recall: {average_recall}, Coverage: {coverage}\")\n",
    "\n",
    "# Close the W&B run\n",
    "# wandb.log({\"test_loss\": avg_test_loss, \"f1_score\": average_f1_score,\n",
    "#           \"precision\": average_precision, \"recall\": average_recall, \"coverage\": coverage})\n",
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
