{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Embedding Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "import logging\n",
    "from src.utils.data import (\n",
    "    seed_everything,\n",
    "    read_pickle,\n",
    ")\n",
    "from src.data.datasets import ProteinDataset, calculate_pos_weight, create_multiple_loaders\n",
    "from src.models.ProTCLTrainer import ProTCLTrainer\n",
    "from src.models.ProTCL import ProTCL\n",
    "from src.models.protein_encoders import ProteInfer\n",
    "from src.utils.evaluation import EvalMetrics, save_evaluation_results\n",
    "from src.utils.models import count_parameters_by_layer, get_label_embeddings\n",
    "from src.utils.configs import get_setup\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "from src.utils.main_utils import get_or_generate_vocabularies,  get_or_generate_label_embeddings, get_or_generate_sequence_embeddings, validate_arguments\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/ncorley/protein/ProteinFunctions/outputs/logs/2023-10-08_14-01-25_Test.log and console...\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"ROOT_PATH\"] = \"/home/ncorley/protein/ProteinFunctions\"\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path='/home/ncorley/protein/ProteinFunctions/configs/base_config.yaml',\n",
    "    run_name=\"Test\",\n",
    "    overrides=None,\n",
    "    train_path_name=\"TRAIN_DATA_PATH\",\n",
    "    val_path_name=\"VAL_DATA_PATH\",\n",
    "    test_paths_names=[\"TEST_DATA_PATH\"],\n",
    "    amlt=False,\n",
    "    is_master=True,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-08 14:01:28 PDT INFO Loaded amino_acid_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-10-08 14:01:28 PDT INFO Loaded GO_label_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-10-08 14:01:28 PDT INFO Loaded GO_label_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-10-08 14:01:29 PDT INFO Loaded sequence_id_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/sequence_id_vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    config['dataset_paths_list'],\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    vocabularies=vocabularies,\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    \"train\": params[\"TRAIN_LABEL_SAMPLE_SIZE\"],\n",
    "    \"validation\": params[\"VALIDATION_LABEL_SAMPLE_SIZE\"],\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    num_workers=params[\"NUM_WORKERS\"],\n",
    "    world_size=1,\n",
    "    rank=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "label2int = datasets[list(datasets.keys())[0]][0].label2int\n",
    "int2label = datasets[list(datasets.keys())[0]][0].int2label\n",
    "label_annotation_map = datasets[list(datasets.keys())[\n",
    "    0]][0].label_annotation_map\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "# # Generate all label embeddings upfront, if not training the label encoder\n",
    "# label_embedding_matrix = None\n",
    "# if not params[\"TRAIN_LABEL_ENCODER\"]:\n",
    "#     # Create a list of text labels\n",
    "#     sorted_labels = sorted(\n",
    "#         vocabularies[\"GO_label_vocab\"], key=lambda x: label2int[x])\n",
    "#     label_annotations = [label_annotation_map[label_id]\n",
    "#                             for label_id in sorted_labels]\n",
    "#     label_encoder = label_encoder.to(device)\n",
    "#     paths[\"LABEL_EMBEDDING_PATH\"] = None  \n",
    "#     label_embedding_matrix = get_or_generate_label_embeddings(\n",
    "#         paths,\n",
    "#         device,\n",
    "#         label_annotations,\n",
    "#         label_tokenizer,\n",
    "#         label_encoder,\n",
    "#         logger,\n",
    "#         LABEL_BATCH_SIZE_LIMIT_NO_GRAD=params[\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"]\n",
    "#     )\n",
    "#     # Move the label encoder to CPU\n",
    "#     label_encoder = label_encoder.cpu()\n",
    "\n",
    "#     import time\n",
    "\n",
    "# FILEPATH: /home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb\n",
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "sorted_labels = sorted(\n",
    "        vocabularies[\"GO_label_vocab\"], key=lambda x: label2int[x])\n",
    "label_annotations = [label_annotation_map[label_id]\n",
    "                            for label_id in sorted_labels]\n",
    "print(len(label_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = \"GO:0000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import tokenize_labels\n",
    "label_annotations = [label_annotation_map[label_id]\n",
    "                            for label_id in sorted_labels]\n",
    "sampled_label_annotations = label_annotations[:2000]\n",
    "print(len(sampled_label_annotations))\n",
    "tokenized_labels = tokenize_labels(sampled_label_annotations, label_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.00 GB (0.00%)\n",
      "Memory reserved: 0.00 GB (0.00%)\n",
      "Total memory: 79.10 GB\n",
      "Memory allocated: 1.30 GB (1.64%)\n",
      "Memory reserved: 1.31 GB (1.65%)\n",
      "Total memory: 79.10 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")\n",
    "\n",
    "tokenized_labels[\"input_ids\"] = tokenized_labels[\"input_ids\"].to(device)\n",
    "tokenized_labels[\"attention_mask\"] = tokenized_labels[\"attention_mask\"].to(device)\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1024])\n",
      "Memory usage of result variable:  7.8125  MB\n",
      "Memory allocated: 1.31 GB (1.66%)\n",
      "Memory reserved: 1.64 GB (2.08%)\n",
      "Total memory: 79.10 GB\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "from torch.cuda.amp import autocast\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# Clear the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        label_embeddings = label_encoder(**tokenized_labels).last_hidden_state.mean(dim=1)\n",
    "end_time = time.time()\n",
    "\n",
    "print(label_embeddings.shape)\n",
    "# print label_embeddings memory usage\n",
    "print(\"Memory usage of result variable: \", label_embeddings.element_size() *\n",
    "        label_embeddings.nelement() / (1024 * 1024), \" MB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")\n",
    "\n",
    "# 38 seconds for 20k labels w/ batch size of 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "# Move model to GPU\n",
    "label_encoder = label_encoder.to(device)\n",
    "sampled_label_annotations = label_annotations[:32000]\n",
    "with torch.no_grad():\n",
    "    embeddings = generate_label_embeddings_from_text(\n",
    "        sampled_label_annotations,\n",
    "        label_tokenizer,\n",
    "        label_encoder,\n",
    "        batch_size_limit=1500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:03:37 PDT INFO Loaded sequence embeddings from /home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer/frozen_proteinfer_sequence_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize ProteInfer\n",
    "sequence_encoder = ProteInfer.from_pretrained(\n",
    "    weights_path=paths[\"PROTEINFER_WEIGHTS_PATH\"],\n",
    "    num_labels=config[\"embed_sequences_params\"][\"PROTEINFER_NUM_LABELS\"],\n",
    "    input_channels=config[\"embed_sequences_params\"][\"INPUT_CHANNELS\"],\n",
    "    output_channels=config[\"embed_sequences_params\"][\"OUTPUT_CHANNELS\"],\n",
    "    kernel_size=config[\"embed_sequences_params\"][\"KERNEL_SIZE\"],\n",
    "    activation=torch.nn.ReLU,\n",
    "    dilation_base=config[\"embed_sequences_params\"][\"DILATION_BASE\"],\n",
    "    num_resnet_blocks=config[\"embed_sequences_params\"][\"NUM_RESNET_BLOCKS\"],\n",
    "    bottleneck_factor=config[\"embed_sequences_params\"][\"BOTTLENECK_FACTOR\"],\n",
    ")\n",
    "\n",
    "# Generate all sequence embeddings upfront, if not training the sequence encoder\n",
    "sequence_embedding_dict = None\n",
    "if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "    sequence_embedding_dict = get_or_generate_sequence_embeddings(\n",
    "        paths,\n",
    "        device,\n",
    "        sequence_encoder,\n",
    "        datasets,\n",
    "        params,\n",
    "        logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_embedding_dict = read_pickle(paths[\"SEQUENCE_EMBEDDING_PATH\"])\n",
    "label_embedding_matrix = torch.load(paths[\"LABEL_EMBEDDING_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(paths_list, label_tokenizer=label_tokenizer)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING train.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"train\": 2000,\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"validation\": 100,\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GO annotations\n",
    "GO_ANNOTATIONS_PATH_OLD = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "GO_ANNOTATIONS_PATH_NEW = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2023_07_23.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.']\n",
      "OUTPUT:\n",
      "tensor([-1.3425,  0.1925,  0.4544,  ..., -0.0675,  0.1730,  0.8976])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3571145/4001248717.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = [go_annotations.loc[\"GO:0000001\"][0]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BioGptTokenizer, BioGptModel, set_seed\n",
    "\n",
    "label_tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "label_encoder = BioGptModel.from_pretrained(\"microsoft/biogpt\")\n",
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Get the value corresponding to the key \"GO:0000001\" from the df\n",
    "text = [go_annotations.loc[\"GO:0000001\"][0]]\n",
    "print(text)\n",
    "\n",
    "# checkpoint = \"microsoft/biogpt\"\n",
    "# label_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# label_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Print the tokenized label\n",
    "tokens = tokenize_labels(text, label_tokenizer)\n",
    "\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "# Get the label embeddings (average across all tokens of the last hidden state)\n",
    "with torch.no_grad():\n",
    "    mean_hidden_states = get_label_embeddings(\n",
    "        tokens,\n",
    "        label_encoder,\n",
    "        1\n",
    "    )\n",
    "print(\"OUTPUT:\")\n",
    "print(mean_hidden_states[0])\n",
    "# print(output.last_hidden_state.shape)\n",
    "# print(output.last_hidden_state)\n",
    "# tensor([-0.4258, -0.6187, -0.2033,  ..., -0.8992,  0.1572, -0.0666])\n",
    "# tensor([[[-0.1223,  0.0990, -0.9851,  ..., -0.8175,  0.1483, -0.5747],\n",
    "#          [-1.9245,  0.7768,  1.8563,  ..., -0.4368,  0.2228,  0.6075],\n",
    "#          [ 0.7696, -2.7319, -1.4810,  ..., -1.4431,  0.1005, -0.2327]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6444,  0.2435, -0.9323,  ..., -3.6236, -0.5964, -0.8596])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the label embedding matrix\n",
    "path = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\"\n",
    "label_embedding_matrix = torch.load(path)\n",
    "label_embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m paths \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mLABEL_EMBEDDING_PATH_3\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m matrix \u001b[39m=\u001b[39m get_or_generate_label_embeddings(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     paths,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     go_annotations,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     label_tokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     label_encoder,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/main_utils.py:93\u001b[0m, in \u001b[0;36mget_or_generate_label_embeddings\u001b[0;34m(paths, device, label_annotations, label_tokenizer, label_encoder, logger, batch_size_limit)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m logger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mGenerating label embeddings...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m label_embedding_matrix \u001b[39m=\u001b[39m generate_label_embeddings_from_text(\n\u001b[1;32m     94\u001b[0m     label_annotations, label_tokenizer, label_encoder, batch_size_limit)\n\u001b[1;32m     95\u001b[0m save_path \u001b[39m=\u001b[39m prompt_user_for_path(\n\u001b[1;32m     96\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mEnter the full file path to save the label embeddings, or hit enter to continue without saving\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     default_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m save_path:\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/models.py:119\u001b[0m, in \u001b[0;36mgenerate_label_embeddings_from_text\u001b[0;34m(label_annotations, label_tokenizer, label_encoder, batch_size_limit)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_label_embeddings_from_text\u001b[39m(label_annotations, label_tokenizer, label_encoder, batch_size_limit\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenize the labels and generate label embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenized_labels \u001b[39m=\u001b[39m tokenize_labels(label_annotations, label_tokenizer)\n\u001b[1;32m    121\u001b[0m     \u001b[39m# Move to GPU\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     tokenized_labels[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenized_labels[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\n\u001b[1;32m    123\u001b[0m         label_encoder\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/models.py:69\u001b[0m, in \u001b[0;36mtokenize_labels\u001b[0;34m(text, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_labels\u001b[39m(text, tokenizer, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    Tokenize a list of text strings.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m        dict: A dictionary containing tokenized labels as 'input_ids' and 'attention_mask'.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(\n\u001b[1;32m     70\u001b[0m         text, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mmax_length, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2602\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2601\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2602\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2603\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2660\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2660\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2662\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2663\u001b[0m     )\n\u001b[1;32m   2665\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2668\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2669\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from src.utils.main_utils import get_or_generate_label_embeddings\n",
    "import logging\n",
    "paths = {\n",
    "    \"LABEL_EMBEDDING_PATH_3\": \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\"\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get a list of all label text from the df\n",
    "\n",
    "\n",
    "matrix = get_or_generate_label_embeddings(\n",
    "    paths,\n",
    "    device,\n",
    "    go_annotations,\n",
    "    label_tokenizer,\n",
    "    label_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6444,  0.2435, -0.9323,  ..., -3.6236, -0.5964, -0.8596],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Embedding Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input_ids': tensor([[    2,    56,    80,    21,   396,    10,     6,   370,     5,    49,\n",
      "           718,     4],\n",
      "        [    2,    39,    80,    22,  1603,    14,   151,    10,    49,   718,\n",
      "             4,     1],\n",
      "        [    2,    39,    80,  1295,    16,   360,  2829,     4,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,    56,    80, 11648,    10,   360,  2829,     4,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from src.utils.models import tokenize_labels\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "\n",
    "label_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "1. This protein is involved in the process of cell signaling.\n",
      "2. A protein that plays a role in cell signaling.\n",
      "3. A protein responsible for muscle contraction.\n",
      "4. This protein aids in muscle contraction.\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "       1   2   3   4\n",
      " 1 |1.00 0.86 0.58 0.71 \n",
      " 2 |0.86 1.00 0.73 0.74 \n",
      " 3 |0.58 0.73 1.00 0.89 \n",
      " 4 |0.71 0.74 0.89 1.00 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def print_cosine_similarity_matrix(protein_descriptions, label_tokenizer, label_encoder, batch_size_limit=2):\n",
    "    \"\"\"\n",
    "    Compute and print the cosine similarity matrix for a list of protein descriptions.\n",
    "\n",
    "    Args:\n",
    "    - protein_descriptions (list): List of protein descriptions.\n",
    "    - label_tokenizer (Tokenizer): Tokenizer for the descriptions.\n",
    "    - label_encoder (torch.nn.Module): Pretrained model to generate embeddings.\n",
    "    - batch_size_limit (int): Batch size limit for embedding generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get embeddings for the tokenized descriptions\n",
    "    with torch.no_grad():\n",
    "        embeddings = generate_label_embeddings_from_text(protein_descriptions, label_tokenizer, label_encoder, batch_size_limit=batch_size_limit)\n",
    "\n",
    "    # Convert the embeddings to numpy for cosine similarity computation\n",
    "    embeddings_matrix = embeddings.cpu().numpy()\n",
    "\n",
    "    # Compute the cosine similarity between the embeddings\n",
    "    cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "    # Print as a matrix\n",
    "    # Print the key for protein descriptions\n",
    "    print(\"Key:\")\n",
    "    for idx, desc in enumerate(protein_descriptions, 1):\n",
    "        print(f\"{idx}. {desc}\")\n",
    "    print(\"\\nCosine Similarity Matrix:\")\n",
    "\n",
    "    # Print the matrix header\n",
    "    print(\"    \", end=\"\")\n",
    "    for i in range(len(protein_descriptions)):\n",
    "        print(f\"{i+1:4}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    # Print the matrix\n",
    "    for i in range(len(protein_descriptions)):\n",
    "        print(f\"{i+1:2} |\", end=\"\")\n",
    "        for j in range(len(protein_descriptions)):\n",
    "            print(f\"{cosine_sim[i][j]:.2f} \", end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "1. Facilitates intracellular communication via signaling molecules.\n",
      "2. Key player in transmitting signals within cells.\n",
      "3. Corrects DNA sequence mismatches.\n",
      "4. Essential for DNA helix repair.\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "       1   2   3   4\n",
      " 1 |1.00 0.76 0.57 0.62 \n",
      " 2 |0.76 1.00 0.54 0.63 \n",
      " 3 |0.57 0.54 1.00 0.79 \n",
      " 4 |0.62 0.63 0.79 1.00 \n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "import torch\n",
    "\n",
    "# protein_descriptions = [\n",
    "#     \"This protein is involved in the process of cell signaling.\",\n",
    "#     \"A protein that plays a role in cell signaling.\",\n",
    "#     \"A protein responsible for muscle contraction.\",\n",
    "#     \"This protein aids in muscle contraction.\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# Raw protein descriptions\n",
    "raw_protein_descriptions = [\n",
    "    \"This gene product is crucial for intracellular communication mediated by signaling molecules.\",\n",
    "    \"The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\",\n",
    "    \"The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\",\n",
    "    \"This gene's product is essential for repairing inaccuracies in the DNA helix.\"\n",
    "]\n",
    "\n",
    "# GPT-optimized protein descriptions\n",
    "\"\"\"\n",
    "Hello, Assistant! I'm providing you with a protein GO annotation description from SwissProt. Your task is to condense this description in a way that, when embedded with a pretrained language model, it will create a distinct and separate representation in the embedding latent space. \n",
    "- This condensed form should retain the core essence and meaning of the original description but be more concise and distinctive. \n",
    "- The most relevant and distinctive pieces of information should come at the beginning of the sentence, if possible. \n",
    "- These are all protein descriptions, so the condensed form should be a sentence that describes the function of the protein. You do not need to include the word \"protein\" in your condensed form.\n",
    "- Do not use non-critical words like \"pivotal\" or \"essential\"\n",
    "Here are the descriptions. Create one summary for each description below.\n",
    "###\n",
    "DESCRIPTION #1: This gene product is crucial for intracellular communication mediated by signaling molecules.\n",
    "DESCRIPTION #2: The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\n",
    "DESCRIPTION #3: The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\n",
    "DESCRIPTION #4: This gene's product is essential for repairing inaccuracies in the DNA helix.\n",
    "###\n",
    "Please provide the condensed form of all descriptions.\n",
    "\n",
    "Here is an example:\n",
    "INPUT_PROTEIN_DESCRIPTION: Any process that activates or increases the frequency, rate or extent of AIM2 inflammasome complex assembly.\n",
    "MODEL_OUTPUT: AIM2 inflammasome complex assembly activator or accelerator.\n",
    "\"\"\"\n",
    "\n",
    "# Raw protein descriptions\n",
    "processed_protein_descriptions = [\n",
    "    \"Facilitates intracellular communication via signaling molecules.\",\n",
    "    \"Key player in transmitting signals within cells.\",\n",
    "    \"Corrects DNA sequence mismatches.\",\n",
    "    \"Essential for DNA helix repair.\"\n",
    "]\n",
    "\n",
    "print_cosine_similarity_matrix(processed_protein_descriptions, label_tokenizer, label_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
