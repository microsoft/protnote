{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Embedding Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "import logging\n",
    "from src.utils.data import (\n",
    "    seed_everything,\n",
    "    read_pickle,\n",
    ")\n",
    "from src.data.datasets import ProteinDataset, calculate_pos_weight, create_multiple_loaders\n",
    "from src.models.ProTCLTrainer import ProTCLTrainer\n",
    "from src.models.ProTCL import ProTCL\n",
    "from src.models.protein_encoders import ProteInfer\n",
    "from src.utils.evaluation import EvalMetrics, save_evaluation_results\n",
    "from src.utils.models import count_parameters_by_layer, get_label_embeddings\n",
    "from src.utils.configs import get_setup\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "from src.utils.main_utils import get_or_generate_vocabularies,  get_or_generate_label_embeddings, get_or_generate_sequence_embeddings, validate_arguments\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST EMBEDDING DISTRIBUTIONS #\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "label_embedding_path = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\"\n",
    "sequence_embedding_path = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import torch\n",
    "\n",
    "def read_pickle(file_path: str):\n",
    "    with open(file_path, \"rb\") as p:\n",
    "        item = pickle.load(p)\n",
    "    return item\n",
    "\n",
    "# Load the embeddings\n",
    "label_embeddings = torch.load('/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl')\n",
    "sequence_embeddings = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_proteinfer_sequence_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Embeddings:\n",
      "Means: [-1.00499    -0.19348931  0.07630822 ...  0.11141217  0.4188767\n",
      "  0.54361475]\n",
      "Variances: [0.39269638 0.42584723 0.27725244 ... 0.2506793  0.388853   0.31982717]\n",
      "Standard Deviations: [0.6266549  0.6525697  0.5265477  ... 0.50067884 0.6235808  0.5655326 ]\n",
      "\n",
      "Sequence Embeddings:\n",
      "Means: [-0.20901579 -0.04067202 -0.08404726 ...  1.6288333   0.0574921\n",
      "  0.1346581 ]\n",
      "Variances: [0.62677294 0.84094894 0.7439483  ... 2.7185543  0.7942297  0.89135265]\n",
      "Standard Deviations: [0.79168993 0.91703266 0.8625244  ... 1.6488038  0.89119565 0.94411474]\n",
      "\n",
      "Label Embeddings:\n",
      "Mean of Means: -0.005486639\n",
      "Mean of Variances: 0.36615825\n",
      "Mean of Standard Deviations: 0.5966133\n",
      "\n",
      "Sequence Embeddings:\n",
      "Mean of Means: -0.06397131\n",
      "Mean of Variances: 1.160447\n",
      "Mean of Standard Deviations: 0.9797921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xdd\\xad\\xb3\\x110v\\x90r\\x8d\\x83\\xa2\\xa8%\\xffq\\xa2P\\xf3 \\xf4\\xc6\\x18\\xe7\\xc3\\xd5/\\xf4]\\xe5\\xe5\\x97\\x02\\x0e\\xcc~\\xa1\\xd8r\\xcfa!\\xb2\\x8d\\x98\\x7f\\xab\\xd9\\xfd%\\xe9\\x82\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19']\n",
      "Bad pipe message: %s [b'\\xba|\\x1d\\xe6Zk\\tZ\\xd7;\\xf4>%\\x80\\xa7Un\\x91 n\\xa3m\\xf98m\\x7f\\xe7l\\x17\\xdfja\\x06\\xfdBV\\xe3]\\xbboq\\xd2\\xb8\\xd1eN\\x15\\xefdE\\x0b\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xdd\\xc18w\\xb3\\x1e\\xfbd\\x82\\xafl\\xc1c\\xb3\\xdd\\xdb}6\\xd4\\x1a\\x00\\x11\\xb1']\n",
      "Bad pipe message: %s [b\"\\x1d \\xd9\\x8eZ\\xdb\\xb1\\xb2|\\xda\\x98\\xb1\\xcd\\xb2\\x1b\\x94%\\xde\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\", b'\\x03\\x03\\x02\\x03\\x03', b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b\"T\\x18\\xe7.\\x96\\xdf\\x02 f1*g_\\xd3\\xffg\\x85g\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\"]\n",
      "Bad pipe message: %s [b'0\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'T\\x8d\\xdc\\xf7x\\x93\\xa2\\x98\\x8bb\\x94l\\xc9\\tU!\\xda\\xde\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b']\n",
      "Bad pipe message: %s [b'jV\\xb0\\x19%\\x9d\\x9d\\xd0\\xb7\\x17\\x16\\xa0\\xf2G\\x9a\\xbf3\\x1f\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00']\n",
      "Bad pipe message: %s [b'\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\x0f\\x00\\x01\\x01\\x15\\x03\\x01']\n",
      "Bad pipe message: %s [b'\\x7f\\xd9nD\\xa8\\xab\\xd5\\xe9%7\\xd2\\x89W)\\x89\\xc8R\\xeb\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008']\n",
      "Bad pipe message: %s [b\"9\\x8a+\\xb9\\xf7lH\\x16nA:\\x02\\xd6\\xe2j\\xc5\\xdbd\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\"]\n",
      "Bad pipe message: %s [b'\\xbcO\\x16\\xdd&o3<!OC\\xc9<t\\xa4\\xacc\\r P+\\xc1s;\\xfa-\\xfa\\x80_\\x10\\xa9\\xdc\\xe7|:\\xd9\\xff\\xc6\\xfct\\xb5\\xe0[h\\xdcG\\xb6|\\x18\\xfa\\x10\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04', b'', b'']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xb3\\xda\\xba\\xe4\\xd1r\\xceD\\xe3\\xc6\\xd1\\xd6_\\xea0Q2\\xf9\\xb0Xw\\x1a']\n",
      "Bad pipe message: %s [b'\\xba\\xb9\\xf4\\x15~\\x03\\xef\\xd5\\x9c.\\x82\\x1f97F\\xb9\\xdaG 0\\xca\\x06\\xf5*TC*\\x92\\xbd\\n\\xb9\\x9fk\\xfd\\x8fA\\xb0\\xc3\\x9c\\xad\\x89l\\xb6YHOZ\\x1d\\x9c\\xac7\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 \\xe6u\\xd0\\xab\\x82\\xa6\\xf4\\xf0\\x0c\\xd5\\xd7\\xfc\\xd0*<\\x0c\\xaf\\x8co\\xaa\\xbd\\xf95r\\xdd\\xba\\x92\\xb9R\\xbfL\\x05']\n",
      "Bad pipe message: %s [b'K~\\xfe\\xd61O\\x10c:\\xae^\\x96\\xa6]\\xd8\\x97h\\x07\\x00\\x00|\\xc0,']\n",
      "Bad pipe message: %s [b\"\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\"]\n",
      "Bad pipe message: %s [b'\\x08\\x08\\t\\x08\\n\\x08']\n",
      "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\x1e)LwQ\\xf0\\xc9\\x88\\xe2\\xfc+\\xd1\\xe0\"\\x17\\x97\\xb6\\xb6\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10']\n",
      "Bad pipe message: %s [b\"\\xc4\\x19u\\xe5\\xd1\\x18\\x9c\\xcf\\xbb\\xfa\\xefz'z\\xa8\\xd08\\r\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\", b'\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a']\n",
      "Bad pipe message: %s [b\"\\xe5R\\xb0\\xb3\\xc9\\x04\\xf8nw\\xe3l\\x9d0\\x19\\xa4\\xabs\\x1e\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\"]\n",
      "Bad pipe message: %s [b'\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 ']\n",
      "Bad pipe message: %s [b\"\\xb3K\\xd8\\x16\\xf3(;&^F\\x98\\x18\\xad\\xf6\\rH\\xae\\x17\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00\", b'B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0']\n",
      "Bad pipe message: %s [b'\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00']\n",
      "Bad pipe message: %s [b'\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0']\n",
      "Bad pipe message: %s [b'\\x01\\x00;\\x00\\x02\\x00\\x01\\x00\\xff\\x02']\n",
      "Bad pipe message: %s [b'']\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate statistical measures\n",
    "def calculate_statistics(embeddings):\n",
    "    embeddings_array = np.array(embeddings)\n",
    "    means = np.mean(embeddings_array, axis=0)\n",
    "    variances = np.var(embeddings_array, axis=0)\n",
    "    std_devs = np.std(embeddings_array, axis=0)\n",
    "    return means, variances, std_devs\n",
    "\n",
    "# Function to plot distributions\n",
    "def plot_distributions(embeddings1, embeddings2):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(embeddings1.ravel(), label='Label Embeddings')\n",
    "    sns.kdeplot(embeddings2.ravel(), label='Sequence Embeddings')\n",
    "    plt.legend()\n",
    "    plt.title('Distribution of Embedding Vectors')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "\n",
    "# Calculate statistics\n",
    "label_means, label_variances, label_std_devs = calculate_statistics(label_embeddings)\n",
    "sequence_means, sequence_variances, sequence_std_devs = calculate_statistics(sequence_embeddings)\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Label Embeddings:\")\n",
    "print(\"Means:\", label_means)\n",
    "print(\"Variances:\", label_variances)\n",
    "print(\"Standard Deviations:\", label_std_devs)\n",
    "print(\"\\nSequence Embeddings:\")\n",
    "print(\"Means:\", sequence_means)\n",
    "print(\"Variances:\", sequence_variances)\n",
    "print(\"Standard Deviations:\", sequence_std_devs)\n",
    "\n",
    "# Print the mean of the means, variances, and standard deviations\n",
    "print(\"\\nLabel Embeddings:\")\n",
    "print(\"Mean of Means:\", np.mean(label_means))\n",
    "print(\"Mean of Variances:\", np.mean(label_variances))\n",
    "print(\"Mean of Standard Deviations:\", np.mean(label_std_devs))\n",
    "print(\"\\nSequence Embeddings:\")\n",
    "print(\"Mean of Means:\", np.mean(sequence_means))\n",
    "print(\"Mean of Variances:\", np.mean(sequence_variances))\n",
    "print(\"Mean of Standard Deviations:\", np.mean(sequence_std_devs))\n",
    "\n",
    "# Plot the distributions\n",
    "# plot_distributions(label_embeddings, sequence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/ncorley/protein/ProteinFunctions/outputs/logs/2023-10-08_14-01-25_Test.log and console...\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"ROOT_PATH\"] = \"/home/ncorley/protein/ProteinFunctions\"\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path='/home/ncorley/protein/ProteinFunctions/configs/base_config.yaml',\n",
    "    run_name=\"Test\",\n",
    "    overrides=None,\n",
    "    train_path_name=\"TRAIN_DATA_PATH\",\n",
    "    val_path_name=\"VAL_DATA_PATH\",\n",
    "    test_paths_names=[\"TEST_DATA_PATH\"],\n",
    "    amlt=False,\n",
    "    is_master=True,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-08 14:01:28 PDT INFO Loaded amino_acid_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-10-08 14:01:28 PDT INFO Loaded GO_label_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-10-08 14:01:28 PDT INFO Loaded GO_label_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-10-08 14:01:29 PDT INFO Loaded sequence_id_vocab vocabulary from /home/ncorley/protein/ProteinFunctions/data/vocabularies/proteinfer/sequence_id_vocab.json\n"
     ]
    }
   ],
   "source": [
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    config['dataset_paths_list'],\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    vocabularies=vocabularies,\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    \"train\": params[\"TRAIN_LABEL_SAMPLE_SIZE\"],\n",
    "    \"validation\": params[\"VALIDATION_LABEL_SAMPLE_SIZE\"],\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    num_workers=params[\"NUM_WORKERS\"],\n",
    "    world_size=1,\n",
    "    rank=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "label2int = datasets[list(datasets.keys())[0]][0].label2int\n",
    "int2label = datasets[list(datasets.keys())[0]][0].int2label\n",
    "label_annotation_map = datasets[list(datasets.keys())[\n",
    "    0]][0].label_annotation_map\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Load label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "# # Generate all label embeddings upfront, if not training the label encoder\n",
    "# label_embedding_matrix = None\n",
    "# if not params[\"TRAIN_LABEL_ENCODER\"]:\n",
    "#     # Create a list of text labels\n",
    "#     sorted_labels = sorted(\n",
    "#         vocabularies[\"GO_label_vocab\"], key=lambda x: label2int[x])\n",
    "#     label_annotations = [label_annotation_map[label_id]\n",
    "#                             for label_id in sorted_labels]\n",
    "#     label_encoder = label_encoder.to(device)\n",
    "#     paths[\"LABEL_EMBEDDING_PATH\"] = None  \n",
    "#     label_embedding_matrix = get_or_generate_label_embeddings(\n",
    "#         paths,\n",
    "#         device,\n",
    "#         label_annotations,\n",
    "#         label_tokenizer,\n",
    "#         label_encoder,\n",
    "#         logger,\n",
    "#         LABEL_BATCH_SIZE_LIMIT_NO_GRAD=params[\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"]\n",
    "#     )\n",
    "#     # Move the label encoder to CPU\n",
    "#     label_encoder = label_encoder.cpu()\n",
    "\n",
    "#     import time\n",
    "\n",
    "# FILEPATH: /home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb\n",
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "sorted_labels = sorted(\n",
    "        vocabularies[\"GO_label_vocab\"], key=lambda x: label2int[x])\n",
    "label_annotations = [label_annotation_map[label_id]\n",
    "                            for label_id in sorted_labels]\n",
    "print(len(label_annotations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_id = \"GO:0000001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import tokenize_labels\n",
    "label_annotations = [label_annotation_map[label_id]\n",
    "                            for label_id in sorted_labels]\n",
    "sampled_label_annotations = label_annotations[:2000]\n",
    "print(len(sampled_label_annotations))\n",
    "tokenized_labels = tokenize_labels(sampled_label_annotations, label_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated: 0.00 GB (0.00%)\n",
      "Memory reserved: 0.00 GB (0.00%)\n",
      "Total memory: 79.10 GB\n",
      "Memory allocated: 1.30 GB (1.64%)\n",
      "Memory reserved: 1.31 GB (1.65%)\n",
      "Total memory: 79.10 GB\n"
     ]
    }
   ],
   "source": [
    "# Clear the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")\n",
    "\n",
    "tokenized_labels[\"input_ids\"] = tokenized_labels[\"input_ids\"].to(device)\n",
    "tokenized_labels[\"attention_mask\"] = tokenized_labels[\"attention_mask\"].to(device)\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1024])\n",
      "Memory usage of result variable:  7.8125  MB\n",
      "Memory allocated: 1.31 GB (1.66%)\n",
      "Memory reserved: 1.64 GB (2.08%)\n",
      "Total memory: 79.10 GB\n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "from torch.cuda.amp import autocast\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "# Clear the cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "with torch.no_grad():\n",
    "    with autocast():\n",
    "        label_embeddings = label_encoder(**tokenized_labels).last_hidden_state.mean(dim=1)\n",
    "end_time = time.time()\n",
    "\n",
    "print(label_embeddings.shape)\n",
    "# print label_embeddings memory usage\n",
    "print(\"Memory usage of result variable: \", label_embeddings.element_size() *\n",
    "        label_embeddings.nelement() / (1024 * 1024), \" MB\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Print memory usage\n",
    "print(f\"Memory allocated: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Memory reserved: {torch.cuda.memory_reserved() / 1024 ** 3:.2f} GB ({100 * torch.cuda.memory_reserved() / torch.cuda.get_device_properties(0).total_memory:.2f}%)\")\n",
    "print(f\"Total memory: {torch.cuda.get_device_properties(0).total_memory / 1024 ** 3:.2f} GB\")\n",
    "\n",
    "# 38 seconds for 20k labels w/ batch size of 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "# Move model to GPU\n",
    "label_encoder = label_encoder.to(device)\n",
    "sampled_label_annotations = label_annotations[:32000]\n",
    "with torch.no_grad():\n",
    "    embeddings = generate_label_embeddings_from_text(\n",
    "        sampled_label_annotations,\n",
    "        label_tokenizer,\n",
    "        label_encoder,\n",
    "        batch_size_limit=1500,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-04 15:03:37 PDT INFO Loaded sequence embeddings from /home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer/frozen_proteinfer_sequence_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Initialize ProteInfer\n",
    "sequence_encoder = ProteInfer.from_pretrained(\n",
    "    weights_path=paths[\"PROTEINFER_WEIGHTS_PATH\"],\n",
    "    num_labels=config[\"embed_sequences_params\"][\"PROTEINFER_NUM_LABELS\"],\n",
    "    input_channels=config[\"embed_sequences_params\"][\"INPUT_CHANNELS\"],\n",
    "    output_channels=config[\"embed_sequences_params\"][\"OUTPUT_CHANNELS\"],\n",
    "    kernel_size=config[\"embed_sequences_params\"][\"KERNEL_SIZE\"],\n",
    "    activation=torch.nn.ReLU,\n",
    "    dilation_base=config[\"embed_sequences_params\"][\"DILATION_BASE\"],\n",
    "    num_resnet_blocks=config[\"embed_sequences_params\"][\"NUM_RESNET_BLOCKS\"],\n",
    "    bottleneck_factor=config[\"embed_sequences_params\"][\"BOTTLENECK_FACTOR\"],\n",
    ")\n",
    "\n",
    "# Generate all sequence embeddings upfront, if not training the sequence encoder\n",
    "sequence_embedding_dict = None\n",
    "if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "    sequence_embedding_dict = get_or_generate_sequence_embeddings(\n",
    "        paths,\n",
    "        device,\n",
    "        sequence_encoder,\n",
    "        datasets,\n",
    "        params,\n",
    "        logger,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_embedding_dict = read_pickle(paths[\"SEQUENCE_EMBEDDING_PATH\"])\n",
    "label_embedding_matrix = torch.load(paths[\"LABEL_EMBEDDING_PATH\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(paths_list, label_tokenizer=label_tokenizer)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING train.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"train\": 2000,\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"validation\": 100,\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GO annotations\n",
    "GO_ANNOTATIONS_PATH_OLD = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "GO_ANNOTATIONS_PATH_NEW = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2023_07_23.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH_OLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.']\n",
      "OUTPUT:\n",
      "tensor([-1.3425,  0.1925,  0.4544,  ..., -0.0675,  0.1730,  0.8976])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3571145/4001248717.py:11: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  text = [go_annotations.loc[\"GO:0000001\"][0]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BioGptTokenizer, BioGptModel, set_seed\n",
    "\n",
    "label_tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "label_encoder = BioGptModel.from_pretrained(\"microsoft/biogpt\")\n",
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Get the value corresponding to the key \"GO:0000001\" from the df\n",
    "text = [go_annotations.loc[\"GO:0000001\"][0]]\n",
    "print(text)\n",
    "\n",
    "# checkpoint = \"microsoft/biogpt\"\n",
    "# label_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# label_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Print the tokenized label\n",
    "tokens = tokenize_labels(text, label_tokenizer)\n",
    "\n",
    "input_ids = tokens[\"input_ids\"]\n",
    "attention_mask = tokens[\"attention_mask\"]\n",
    "# Get the label embeddings (average across all tokens of the last hidden state)\n",
    "with torch.no_grad():\n",
    "    mean_hidden_states = get_label_embeddings(\n",
    "        tokens,\n",
    "        label_encoder,\n",
    "        1\n",
    "    )\n",
    "print(\"OUTPUT:\")\n",
    "print(mean_hidden_states[0])\n",
    "# print(output.last_hidden_state.shape)\n",
    "# print(output.last_hidden_state)\n",
    "# tensor([-0.4258, -0.6187, -0.2033,  ..., -0.8992,  0.1572, -0.0666])\n",
    "# tensor([[[-0.1223,  0.0990, -0.9851,  ..., -0.8175,  0.1483, -0.5747],\n",
    "#          [-1.9245,  0.7768,  1.8563,  ..., -0.4368,  0.2228,  0.6075],\n",
    "#          [ 0.7696, -2.7319, -1.4810,  ..., -1.4431,  0.1005, -0.2327]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6444,  0.2435, -0.9323,  ..., -3.6236, -0.5964, -0.8596])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the label embedding matrix\n",
    "path = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\"\n",
    "label_embedding_matrix = torch.load(path)\n",
    "label_embedding_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb Cell 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m paths \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mLABEL_EMBEDDING_PATH_3\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m matrix \u001b[39m=\u001b[39m get_or_generate_label_embeddings(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     paths,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     device,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     go_annotations,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     label_tokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     label_encoder,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m )\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/main_utils.py:93\u001b[0m, in \u001b[0;36mget_or_generate_label_embeddings\u001b[0;34m(paths, device, label_annotations, label_tokenizer, label_encoder, logger, batch_size_limit)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39mif\u001b[39;00m logger \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mGenerating label embeddings...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m label_embedding_matrix \u001b[39m=\u001b[39m generate_label_embeddings_from_text(\n\u001b[1;32m     94\u001b[0m     label_annotations, label_tokenizer, label_encoder, batch_size_limit)\n\u001b[1;32m     95\u001b[0m save_path \u001b[39m=\u001b[39m prompt_user_for_path(\n\u001b[1;32m     96\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mEnter the full file path to save the label embeddings, or hit enter to continue without saving\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     97\u001b[0m     default_path\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n\u001b[1;32m     98\u001b[0m )\n\u001b[1;32m     99\u001b[0m \u001b[39mif\u001b[39;00m save_path:\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/models.py:119\u001b[0m, in \u001b[0;36mgenerate_label_embeddings_from_text\u001b[0;34m(label_annotations, label_tokenizer, label_encoder, batch_size_limit)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_label_embeddings_from_text\u001b[39m(label_annotations, label_tokenizer, label_encoder, batch_size_limit\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m):\n\u001b[1;32m    118\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Tokenize the labels and generate label embeddings.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     tokenized_labels \u001b[39m=\u001b[39m tokenize_labels(label_annotations, label_tokenizer)\n\u001b[1;32m    121\u001b[0m     \u001b[39m# Move to GPU\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     tokenized_labels[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m tokenized_labels[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(\n\u001b[1;32m    123\u001b[0m         label_encoder\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/protein/ProteinFunctions/src/utils/models.py:69\u001b[0m, in \u001b[0;36mtokenize_labels\u001b[0;34m(text, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_labels\u001b[39m(text, tokenizer, max_length\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m):\n\u001b[1;32m     59\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    Tokenize a list of text strings.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39m        dict: A dictionary containing tokenized labels as 'input_ids' and 'attention_mask'.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer(\n\u001b[1;32m     70\u001b[0m         text, padding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlongest\u001b[39;49m\u001b[39m'\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, max_length\u001b[39m=\u001b[39;49mmax_length, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     71\u001b[0m     )\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2602\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2600\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2601\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2602\u001b[0m     encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_one(text\u001b[39m=\u001b[39;49mtext, text_pair\u001b[39m=\u001b[39;49mtext_pair, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs)\n\u001b[1;32m   2603\u001b[0m \u001b[39mif\u001b[39;00m text_target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2604\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2660\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2657\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2660\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2661\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2662\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2663\u001b[0m     )\n\u001b[1;32m   2665\u001b[0m \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2666\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2668\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2669\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "from src.utils.main_utils import get_or_generate_label_embeddings\n",
    "import logging\n",
    "paths = {\n",
    "    \"LABEL_EMBEDDING_PATH_3\": \"/home/ncorley/protein/ProteinFunctions/data/embeddings/frozen_BioGPT_label_embeddings.pkl\"\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Get a list of all label text from the df\n",
    "\n",
    "\n",
    "matrix = get_or_generate_label_embeddings(\n",
    "    paths,\n",
    "    device,\n",
    "    go_annotations,\n",
    "    label_tokenizer,\n",
    "    label_encoder,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6444,  0.2435, -0.9323,  ..., -3.6236, -0.5964, -0.8596],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing LoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BioGptModel(\n",
      "  (embed_tokens): Embedding(42384, 1024, padding_idx=1)\n",
      "  (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-23): 24 x BioGptDecoderLayer(\n",
      "      (self_attn): BioGptAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "47401\n",
      "The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, set_seed\n",
    "import torch\n",
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "from src.utils.data import read_pickle\n",
    "import copy\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# Load label encoder\n",
    "model = AutoModel.from_pretrained(\n",
    "    \"microsoft/biogpt\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "print(model)\n",
    "\n",
    "def print_model_size(model):\n",
    "    mem_params = sum([param.nelement()*param.element_size() for param in model.parameters()])\n",
    "    mem_bufs = sum([buf.nelement()*buf.element_size() for buf in model.buffers()])\n",
    "    mem = mem_params + mem_bufs # in bytes\n",
    "    # Print in GB to 2 decimal places\n",
    "    print(\"Model size:\", round(mem / 1024**3, 2), \"GB\")\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Load GO annotations\n",
    "GO_ANNOTATIONS_PATH_OLD = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "GO_ANNOTATIONS_PATH_NEW = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2023_07_23.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH_OLD)\n",
    "\n",
    "# Get first 1000 labels\n",
    "text = go_annotations.iloc[:, 0].tolist()\n",
    "print(len(text))\n",
    "print(text[0])\n",
    "\n",
    "tokenized_labels = tokenizer(\n",
    "    text, padding='longest', truncation=True, max_length=512, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Move to GPU\n",
    "tokenized_labels[\"input_ids\"] = tokenized_labels[\"input_ids\"].to(\n",
    "    'cuda')\n",
    "tokenized_labels[\"attention_mask\"] = tokenized_labels[\"attention_mask\"].to(\n",
    "    'cuda')\n",
    "\n",
    "# Move model to gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to('cuda')\n",
    "\n",
    "# Copy original model\n",
    "original_model = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_hidden_states(last_hidden_states, attention_mask):\n",
    "    \"\"\"Compute the mean of the last hidden state for only the relevant tokens.\"\"\"\n",
    "    # Compute the number of relevant tokens for each sequence\n",
    "    num_relevant_tokens = attention_mask.sum(dim=1, keepdim=True)\n",
    "    # Mask the last_hidden_state tensor and compute the sum\n",
    "    sum_hidden_states = (last_hidden_states *\n",
    "                         attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    "    # Compute the mean of the last hidden state\n",
    "    return sum_hidden_states / num_relevant_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 346763264 || all params: 346763264 || trainable%: 100.0\n",
      "Model size: 0.08339691162109375 GB\n",
      "Last hidden states size: 37.00778388977051 GB\n",
      "Result size: -19.01560688018799 GB\n",
      "tensor([[-1.3432,  0.1918,  0.4550,  ..., -0.0691,  0.1732,  0.8977],\n",
      "        [-0.5855,  0.0037,  0.9944,  ...,  0.7357,  1.3796,  1.2025],\n",
      "        [-0.4864, -0.2691,  0.1813,  ..., -0.3580,  0.8959,  0.8707],\n",
      "        ...,\n",
      "        [-1.9372,  1.0084,  0.4448,  ...,  0.6840, -0.2855,  0.2953],\n",
      "        [-1.5028,  0.3953, -0.2451,  ...,  0.4273,  0.1786, -0.1587],\n",
      "        [-1.1314,  0.0936, -1.0735,  ...,  0.7942, -0.4111, -0.1751]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Max memory usage: 57.664806842803955 GB\n",
      "Memory allocated: 38.39514446258545 GB\n"
     ]
    }
   ],
   "source": [
    "# WITHOUT LORA\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "import gc\n",
    "\n",
    "limit = 100\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Note the size of the model \n",
    "a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "model.to('cpu')\n",
    "b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "print(\"Model size:\", a - b, \"GB\")\n",
    "model.to('cuda')\n",
    "\n",
    "with autocast():\n",
    "\n",
    "    # Note the size of the outputs, including their activations and gradients\n",
    "    a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "\n",
    "    last_hidden_states = model(\n",
    "        input_ids=tokenized_labels[\"input_ids\"][:limit],\n",
    "        attention_mask=tokenized_labels[\"attention_mask\"][:limit]\n",
    "    ).last_hidden_state\n",
    "\n",
    "    b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    print(\"Last hidden states size:\", b - a, \"GB\")\n",
    "\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    # Compute only the mean of the last hidden states\n",
    "    a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    result = compute_mean_hidden_states(last_hidden_states, tokenized_labels[\"attention_mask\"][:limit])\n",
    "    b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    print(\"Result size:\", b - a, \"GB\")\n",
    "\n",
    "    # Delete the last hidden states\n",
    "    del last_hidden_states\n",
    " \n",
    "print(result)\n",
    "# del last_hidden_states\n",
    "max_usage = torch.cuda.max_memory_allocated(device=device) / 1024 ** 3\n",
    "print(\"Max memory usage:\", max_usage, \"GB\")\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated(device=device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 346763264 || all params: 346763264 || trainable%: 100.0\n",
      "trainable params: 1179648 || all params: 347942912 || trainable%: 0.3390349276607767\n",
      "PeftModel(\n",
      "  (base_model): LoraModel(\n",
      "    (model): BioGptModel(\n",
      "      (embed_tokens): Embedding(42384, 1024, padding_idx=1)\n",
      "      (embed_positions): BioGptLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x BioGptDecoderLayer(\n",
      "          (self_attn): BioGptAttention(\n",
      "            (k_proj): Linear(\n",
      "              in_features=1024, out_features=1024, bias=True\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (v_proj): Linear(\n",
      "              in_features=1024, out_features=1024, bias=True\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (q_proj): Linear(\n",
      "              in_features=1024, out_features=1024, bias=True\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (default): Identity()\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (default): Linear(in_features=1024, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "            )\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# WITH LORA\n",
    "from torch import nn\n",
    "\n",
    "print_trainable_parameters(original_model)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model \n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    # lora_alpha=8,\n",
    "    target_modules=[\"k_proj\", \"v_proj\"], # Also target fc1, fc2 if we wanted to\n",
    "    # lora_dropout=0.05,\n",
    "    # bias=\"none\",\n",
    "    # task_type=\"FEATURE_EXTRACTION\"\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "optimizer = torch.optim.Adam(lora_model.parameters(), lr=0.00003)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1179648 || all params: 347942912 || trainable%: 0.3390349276607767\n",
      "Model size: 0.09365081787109375 GB\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 292.00 MiB (GPU 0; 79.10 GiB total capacity; 74.59 GiB already allocated; 264.38 MiB free; 77.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb Cell 25\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m a \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmemory_allocated(device\u001b[39m=\u001b[39mdevice) \u001b[39m/\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m last_hidden_states \u001b[39m=\u001b[39m lora_model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mtokenized_labels[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m][:limit],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mtokenized_labels[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m][:limit]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m )\u001b[39m.\u001b[39mlast_hidden_state\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m b \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mmemory_allocated(device\u001b[39m=\u001b[39mdevice) \u001b[39m/\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X54sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLast hidden states size:\u001b[39m\u001b[39m\"\u001b[39m, b \u001b[39m-\u001b[39m a, \u001b[39m\"\u001b[39m\u001b[39mGB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/peft/peft_model.py:442\u001b[0m, in \u001b[0;36mPeftModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any):\n\u001b[1;32m    439\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m    Forward pass of the model.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_base_model()(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:605\u001b[0m, in \u001b[0;36mBioGptModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    597\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    598\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    599\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    603\u001b[0m     )\n\u001b[1;32m    604\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    606\u001b[0m         hidden_states,\n\u001b[1;32m    607\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    608\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    609\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    610\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    611\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    612\u001b[0m     )\n\u001b[1;32m    614\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    616\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:322\u001b[0m, in \u001b[0;36mBioGptDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    320\u001b[0m self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[39m# add present self-attn cache to positions 1,2 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    323\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    324\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    325\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    326\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    327\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    328\u001b[0m )\n\u001b[1;32m    329\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    330\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions/lib/python3.10/site-packages/transformers/models/biogpt/modeling_biogpt.py:223\u001b[0m, in \u001b[0;36mBioGptAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[39mif\u001b[39;00m attention_mask\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39m1\u001b[39m, tgt_len, src_len):\n\u001b[1;32m    220\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    221\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention mask should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz,\u001b[39m \u001b[39m\u001b[39m1\u001b[39m,\u001b[39m \u001b[39mtgt_len,\u001b[39m \u001b[39msrc_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is \u001b[39m\u001b[39m{\u001b[39;00mattention_mask\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         )\n\u001b[0;32m--> 223\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39;49mview(bsz, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, tgt_len, src_len) \u001b[39m+\u001b[39;49m attention_mask\n\u001b[1;32m    224\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights\u001b[39m.\u001b[39mview(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len)\n\u001b[1;32m    226\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(attn_weights, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 292.00 MiB (GPU 0; 79.10 GiB total capacity; 74.59 GiB already allocated; 264.38 MiB free; 77.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# WITH LORA\n",
    "\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "limit = 100\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "# Note the size of the model \n",
    "a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "lora_model.to('cpu')\n",
    "b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "print(\"Model size:\", a - b, \"GB\")\n",
    "lora_model.to('cuda')\n",
    "\n",
    "with autocast():\n",
    "    # Note the size of the outputs, including their activations and gradients\n",
    "    a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    last_hidden_states = lora_model(\n",
    "        input_ids=tokenized_labels[\"input_ids\"][:limit],\n",
    "        attention_mask=tokenized_labels[\"attention_mask\"][:limit]\n",
    "    ).last_hidden_state\n",
    "    b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    print(\"Last hidden states size:\", b - a, \"GB\")\n",
    "\n",
    "    # Compute only the mean of the last hidden states\n",
    "    a = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    result = compute_mean_hidden_states(last_hidden_states, tokenized_labels[\"attention_mask\"][:limit])\n",
    "    b = torch.cuda.memory_allocated(device=device) / 1024 ** 3\n",
    "    print(\"Result size:\", b - a, \"GB\")\n",
    "\n",
    "    # Delete the last hidden states\n",
    "    del last_hidden_states\n",
    " \n",
    "print(result)\n",
    "# del last_hidden_states\n",
    "max_usage = torch.cuda.max_memory_allocated(device=device) / 1024 ** 3\n",
    "print(\"Max memory usage:\", max_usage, \"GB\")\n",
    "print(\"Memory allocated:\", torch.cuda.memory_allocated(device=device) / 1024 ** 3, \"GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.65 GB\n",
      "trainable params: 196608 || all params: 346959872 || trainable%: 0.05666591899134664\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'get_max_memory' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb Cell 28\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m print_trainable_parameters(lora_model)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mget_max_memory(lora_model)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m GB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Btunnel/home/ncorley/protein/ProteinFunctions/notebooks/datasets.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mget_max_memory(lora_model)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m GB\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_max_memory' is not defined"
     ]
    }
   ],
   "source": [
    "# Print which parameters require gradients, and how many parameters are in the model\n",
    "print_model_size(lora_model)\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(f\"{get_max_memory(lora_model):.2f} GB\")\n",
    "print(f\"{get_max_memory(lora_model):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "The distribution of mitochondria, including the mitochondrial genome, into daughter cells after mitosis or meiosis, mediated by interactions between mitochondria and the cytoskeleton.\n",
      "Label embeddings device: cuda:0\n",
      "Label annotations device: cuda:0\n",
      "Label attention mask device: cuda:0\n",
      "Label embeddings memory usage: 40.234375 MB\n",
      "GPU memory usage: 11138.88427734375 MB\n",
      "GPU reserved memory: 13604.0 MB\n",
      "Peak bytes requirement:  13175.6904296875  MB\n",
      "trainable params: 98304 || all params: 346861568 || trainable%: 0.028340989336702763\n",
      "torch.Size([100, 1024])\n",
      "tensor([-1.3431,  0.1927,  0.4538,  ..., -0.0671,  0.1733,  0.8978],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor(0.0012, grad_fn=<MeanBackward1>)\n",
      "Max allocated (GB): 12.8668851852417\n"
     ]
    }
   ],
   "source": [
    "text_subset_lora = text[:100]\n",
    "\n",
    "print(len(text_subset_lora))\n",
    "print(text_subset_lora[0])\n",
    "lora_model.eval()\n",
    "embeddings = generate_label_embeddings_from_text(\n",
    "    text_subset_lora,\n",
    "    tokenizer,\n",
    "    lora_model,\n",
    "    batch_size_limit=1000,\n",
    ")\n",
    "\n",
    "print_trainable_parameters(lora_model)\n",
    "    \n",
    "print(embeddings.shape)\n",
    "print(embeddings[0])\n",
    "# Print mean of first embedding\n",
    "print(embeddings[0].mean(dim=0))\n",
    "# Print max GPU usage\n",
    "print(\"Max allocated (GB):\", torch.cuda.max_memory_allocated(device=device) / 1024 ** 3)\n",
    "\n",
    "# tensor([-1.3431,  0.1927,  0.4538,  ..., -0.0671,  0.1733,  0.8978],\n",
    "#        grad_fn=<SelectBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Embedding Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "{'input_ids': tensor([[    2,    56,    80,    21,   396,    10,     6,   370,     5,    49,\n",
      "           718,     4],\n",
      "        [    2,    39,    80,    22,  1603,    14,   151,    10,    49,   718,\n",
      "             4,     1],\n",
      "        [    2,    39,    80,  1295,    16,   360,  2829,     4,     1,     1,\n",
      "             1,     1],\n",
      "        [    2,    56,    80, 11648,    10,   360,  2829,     4,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from src.utils.models import tokenize_labels\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "\n",
    "label_encoder = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "1. This protein is involved in the process of cell signaling.\n",
      "2. A protein that plays a role in cell signaling.\n",
      "3. A protein responsible for muscle contraction.\n",
      "4. This protein aids in muscle contraction.\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "       1   2   3   4\n",
      " 1 |1.00 0.86 0.58 0.71 \n",
      " 2 |0.86 1.00 0.73 0.74 \n",
      " 3 |0.58 0.73 1.00 0.89 \n",
      " 4 |0.71 0.74 0.89 1.00 \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def print_cosine_similarity_matrix(protein_descriptions, label_tokenizer, label_encoder, batch_size_limit=2):\n",
    "    \"\"\"\n",
    "    Compute and print the cosine similarity matrix for a list of protein descriptions.\n",
    "\n",
    "    Args:\n",
    "    - protein_descriptions (list): List of protein descriptions.\n",
    "    - label_tokenizer (Tokenizer): Tokenizer for the descriptions.\n",
    "    - label_encoder (torch.nn.Module): Pretrained model to generate embeddings.\n",
    "    - batch_size_limit (int): Batch size limit for embedding generation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get embeddings for the tokenized descriptions\n",
    "    with torch.no_grad():\n",
    "        embeddings = generate_label_embeddings_from_text(protein_descriptions, label_tokenizer, label_encoder, batch_size_limit=batch_size_limit)\n",
    "\n",
    "    # Convert the embeddings to numpy for cosine similarity computation\n",
    "    embeddings_matrix = embeddings.cpu().numpy()\n",
    "\n",
    "    # Compute the cosine similarity between the embeddings\n",
    "    cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "    # Print as a matrix\n",
    "    # Print the key for protein descriptions\n",
    "    print(\"Key:\")\n",
    "    for idx, desc in enumerate(protein_descriptions, 1):\n",
    "        print(f\"{idx}. {desc}\")\n",
    "    print(\"\\nCosine Similarity Matrix:\")\n",
    "\n",
    "    # Print the matrix header\n",
    "    print(\"    \", end=\"\")\n",
    "    for i in range(len(protein_descriptions)):\n",
    "        print(f\"{i+1:4}\", end=\"\")\n",
    "    print()\n",
    "\n",
    "    # Print the matrix\n",
    "    for i in range(len(protein_descriptions)):\n",
    "        print(f\"{i+1:2} |\", end=\"\")\n",
    "        for j in range(len(protein_descriptions)):\n",
    "            print(f\"{cosine_sim[i][j]:.2f} \", end=\"\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key:\n",
      "1. Facilitates intracellular communication via signaling molecules.\n",
      "2. Key player in transmitting signals within cells.\n",
      "3. Corrects DNA sequence mismatches.\n",
      "4. Essential for DNA helix repair.\n",
      "\n",
      "Cosine Similarity Matrix:\n",
      "       1   2   3   4\n",
      " 1 |1.00 0.76 0.57 0.62 \n",
      " 2 |0.76 1.00 0.54 0.63 \n",
      " 3 |0.57 0.54 1.00 0.79 \n",
      " 4 |0.62 0.63 0.79 1.00 \n"
     ]
    }
   ],
   "source": [
    "from src.utils.models import generate_label_embeddings_from_text\n",
    "import torch\n",
    "\n",
    "# protein_descriptions = [\n",
    "#     \"This protein is involved in the process of cell signaling.\",\n",
    "#     \"A protein that plays a role in cell signaling.\",\n",
    "#     \"A protein responsible for muscle contraction.\",\n",
    "#     \"This protein aids in muscle contraction.\"\n",
    "# ]\n",
    "\n",
    "\n",
    "# Raw protein descriptions\n",
    "raw_protein_descriptions = [\n",
    "    \"This gene product is crucial for intracellular communication mediated by signaling molecules.\",\n",
    "    \"The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\",\n",
    "    \"The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\",\n",
    "    \"This gene's product is essential for repairing inaccuracies in the DNA helix.\"\n",
    "]\n",
    "\n",
    "# GPT-optimized protein descriptions\n",
    "\"\"\"\n",
    "Hello, Assistant! I'm providing you with a protein GO annotation description from SwissProt. Your task is to condense this description in a way that, when embedded with a pretrained language model, it will create a distinct and separate representation in the embedding latent space. \n",
    "- This condensed form should retain the core essence and meaning of the original description but be more concise and distinctive. \n",
    "- The most relevant and distinctive pieces of information should come at the beginning of the sentence, if possible. \n",
    "- These are all protein descriptions, so the condensed form should be a sentence that describes the function of the protein. You do not need to include the word \"protein\" in your condensed form.\n",
    "- Do not use non-critical words like \"pivotal\" or \"essential\"\n",
    "Here are the descriptions. Create one summary for each description below.\n",
    "###\n",
    "DESCRIPTION #1: This gene product is crucial for intracellular communication mediated by signaling molecules.\n",
    "DESCRIPTION #2: The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\n",
    "DESCRIPTION #3: The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\n",
    "DESCRIPTION #4: This gene's product is essential for repairing inaccuracies in the DNA helix.\n",
    "###\n",
    "Please provide the condensed form of all descriptions.\n",
    "\n",
    "Here is an example:\n",
    "INPUT_PROTEIN_DESCRIPTION: Any process that activates or increases the frequency, rate or extent of AIM2 inflammasome complex assembly.\n",
    "MODEL_OUTPUT: AIM2 inflammasome complex assembly activator or accelerator.\n",
    "\"\"\"\n",
    "\n",
    "# Raw protein descriptions\n",
    "processed_protein_descriptions = [\n",
    "    \"Facilitates intracellular communication via signaling molecules.\",\n",
    "    \"Key player in transmitting signals within cells.\",\n",
    "    \"Corrects DNA sequence mismatches.\",\n",
    "    \"Essential for DNA helix repair.\"\n",
    "]\n",
    "\n",
    "print_cosine_similarity_matrix(processed_protein_descriptions, label_tokenizer, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 800/800 [00:00<00:00, 5.75MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 4.92G/4.92G [00:40<00:00, 122MB/s] \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name_or_path = \"bigscience/mt0-large\"\n",
    "tokenizer_name_or_path = \"bigscience/mt0-large\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
