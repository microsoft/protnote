{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "os.chdir(str(curdir.parent.absolute()))\n",
    "curdir = Path(os.getcwd())\n",
    "\n",
    "from src.utils.data import (\n",
    "    load_model,\n",
    "    seed_everything,\n",
    "    log_gpu_memory_usage\n",
    ")\n",
    "from src.utils.main_utils import get_or_generate_vocabularies,  get_or_generate_label_embeddings, get_or_generate_sequence_embeddings, validate_arguments\n",
    "from src.data.datasets import ProteinDataset, calculate_pos_weight, create_multiple_loaders, calculate_label_weights\n",
    "from src.models.ProTCLTrainer import ProTCLTrainer\n",
    "from src.models.ProTCL import ProTCL\n",
    "from src.models.protein_encoders import ProteInfer\n",
    "from src.utils.evaluation import EvalMetrics\n",
    "from src.utils.models import count_parameters_by_layer, sigmoid_bias_from_prob,load_checkpoint\n",
    "from src.utils.configs import get_setup\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "import mlflow\n",
    "import loralib as lora\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-16 10:06:14 PST INFO Logging to ./outputs/logs/2023-12-16_10-06-14_Test.log and console...\n",
      "2023-12-16 10:06:14 PST INFO Using device: cuda:0\n",
      "2023-12-16 10:06:14 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 32,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"GRID_SAMPLER_LABEL_BATCH_SIZE\": 1000,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.0003,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 3,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 3,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"PROJECTION_HEAD_NUM_LAYERS\": 3,\n",
      "    \"PROJECTION_HEAD_HIDDEN_DIM_SCALE_FACTOR\": 2,\n",
      "    \"FEATURE_FUSION\": \"concatenation\",\n",
      "    \"LABEL_EMBEDDING_POOLING_METHOD\": \"mean\",\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 4,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"FocalLoss\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"SUPCON_TEMP\": 0.07,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\": 0,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 1,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": false\n",
      "}\n",
      "2023-12-16 10:06:19 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-12-16 10:06:19 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-12-16 10:06:19 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SETUP ###\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if master process\n",
    "is_master = True\n",
    "config = \"configs/base_config.yaml\"\n",
    "name = \"Test\"\n",
    "train_path_name = \"TRAIN_DATA_PATH\"\n",
    "validation_path_name = \"VAL_DATA_PATH\"\n",
    "test_paths_names = [\"TEST_DATA_PATH\"]\n",
    "amlt = False\n",
    "gpu=0\n",
    "rank=0\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path=config,\n",
    "    run_name=name,\n",
    "    overrides=[],\n",
    "    train_path_name=train_path_name,\n",
    "    val_path_name=validation_path_name,\n",
    "    test_paths_names=test_paths_names,\n",
    "    amlt=amlt,\n",
    "    is_master=is_master,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n",
    "\n",
    "# Set the GPU device, if using\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Log the params\n",
    "logger.info(json.dumps(params, indent=4))\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "if params[\"GRADIENT_CHECKPOINTING\"]:\n",
    "    raise NotImplementedError(\n",
    "        \"Gradient checkpointing is not yet implemented.\")\n",
    "\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.utils.data import read_fasta, read_json, get_vocab_mappings, read_pickle\n",
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import List\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from src.utils.main_utils import get_or_generate_label_embeddings\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for protein sequences with GO annotations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_paths: dict,\n",
    "        config: dict,\n",
    "        vocabularies: dict,\n",
    "        label_tokenizer=None,\n",
    "        label_encoder=None,\n",
    "        logger=None,\n",
    "        require_label_idxs=False,\n",
    "        subset_fraction: float = 1.0,\n",
    "        deduplicate: bool = False,\n",
    "        is_master: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        paths (dict): Dictionary containing paths to the data and vocabularies.\n",
    "            data_path (str): Path to the FASTA file containing the protein sequences and corresponding GO annotations\n",
    "            dataset_type (str): One of 'train', 'validation', or 'test'\n",
    "            go_descriptions_path (str): Path to the pickled file containing the GO term descriptions mapped to GO term IDs\n",
    "        deduplicate (bool): Whether to remove duplicate sequences (default: False)\n",
    "        \"\"\"\n",
    "        # Error handling: check for missing keys and invalid dataset types\n",
    "        required_keys = [\"data_path\", \"dataset_type\"]\n",
    "        for key in required_keys:\n",
    "            if key not in data_paths:\n",
    "                raise ValueError(\n",
    "                    f\"Missing required key in paths dictionary: {key}\")\n",
    "\n",
    "        assert data_paths[\"dataset_type\"] in [\n",
    "            \"train\",\n",
    "            \"validation\",\n",
    "            \"test\",\n",
    "        ], \"dataset_type must be one of 'train', 'val', or 'test'\"\n",
    "\n",
    "        # Set the dataset type and data path\n",
    "        self.dataset_type = data_paths[\"dataset_type\"]\n",
    "        self.data_path = data_paths[\"data_path\"]\n",
    "\n",
    "        # Set and process the vocabularies\n",
    "        self.amino_acid_vocabulary = vocabularies[\"amino_acid_vocab\"]\n",
    "        self.label_vocabulary = vocabularies[\"GO_label_vocab\"]\n",
    "        self.sequence_id_vocabulary = vocabularies[\"sequence_id_vocab\"]\n",
    "        self._process_vocab()\n",
    "\n",
    "        # Initialize class variables\n",
    "        self.data = read_fasta(data_paths[\"data_path\"])\n",
    "        self.label_embedding_matrix = self.sequence_embedding_df = None\n",
    "        \n",
    "        #Flag to know how Dataset indexing will be handle.\n",
    "        self.require_label_idxs = require_label_idxs\n",
    "\n",
    "        # Subset the data if subset_fraction is provided\n",
    "        if subset_fraction < 1.0:\n",
    "            logging.info(\n",
    "                f\"Subsetting {subset_fraction*100}% of the {self.dataset_type} set...\"\n",
    "            )\n",
    "            self.data = self.data[:int(subset_fraction * len(self.data))]\n",
    "\n",
    "        # Deduplicate the data if deduplicate is True\n",
    "        if deduplicate:\n",
    "            self._remove_duplicates()\n",
    "\n",
    "        # Load the map from alphanumeric label id to text label\n",
    "        self.label_annotation_map = {key: value['label'] for key, value in read_pickle(\n",
    "            data_paths[\"go_annotations_path\"]).to_dict(orient='index').items()}\n",
    "\n",
    "        # Create ordered list of labels\n",
    "        label_text_list = []\n",
    "        for label_id in self.label_vocabulary:\n",
    "            label_text_list.append(self.label_annotation_map[label_id])\n",
    "        self.label_text_list = label_text_list\n",
    "\n",
    "        # Loop through the label IDs and tokenize the labels if a label tokenizer is provided\n",
    "        self.tokenized_labels = None\n",
    "        self.label_tokenizer = None\n",
    "        if label_tokenizer is not None:\n",
    "            self.label_tokenizer = label_tokenizer\n",
    "            self.tokenized_labels = tokenize_labels(\n",
    "                label_text_list, label_tokenizer)\n",
    "\n",
    "        # If a label encoder is provided, encode the labels\n",
    "        # TODO: Move back to main to remove warning\n",
    "        self.label_embedding_matrix = None\n",
    "        self.label_encoder = None\n",
    "        if label_encoder is not None and config[\"params\"][\"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\"]==0:\n",
    "            self.label_encoder = label_encoder\n",
    "            label_embedding_matrix = get_or_generate_label_embeddings(\n",
    "                label_annotations=self.label_text_list,\n",
    "                label_tokenizer=label_tokenizer,\n",
    "                label_encoder=label_encoder,\n",
    "                label_embedding_path=config[\"paths\"][\"LABEL_EMBEDDING_PATH\"],\n",
    "                logger=logger,\n",
    "                batch_size_limit=config[\"params\"][\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"],\n",
    "                is_master=is_master,\n",
    "                pooling_method=config[\"params\"][\"LABEL_EMBEDDING_POOLING_METHOD\"]\n",
    "            )\n",
    "            self.label_embedding_matrix = label_embedding_matrix\n",
    "\n",
    "    # Helper functions for setting embedding dictionaries\n",
    "    def set_sequence_embedding_df(self, embedding_df: pd.DataFrame):\n",
    "        self.sequence_embedding_df = embedding_df\n",
    "\n",
    "    def set_label_embedding_matrix(self, embedding_matrix: torch.Tensor):\n",
    "        self.label_embedding_matrix = embedding_matrix\n",
    "\n",
    "    def _remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Remove duplicate sequences from self.data, keeping only the first instance of each sequence\n",
    "        Use pandas to improve performance\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert self.data to a DataFrame\n",
    "        df = pd.DataFrame(self.data, columns=[\"sequence\", \"labels\"])\n",
    "\n",
    "        # Drop duplicate rows based on the 'sequence' column, keeping the first instance\n",
    "        df = df.drop_duplicates(subset=\"sequence\", keep=\"first\")\n",
    "\n",
    "        # Log the number of duplicate sequences removed\n",
    "        num_duplicates = len(self.data) - len(df)\n",
    "        logging.info(\n",
    "            f\"Removing {num_duplicates} duplicate sequences from {self.data_path}...\")\n",
    "\n",
    "        # Convert the DataFrame back to the list of tuples format\n",
    "        self.data = list(df.itertuples(index=False, name=None))\n",
    "\n",
    "    # Helper functions for processing and loading vocabularies\n",
    "    def _process_vocab(self):\n",
    "        self._process_amino_acid_vocab()\n",
    "        self._process_label_vocab()\n",
    "        self._process_sequence_id_vocab()\n",
    "\n",
    "    def _process_amino_acid_vocab(self):\n",
    "        self.aminoacid2int, self.int2aminoacid = get_vocab_mappings(\n",
    "            self.amino_acid_vocabulary\n",
    "        )\n",
    "\n",
    "    def _process_label_vocab(self):\n",
    "        self.label2int, self.int2label = get_vocab_mappings(\n",
    "            self.label_vocabulary)\n",
    "\n",
    "    def _process_sequence_id_vocab(self):\n",
    "        self.sequence_id2int, self.int2sequence_id = get_vocab_mappings(\n",
    "            self.sequence_id_vocabulary\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_example(self, sequence: str, labels: list[str], label_idxs:list[int] = None) -> dict:\n",
    "        sequence_id_alphanumeric, labels = labels[0], labels[1:]\n",
    "\n",
    "        \n",
    "\n",
    "        # Convert the sequence and labels to integers for one-hot encoding\n",
    "        amino_acid_ints = torch.tensor(\n",
    "            [self.aminoacid2int[aa] for aa in sequence], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        labels_ints = torch.tensor(\n",
    "            [self.label2int[label] for label in labels], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Get the length of the sequence\n",
    "        sequence_length = torch.tensor(len(amino_acid_ints))\n",
    "\n",
    "        # Get multi-hot encoding of sequence and labels\n",
    "        sequence_onehots = torch.nn.functional.one_hot(\n",
    "            amino_acid_ints, num_classes=len(self.amino_acid_vocabulary)\n",
    "        ).permute(1, 0)\n",
    "        label_multihots = torch.nn.functional.one_hot(\n",
    "            labels_ints, num_classes=len(self.label_vocabulary)\n",
    "        ).sum(dim=0)\n",
    "\n",
    "        if label_idxs is not None:\n",
    "            label_multihots = label_multihots[label_idxs]\n",
    "            label_idxs = torch.tensor(label_idxs)\n",
    "\n",
    "        # Set the label embeddings, if provided\n",
    "        label_embeddings = self.label_embedding_matrix if self.label_embedding_matrix is not None else None\n",
    "\n",
    "        # Get the sequence embedding, if provided\n",
    "        sequence_embedding = None\n",
    "        # TODO: Remove this check\n",
    "        if self.sequence_embedding_df is not None:\n",
    "            sequence_embedding = torch.tensor(\n",
    "                self.sequence_embedding_df.loc[sequence_id_alphanumeric].values)\n",
    "\n",
    "        # Get the tokenized labels, if provided\n",
    "        tokenized_labels = self.tokenized_labels if self.tokenized_labels is not None else None\n",
    "\n",
    "        # Return a dict containing the processed example\n",
    "        return {\n",
    "            \"sequence_onehots\": sequence_onehots,\n",
    "            \"sequence_id\": sequence_id_alphanumeric,\n",
    "            \"sequence_embedding\": sequence_embedding,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"label_multihots\": label_multihots,\n",
    "            \"tokenized_labels\": tokenized_labels,\n",
    "            \"label_embeddings\": label_embeddings,\n",
    "            \"label_idxs\":label_idxs\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "        \n",
    "        if self.require_label_idxs:\n",
    "            sequence_idx,label_idxs = idx[0],idx[1]\n",
    "            sequence = self.data[sequence_idx][0]\n",
    "            labels = self.data[sequence_idx][1]\n",
    "        else:\n",
    "            label_idxs=None\n",
    "            sequence, labels = self.data[idx]\n",
    "        \n",
    "\n",
    "        return self.process_example(sequence, labels,label_idxs)\n",
    "\n",
    "    @classmethod\n",
    "    def create_multiple_datasets(\n",
    "        cls,\n",
    "        paths_list: List[Dict[str, str]],\n",
    "        config: dict,\n",
    "        vocabularies: dict,\n",
    "        require_train_label_idxs:bool,\n",
    "        subset_fractions: dict = None,\n",
    "        label_tokenizer=None,\n",
    "        label_encoder=None,\n",
    "        logger=None,\n",
    "        deduplicate: bool = False,\n",
    "    ) -> List[Dataset]:\n",
    "        \"\"\"\n",
    "        paths_list (List[Dict[str, str]]): List of dictionaries, each containing paths to the data and vocabularies.\n",
    "        subset_fractions (dict): Dictionary containing the subset fraction for each dataset type (default: None)\n",
    "        \"\"\"\n",
    "        datasets = defaultdict(list)\n",
    "        subset_fractions = subset_fractions or {}\n",
    "        for data_paths in paths_list:\n",
    "            datasets[data_paths[\"dataset_type\"]].append(\n",
    "                cls(\n",
    "                    data_paths,\n",
    "                    config,\n",
    "                    vocabularies,\n",
    "                    label_tokenizer=label_tokenizer,\n",
    "                    label_encoder=label_encoder,\n",
    "                    logger=logger,\n",
    "                    require_label_idxs=require_train_label_idxs if data_paths[\"dataset_type\"]=='train' else False,\n",
    "                    subset_fraction=subset_fractions.get(\n",
    "                        data_paths[\"dataset_type\"], 1.0),\n",
    "                    deduplicate=deduplicate\n",
    "                )\n",
    "            )\n",
    "        return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-16 05:32:17 PST INFO Removing 66586 duplicate sequences from ./data/swissprot/proteinfer_splits/random/train_GO.fasta...\n",
      "2023-12-16 05:32:31 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n",
      "2023-12-16 05:32:31 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-12-16 05:32:43 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n",
      "2023-12-16 05:32:44 PST INFO Removing 8176 duplicate sequences from ./data/swissprot/proteinfer_splits/random/test_GO.fasta...\n",
      "2023-12-16 05:32:55 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n"
     ]
    }
   ],
   "source": [
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    paths_list=config['dataset_paths_list'],\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    vocabularies=vocabularies,\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"],\n",
    "    },\n",
    "    deduplicate=params[\"DEDUPLICATE\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ProteinDataset(\n",
    "    data_paths=config['dataset_paths_list'][0],\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    vocabularies=vocabularies,\n",
    "    require_label_idxs=True\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13652,\n",
       " 11611,\n",
       " 3049,\n",
       " 8380,\n",
       " 3172,\n",
       " 27552,\n",
       " 27610,\n",
       " 15406,\n",
       " 16259,\n",
       " 10173,\n",
       " 3029,\n",
       " 15405,\n",
       " 14624,\n",
       " 11617,\n",
       " 3041,\n",
       " 19707,\n",
       " 11613,\n",
       " 3067,\n",
       " 4662,\n",
       " 6054,\n",
       " 25242,\n",
       " 3697,\n",
       " 108,\n",
       " 4666,\n",
       " 13992,\n",
       " 1872,\n",
       " 16297,\n",
       " 25390]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[d.label2int[i] for i in (d.data[0][1][1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSKIIEYDETARRAIEAGVNTLADAVRVTLGPRGRHVVLAKAFGGPAVTNDGVTVAREIDLEDPFENLGAQLVKSVATKTNDVAGDGTTTATVLAQALVKGGLRLVAAGANPIELGAGISKAADAVSEALLASATPVSGKDAIAQVATVSSRDQVLGELVGEAMTKVGVDGVVSVEESSTLNTELEFTEGVGFDKGFLSAYFVTDFDAQQAVLDDPVILLHQEKISSLPDLLPMLEKVAESGKPLLIIAEDIEGEALATLVVNSIRKTLKAVAVKAPFFGDRRKAFLEDLAIVTGGQVINPDTGLLLREVGTEVLGSARRVVVSKDDTIIVDGGGAKDAVANRIKQLRAEIEKTDSDWDREKLQERLAKLAGGVAVIKVGAATETALKERKESVEDAVAAAKAAVEEGIVAGGGSALLQARKALDELRGSLSGDQALGVDVFAEALGAPLYWIASNAGLDGAVAVHKVAELPAGHGLNAEKLSYGDLIADGVIDPVKVTRSAVLNSASVARMVLTTETAVVDKPAEEADDHGHGHHHH ['P60545', 'GO:0035639', 'GO:0032553', 'GO:0005524', 'GO:0017076', 'GO:0005737', 'GO:1901265', 'GO:1901363', 'GO:0043168', 'GO:0044424', 'GO:0030554', 'GO:0005488', 'GO:0043167', 'GO:0042026', 'GO:0032559', 'GO:0005515', 'GO:0051082', 'GO:0032555', 'GO:0005575', 'GO:0008144', 'GO:0009987', 'GO:0097159', 'GO:0006457', 'GO:0000166', 'GO:0008150', 'GO:0036094', 'GO:0003674', 'GO:0044464', 'GO:0097367']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sequence_onehots': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'sequence_id': 'P60545',\n",
       " 'sequence_embedding': None,\n",
       " 'sequence_length': tensor(538),\n",
       " 'label_multihots': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0]),\n",
       " 'tokenized_labels': None,\n",
       " 'label_embeddings': None,\n",
       " 'label_idxs': tensor([13652, 11611,  3049,  8380,  3172, 27552, 27610, 15406, 16259, 10173,\n",
       "          3029, 15405, 14624, 11617,  3041, 19707, 11613,  3067,  4662,  6054,\n",
       "         25242,  3697,   108,  4666, 13992,  1872, 16297, 25391])}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[(0,[(d.label2int[i]+1 if d.label2int[i]==25390 else d.label2int[i]) for i in (d.data[0][1][1:])])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import BatchSampler,RandomSampler,SequentialSampler\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data =[(j*1000,[i for i in range(10)]) for j in tqdm(range(1,8))]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        observation_idx,label_idxs = idx[0],idx[1]\n",
    "        features = torch.tensor([self.data[observation_idx][0]])\n",
    "        labels = torch.tensor([self.data[observation_idx][1][i] for i in label_idxs])\n",
    "\n",
    "        return features,labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "class GridSampler(BatchSampler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_sampler,\n",
    "                 observations_batch_size,\n",
    "                 drop_last_observation_batch,\n",
    "                 num_labels,\n",
    "                 labels_batch_size):\n",
    "        \n",
    "        self.observation_sampler = observation_sampler\n",
    "        self.observations_batch_size = observations_batch_size\n",
    "        self.drop_last_observation_batch = drop_last_observation_batch\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.labels_batch_size = labels_batch_size\n",
    "        self.labels_idxs = list(range(num_labels))\n",
    "        self.calculate_num_batches()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.labels_idxs)\n",
    "        print('Getting label batches...')\n",
    "        observation_batches = self.get_observation_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting observation batches...')\n",
    "        label_batches = self.get_label_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting combinations...')\n",
    "        obs_labels_batch_combinations = list(product(observation_batches,label_batches))\n",
    "\n",
    "        print('Done...')\n",
    "        print('Shuffling...')\n",
    "        random.shuffle(obs_labels_batch_combinations)\n",
    "        print('Done...')\n",
    "        print(len(obs_labels_batch_combinations))\n",
    "        for observation_batch,label_batch in obs_labels_batch_combinations:\n",
    "            yield list(product(observation_batch, [label_batch]))#[observation_batch,label_batch]\n",
    "    \n",
    "    def calculate_num_batches(self):\n",
    "        \n",
    "        num_label_batches = np.ceil(self.num_labels/self.labels_batch_size)\n",
    "        num_observation_batches = (np.ceil(len(self.observation_sampler)/self.observations_batch_size)\n",
    "                                   if not self.drop_last_observation_batch\n",
    "                                   else len(self.observation_sampler)//self.observations_batch_size)\n",
    "        print('Done...')\n",
    "\n",
    "        self.total_num_batches = int(num_label_batches*num_observation_batches)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_num_batches\n",
    "    \n",
    "\n",
    "    def get_label_batches(self):\n",
    "\n",
    "        #n_chunks = int(np.ceil(self.num_labels/self.labels_batch_size))\n",
    "        return [self.labels_idxs[i:i+self.labels_batch_size] for i in range(0,self.num_labels,self.labels_batch_size)]\n",
    "        \n",
    "\n",
    "    def get_observation_batches(self):\n",
    "\n",
    "        batches = []\n",
    "\n",
    "        if self.drop_last_observation_batch:\n",
    "            observation_sampler_iter = iter(self.observation_sampler)\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = [next(observation_sampler_iter) for _ in range(self.observations_batch_size)]\n",
    "                    batches.append(batch)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        else:\n",
    "            batch = [0] * self.observations_batch_size\n",
    "            idx_in_batch = 0\n",
    "            for idx in self.observation_sampler:\n",
    "                batch[idx_in_batch] = idx\n",
    "                idx_in_batch += 1\n",
    "                if idx_in_batch == self.observations_batch_size:\n",
    "                    batches.append(batch)\n",
    "                    idx_in_batch = 0\n",
    "                    batch = [0] * self.observations_batch_size\n",
    "            if idx_in_batch > 0:\n",
    "                batches.append(batch[:idx_in_batch])\n",
    "        return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 131659.77it/s]\n"
     ]
    }
   ],
   "source": [
    "a = MyDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n",
      "Getting label batches...\n",
      "Done...\n",
      "Getting observation batches...\n",
      "Done...\n",
      "Getting combinations...\n",
      "Done...\n",
      "Shuffling...\n",
      "Done...\n",
      "12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[7000],\n",
       "         [4000],\n",
       "         [1000]]),\n",
       " tensor([[7, 9, 4],\n",
       "         [7, 9, 4],\n",
       "         [7, 9, 4]])]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GS=GridSampler(observation_sampler=RandomSampler(data_source=a),\n",
    "                 observations_batch_size=3,\n",
    "                 drop_last_observation_batch=False,\n",
    "                 num_labels=10,\n",
    "                 labels_batch_size=3)\n",
    "l = DataLoader(dataset=a,batch_sampler=GS)\n",
    "l_iter = iter(l)\n",
    "next(l_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "DS = DistributedSampler(\n",
    "    a,\n",
    "    num_replicas=4,\n",
    "    rank=rank,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "DSGS=GridSampler(observation_sampler=DS,\n",
    "                 observations_batch_size=3,\n",
    "                 drop_last_observation_batch=False,\n",
    "                 num_labels=10,\n",
    "                 labels_batch_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = DataLoader(dataset=a,batch_sampler=DSGS)\n",
    "l_iter = iter(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32102"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d.label_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting label batches...\n",
      "Done...\n",
      "Getting observation batches...\n",
      "Done...\n",
      "Getting combinations...\n",
      "Done...\n",
      "Shuffling...\n",
      "Done...\n",
      "30954\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml_iter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m observation_idx,label_idxs \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m0\u001b[39m],idx[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m---> 13\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m1\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label_idxs])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features,labels\n",
      "Cell \u001b[0;32mIn[36], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m observation_idx,label_idxs \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m0\u001b[39m],idx[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m---> 13\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobservation_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label_idxs])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features,labels\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "next(l_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting label batches...\n",
      "Done...\n",
      "Getting observation batches...\n",
      "Done...\n",
      "Getting combinations...\n",
      "Done...\n",
      "Shuffling...\n",
      "Done...\n",
      "61875\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l:\n\u001b[1;32m      3\u001b[0m     labels,obs \u001b[38;5;241m=\u001b[39m i \n\u001b[1;32m      5\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     11\u001b[0m observation_idx,label_idxs \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m0\u001b[39m],idx[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m---> 13\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m1\u001b[39m][i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label_idxs])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features,labels\n",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m observation_idx,label_idxs \u001b[38;5;241m=\u001b[39m idx[\u001b[38;5;241m0\u001b[39m],idx[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     12\u001b[0m features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[observation_idx][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m---> 13\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobservation_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label_idxs])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features,labels\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "d = defaultdict(list)\n",
    "for i in l:\n",
    "    labels,obs = i \n",
    "\n",
    "    labels = labels.flatten().tolist()\n",
    "    print(labels,obs.tolist())\n",
    "    for j_idx,j in enumerate(obs.tolist()):\n",
    "        d[labels[j_idx]].extend(j)\n",
    "        \n",
    "\n",
    "    #print(obs.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{3000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 2000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 7000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 5000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 8000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       " 6000: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k:sorted(v) for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 2]), array([3, 4, 5]), array([6, 7, 8]), array([ 9, 10])]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "N = 4\n",
    "np.array_split(list(range(11)),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3], [4, 5], [6, 7], [8, 9]]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_list_into_chunks(my_list,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MyDataset()\n",
    "s=RandomSampler(data_source=a)\n",
    "B = BatchSampler(sampler=s,batch_size=2,drop_last=True)\n",
    "l = DataLoader(dataset=a,batch_sampler=B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 0], [5, 3], [4, 6], [7, 1]]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 7, 2, 4, 1, 5, 6]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions_310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
