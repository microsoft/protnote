{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "os.chdir(str(curdir.parent.absolute()))\n",
    "curdir = Path(os.getcwd())\n",
    "\n",
    "from src.utils.data import (\n",
    "    seed_everything,\n",
    "    log_gpu_memory_usage\n",
    ")\n",
    "from src.utils.main_utils import get_or_generate_vocabularies,  get_or_generate_label_embeddings, get_or_generate_sequence_embeddings, validate_arguments\n",
    "from src.data.datasets import ProteinDataset, create_multiple_loaders\n",
    "from src.models.ProTCLTrainer import ProTCLTrainer\n",
    "from src.models.ProTCL import ProTCL\n",
    "from src.models.protein_encoders import ProteInfer\n",
    "from src.utils.evaluation import EvalMetrics\n",
    "from src.utils.models import count_parameters_by_layer, sigmoid_bias_from_prob,load_checkpoint,    load_model\n",
    "\n",
    "from src.utils.configs import get_setup\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributed as dist\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "import mlflow\n",
    "import loralib as lora\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from torch.utils.data import BatchSampler\n",
    "from torch.utils.data import Sampler, WeightedRandomSampler\n",
    "from typing import Optional\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GeneralDistributedSampler(DistributedSampler):\n",
    "\n",
    "    \"\"\"\n",
    "    Class to use distributed sampler with any sampler!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler: Sampler, num_replicas: Optional[int] = None,\n",
    "                 rank: Optional[int] = None,\n",
    "                 seed: int = 0, drop_last: bool = False):\n",
    "        \n",
    "        #Same as normal DistributedSampler with shuffle = False\n",
    "        super().__init__(dataset = sampler,\n",
    "                         num_replicas=num_replicas,\n",
    "                         rank=rank,\n",
    "                         shuffle=False,\n",
    "                         seed = seed,\n",
    "                         drop_last=drop_last)\n",
    "        \n",
    "        assert len(sampler)>num_replicas, \"Total samples must be > num replicas\"\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # deterministically shuffle based on epoch\n",
    "        torch.manual_seed(self.epoch+self.seed)\n",
    "        indices = list(self.dataset)\n",
    "        \n",
    "        if not self.drop_last:\n",
    "            # add extra samples to make it evenly divisible\n",
    "            padding_size = self.total_size - len(indices)\n",
    "            if padding_size <= len(indices):\n",
    "                indices += indices[:padding_size]\n",
    "            else:\n",
    "                indices += (indices * math.ceil(padding_size / len(indices)))[:padding_size]\n",
    "        else:\n",
    "            # remove tail of data to make it evenly divisible.\n",
    "            indices = indices[:self.total_size]\n",
    "        assert len(indices) == self.total_size\n",
    "\n",
    "        # subsample\n",
    "        indices = indices[self.rank:self.total_size:self.num_replicas]\n",
    "        assert len(indices) == self.num_samples\n",
    "        return iter(indices)\n",
    "    \n",
    "class DistributedWeightedSampler(Sampler):\n",
    "    def __init__(self, weights, world_size=None, rank=None, replacement=True):\n",
    "        # Get the world size and rank if not provided\n",
    "        if world_size is None:\n",
    "            if not dist.is_available():\n",
    "                raise RuntimeError(\"Requires distributed package to be available\")\n",
    "            world_size = dist.get_world_size()\n",
    "        if rank is None:\n",
    "            rank = dist.get_rank()\n",
    "\n",
    "        self.weights = weights\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "        self.epoch = 0\n",
    "        self.replacement = replacement\n",
    "\n",
    "        # Determine the number of samples for each GPU, rounding down to ensure it is evenly divisible\n",
    "        self.num_samples = int(math.floor(len(self.weights) * 1.0 / self.world_size))\n",
    "        \n",
    "        # Determine the total number of samples\n",
    "        self.total_size = self.num_samples * self.world_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Shuffle based on the epoch\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self.epoch)\n",
    "        \n",
    "        # Create a weighted sample for the entire dataset\n",
    "        if self.replacement:\n",
    "            indices = torch.multinomial(self.weights, self.total_size, replacement=True, generator=g)\n",
    "        else:\n",
    "            assert len(self.weights) > self.total_size, \"When sampling without replacement, number of samples to draw must be less than the number of elements in the dataset\"\n",
    "            indices = torch.multinomial(self.weights, self.total_size, replacement=False, generator=g)\n",
    "\n",
    "        # Subsample for the current process\n",
    "        indices_for_one_gpu = indices[self.rank:self.total_size:self.world_size]\n",
    "        \n",
    "        # Shuffle each epoch\n",
    "        indices_for_one_gpu = indices_for_one_gpu[torch.randperm(len(indices_for_one_gpu), generator=g)].tolist()\n",
    "            \n",
    "        return iter(indices_for_one_gpu)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "\n",
    "class GridBatchSampler(BatchSampler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_sampler,\n",
    "                 observations_batch_size,\n",
    "                 drop_last_observation_batch,\n",
    "                 num_labels,\n",
    "                 labels_batch_size,\n",
    "                 shuffle_grid = True):\n",
    "        \n",
    "        self.observation_sampler = observation_sampler\n",
    "        self.observations_batch_size = observations_batch_size\n",
    "        self.drop_last_observation_batch = drop_last_observation_batch\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.labels_batch_size = labels_batch_size\n",
    "        self.shuffle_grid = shuffle_grid\n",
    "        self.labels_idxs = list(range(num_labels))\n",
    "        self.calculate_num_batches()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.Random(self.epoch).shuffle(self.labels_idxs)\n",
    "        print('Getting label batches...')\n",
    "        observation_batches = self.get_observation_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting observation batches...')\n",
    "        label_batches = self.get_label_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting combinations...')\n",
    "        obs_labels_batch_combinations = list(product(observation_batches,label_batches))\n",
    "\n",
    "        print('Done...')\n",
    "        \n",
    "        if self.shuffle_grid:\n",
    "            print('Shuffling...')\n",
    "            random.shuffle(obs_labels_batch_combinations)\n",
    "        print('Done...')\n",
    "        for observation_batch,label_batch in obs_labels_batch_combinations:\n",
    "            yield list(product(observation_batch, [label_batch]))#[observation_batch,label_batch]\n",
    "    \n",
    "    def calculate_num_batches(self):\n",
    "        \n",
    "        num_label_batches = np.ceil(self.num_labels/self.labels_batch_size)\n",
    "        num_observation_batches = (np.ceil(len(self.observation_sampler)/self.observations_batch_size)\n",
    "                                   if not self.drop_last_observation_batch\n",
    "                                   else len(self.observation_sampler)//self.observations_batch_size)\n",
    "        print('Done...')\n",
    "\n",
    "        self.total_num_batches = int(num_label_batches*num_observation_batches)\n",
    "        print(f\"num label batches = {num_label_batches}, num observation batches = {num_observation_batches}\")\n",
    "        print(f\"total batches = {self.total_num_batches}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_num_batches\n",
    "    \n",
    "\n",
    "    def get_label_batches(self):\n",
    "\n",
    "        #n_chunks = int(np.ceil(self.num_labels/self.labels_batch_size))\n",
    "        return [self.labels_idxs[i:i+self.labels_batch_size] for i in range(0,self.num_labels,self.labels_batch_size)]\n",
    "        \n",
    "\n",
    "    def get_observation_batches(self):\n",
    "\n",
    "        batches = []\n",
    "\n",
    "        if self.drop_last_observation_batch:\n",
    "            observation_sampler_iter = iter(self.observation_sampler)\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = [next(observation_sampler_iter) for _ in range(self.observations_batch_size)]\n",
    "                    batches.append(batch)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        else:\n",
    "            batch = [0] * self.observations_batch_size\n",
    "            idx_in_batch = 0\n",
    "            for idx in self.observation_sampler:\n",
    "                batch[idx_in_batch] = idx\n",
    "                idx_in_batch += 1\n",
    "                if idx_in_batch == self.observations_batch_size:\n",
    "                    batches.append(batch)\n",
    "                    idx_in_batch = 0\n",
    "                    batch = [0] * self.observations_batch_size\n",
    "            if idx_in_batch > 0:\n",
    "                batches.append(batch[:idx_in_batch])\n",
    "        return batches\n",
    "    \n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "    \n",
    "def observation_sampler_factory(\n",
    "    distribute_labels:bool,\n",
    "    weighted_sampling:bool,\n",
    "    dataset: Dataset = None,\n",
    "    world_size: int = 1,\n",
    "    rank: int = 0,\n",
    "    sequence_weights: torch.Tensor = None\n",
    "\n",
    "    ):\n",
    "\n",
    "    if distribute_labels and not weighted_sampling:\n",
    "        print(\"WARNING: No Sampler used for distribute labels\")\n",
    "        sampler = None\n",
    "    elif not distribute_labels and world_size == 1 and weighted_sampling:\n",
    "        # If NOT distributing labels, and not training on multiple GPU's, create a non-distributed weighted sampler with replacement\n",
    "        assert sequence_weights is not None, \"Weighted RandomSampler requires weights\"\n",
    "\n",
    "        sampler = WeightedRandomSampler(\n",
    "            sequence_weights, \n",
    "            len(sequence_weights), \n",
    "            replacement=True\n",
    "        )\n",
    "    elif not distribute_labels and world_size > 1 and weighted_sampling:\n",
    "        # If distributing sequences across multiple GPUs with a weighted sampler, create custom DistributedWeightedSampler\n",
    "        sampler = DistributedWeightedSampler(\n",
    "            sequence_weights,\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            replacement=True,\n",
    "        )\n",
    "    elif not distribute_labels and not weighted_sampling:\n",
    "        # If simply distributing sequences across GPU's without weighted sampling, use a distributed sampler\n",
    "\n",
    "        assert dataset is not None, \"DistributeSampler requires dataset\"\n",
    "\n",
    "        sampler = DistributedSampler(\n",
    "            dataset,\n",
    "            num_replicas=world_size,\n",
    "            rank=rank,\n",
    "            shuffle=True\n",
    "        )\n",
    "    else:\n",
    "        # Raise error\n",
    "        raise ValueError(\"Invalid combination of WEIGHTED_SAMPLING, WORLD_SIZE, and DISTRIBUTE_LABELS parameters\")\n",
    "    \n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 12\n",
    "b = 5\n",
    "\n",
    "c=int(np.floor(n/b))\n",
    "l=np.random.randint(0,100,size=(n,))\n",
    "l_i = list(range(n))\n",
    "\n",
    "splits=np.array_split(l_i,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([94, 16, 27]),\n",
       " array([73, 71, 50]),\n",
       " array([ 0, 35]),\n",
       " array([32, 75]),\n",
       " array([18, 53])]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array_split(l,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([94, 16, 27]),\n",
       " array([73, 71, 50]),\n",
       " array([ 0, 35]),\n",
       " array([32, 75]),\n",
       " array([18, 53])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[l[list(i)] for i in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 3, 4, 4]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([i for r in range(b) for i in l[r:b:c]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DistributedGridBatchSampler(BatchSampler):\n",
    "\n",
    "    def __init__(self,\n",
    "                 observation_sampler,\n",
    "                 observations_batch_size,\n",
    "                 drop_last_observation_batch,\n",
    "                 num_labels,\n",
    "                 labels_batch_size,\n",
    "                 world_size,\n",
    "                 rank,\n",
    "                 shuffle_grid = True):\n",
    "        \n",
    "        self.observation_sampler = observation_sampler\n",
    "        self.observations_batch_size = observations_batch_size\n",
    "        self.drop_last_observation_batch = drop_last_observation_batch\n",
    "        self.world_size = world_size\n",
    "        self.rank = rank\n",
    "\n",
    "        self.num_labels = num_labels\n",
    "        self.labels_batch_size = labels_batch_size\n",
    "        self.shuffle_grid = shuffle_grid\n",
    "        self.labels_idxs = list(range(num_labels))\n",
    "        self.calculate_num_batches()\n",
    "        \n",
    "    def __iter__(self):\n",
    "        random.Random(self.epoch).shuffle(self.labels_idxs)\n",
    "        print('Getting label batches...')\n",
    "        observation_batches = self.get_observation_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting observation batches...')\n",
    "        label_batches = self.get_label_batches()\n",
    "        print('Done...')\n",
    "\n",
    "        print('Getting combinations...')\n",
    "        obs_labels_batch_combinations = list(product(observation_batches,label_batches))\n",
    "\n",
    "        print('Done...')\n",
    "        \n",
    "        if self.shuffle_grid:\n",
    "            print('Shuffling...')\n",
    "            random.shuffle(obs_labels_batch_combinations)\n",
    "        print('Done...')\n",
    "        for observation_batch,label_batch in obs_labels_batch_combinations:\n",
    "            yield list(product(observation_batch, [label_batch]))#[observation_batch,label_batch]\n",
    "    \n",
    "    def calculate_num_batches(self):\n",
    "        \n",
    "        num_label_batches = np.ceil(self.num_labels/self.labels_batch_size)\n",
    "        num_observation_batches = (np.ceil(len(self.observation_sampler)/self.observations_batch_size)\n",
    "                                   if not self.drop_last_observation_batch\n",
    "                                   else len(self.observation_sampler)//self.observations_batch_size)\n",
    "        print('Done...')\n",
    "\n",
    "        self.total_num_batches = int(num_label_batches*num_observation_batches)\n",
    "        print(f\"num label batches = {num_label_batches}, num observation batches = {num_observation_batches}\")\n",
    "        print(f\"total batches = {self.total_num_batches}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_num_batches\n",
    "    \n",
    "\n",
    "    def get_label_batches(self):\n",
    "\n",
    "        label_batches_of_gpu = []\n",
    "        for i in range(0,self.num_labels,self.labels_batch_size):\n",
    "            labels = self.labels_idxs[i:i+self.labels_batch_size]\n",
    "            label_batches_of_gpu.append(np.array_split(labels,self.world_size)[self.rank])\n",
    "\n",
    "    def set_epoch(self, epoch):\n",
    "        self.epoch = epoch\n",
    "\n",
    "    def get_observation_batches(self):\n",
    "\n",
    "        batches = []\n",
    "\n",
    "        if self.drop_last_observation_batch:\n",
    "            observation_sampler_iter = iter(self.observation_sampler)\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = [next(observation_sampler_iter) for _ in range(self.observations_batch_size)]\n",
    "                    batches.append(batch)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "        else:\n",
    "            batch = [0] * self.observations_batch_size\n",
    "            idx_in_batch = 0\n",
    "            for idx in self.observation_sampler:\n",
    "                batch[idx_in_batch] = idx\n",
    "                idx_in_batch += 1\n",
    "                if idx_in_batch == self.observations_batch_size:\n",
    "                    batches.append(batch)\n",
    "                    idx_in_batch = 0\n",
    "                    batch = [0] * self.observations_batch_size\n",
    "            if idx_in_batch > 0:\n",
    "                batches.append(batch[:idx_in_batch])\n",
    "        return batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_sampler:\n",
    "    assert label_sample_size is not None,\"Provide label_sample_size when using grid sampler\"\n",
    "    batch_sampler=GridBatchSampler(observation_sampler=sequence_sampler,\n",
    "        observations_batch_size=batch_size_for_type,\n",
    "        drop_last_observation_batch=True,\n",
    "        num_labels=len(dataset.label_vocabulary),\n",
    "        labels_batch_size=label_sample_size,\n",
    "        shuffle_grid=True\n",
    "        )\n",
    "    #When defining a BatchSampler, these paramters are ignored in the Dataloader. Must be set \n",
    "    #To these values to avoid pytorch error.\n",
    "    batch_size_for_type = 1\n",
    "    sequence_sampler = None\n",
    "    drop_last = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "weights = torch.randint(0,100,(100,))*1.0\n",
    "num_samples = 100\n",
    "\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, num_samples)\n",
    "\n",
    "num_replicas = 4\n",
    "\n",
    "dist_samplers2 = [\n",
    "    DistributedWeightedSampler(\n",
    "                        weights,\n",
    "                        world_size=num_replicas,\n",
    "                        rank=i,\n",
    "                        replacement=True,\n",
    "                    ) \n",
    "    for i in range(num_replicas)\n",
    "]\n",
    "\n",
    "\n",
    "torch.manual_seed(1)\n",
    "true_indices = list(sampler)\n",
    "\n",
    "indices_per_rank = []\n",
    "for s in dist_samplers2:\n",
    "    s.set_epoch(1)\n",
    "    indices_per_rank += list(s)\n",
    "\n",
    "set(indices_per_rank) == set(true_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.DistributedWeightedSampler at 0x7f80ec1aa140>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2af9120>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2afa230>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2af8d30>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_samplers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.DistributedWeightedSampler at 0x7f80ec0d5930>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2964280>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2af8cd0>,\n",
       " <__main__.DistributedWeightedSampler at 0x7f80f2afa1d0>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_samplers2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32/4)*(32/16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n",
      "num label batches = 2.0, num observation batches = 8\n",
      "total batches = 16\n"
     ]
    }
   ],
   "source": [
    "num_samples = 32\n",
    "num_labels = 32\n",
    "seq_batch_size = 4\n",
    "label_batch_size = 16\n",
    "num_replicas = 4\n",
    "\n",
    "weights = torch.randint(0,100,(num_samples,))*1.0\n",
    "\n",
    "labels = torch.arange(num_labels)\n",
    "\n",
    "sampler = observation_sampler_factory(\n",
    "    distribute_labels = False,\n",
    "    weighted_sampling = True,\n",
    "    dataset = None,\n",
    "    world_size = 1,\n",
    "    rank = None,\n",
    "    sequence_weights=weights)\n",
    "\n",
    "sampler_base = WeightedRandomSampler(weights, num_samples)\n",
    "\n",
    "batch_sampler_base=GridBatchSampler(observation_sampler=sampler_base,\n",
    "    observations_batch_size=seq_batch_size,\n",
    "    drop_last_observation_batch=True,\n",
    "    num_labels=num_labels,\n",
    "    labels_batch_size=label_batch_size,\n",
    "    shuffle_grid=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting label batches...\n",
      "Done...\n",
      "Getting observation batches...\n",
      "Done...\n",
      "Getting combinations...\n",
      "Done...\n",
      "Shuffling...\n",
      "Done...\n"
     ]
    }
   ],
   "source": [
    "batch_sampler_base.set_epoch(1)\n",
    "a=list(batch_sampler_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(17, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (1, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (7, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (2, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(20, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (17, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (27, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (29, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(29, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (1, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (6, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (28, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(20, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (17, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (27, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (29, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(24, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (10, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (21, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (16, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(21, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (15, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (21, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (9, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(24, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (10, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (21, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (16, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(18, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (29, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (6, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (19, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(15, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (15, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (7, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (22, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(15, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (15, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (7, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (22, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(3, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (5, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (5, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (20, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(29, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (1, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (6, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0]),\n",
       "  (28, [26, 17, 11, 10, 28, 1, 5, 4, 7, 16, 9, 19, 30, 13, 22, 0])],\n",
       " [(3, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (5, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (5, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (20, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(21, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (15, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (21, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (9, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(18, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (29, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (6, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (19, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])],\n",
       " [(17, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (1, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (7, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8]),\n",
       "  (2, [21, 29, 6, 12, 20, 23, 14, 15, 3, 31, 2, 24, 25, 27, 18, 8])]]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, [2, 6, 31, 21, 11, 15, 22, 10, 14, 13, 27, 26, 4, 17, 1, 20]),\n",
       " (8, [2, 6, 31, 21, 11, 15, 22, 10, 14, 13, 27, 26, 4, 17, 1, 20]),\n",
       " (11, [2, 6, 31, 21, 11, 15, 22, 10, 14, 13, 27, 26, 4, 17, 1, 20]),\n",
       " (13, [2, 6, 31, 21, 11, 15, 22, 10, 14, 13, 27, 26, 4, 17, 1, 20])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dist_samplers2 = []\n",
    "for i in range(num_replicas):\n",
    "    sampler_ = observation_sampler_factory(\n",
    "        distribute_labels = False,\n",
    "        weighted_sampling = True,\n",
    "        dataset = None,\n",
    "        world_size = num_replicas,\n",
    "        rank = i,\n",
    "        sequence_weights=weights)\n",
    "    sampler_.set_epoch(1)\n",
    "    \n",
    "    batch_sampler=GridBatchSampler(observation_sampler=sampler_,\n",
    "        observations_batch_size=seq_batch_size,\n",
    "        drop_last_observation_batch=True,\n",
    "        num_labels=num_labels,\n",
    "        labels_batch_size=label_batch_size,\n",
    "        shuffle_grid=True\n",
    "        )\n",
    "    dist_samplers2.append(batch_sampler)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "true_indices = list(sampler)\n",
    "\n",
    "torch.manual_seed(1)\n",
    "true_indices_base = list(sampler_base)\n",
    "\n",
    "indices_per_rank = []\n",
    "for s in dist_samplers2:\n",
    "    \n",
    "    indices_per_rank += list(s)\n",
    "\n",
    "#print(set(indices_per_rank) == set(true_indices))\n",
    "#print(set(true_indices_base)==set(true_indices))\n",
    "\n",
    "\n",
    "'''\n",
    "dist_samplers2 = [\n",
    "    DistributedWeightedSampler(\n",
    "                        weights,\n",
    "                        world_size=num_replicas,\n",
    "                        rank=i,\n",
    "                        replacement=True,\n",
    "                    ) \n",
    "    \n",
    "]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n",
      "num label batches = 10.0, num observation batches = 20\n",
      "total batches = 200\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12,\n",
       " 33,\n",
       " 80,\n",
       " 40,\n",
       " 37,\n",
       " 24,\n",
       " 4,\n",
       " 2,\n",
       " 33,\n",
       " 16,\n",
       " 9,\n",
       " 6,\n",
       " 64,\n",
       " 67,\n",
       " 29,\n",
       " 25,\n",
       " 80,\n",
       " 1,\n",
       " 44,\n",
       " 21,\n",
       " 22,\n",
       " 20,\n",
       " 21,\n",
       " 24,\n",
       " 61,\n",
       " 8,\n",
       " 59,\n",
       " 28,\n",
       " 21,\n",
       " 25,\n",
       " 14,\n",
       " 23,\n",
       " 68,\n",
       " 41,\n",
       " 30,\n",
       " 55,\n",
       " 9,\n",
       " 40,\n",
       " 19,\n",
       " 0,\n",
       " 30,\n",
       " 98,\n",
       " 42,\n",
       " 22,\n",
       " 65,\n",
       " 47,\n",
       " 29,\n",
       " 47,\n",
       " 71,\n",
       " 65,\n",
       " 21,\n",
       " 25,\n",
       " 11,\n",
       " 26,\n",
       " 30,\n",
       " 30,\n",
       " 27,\n",
       " 99,\n",
       " 65,\n",
       " 30,\n",
       " 67,\n",
       " 43,\n",
       " 9,\n",
       " 6,\n",
       " 14,\n",
       " 57,\n",
       " 7,\n",
       " 73,\n",
       " 42,\n",
       " 20,\n",
       " 32,\n",
       " 14,\n",
       " 32,\n",
       " 15,\n",
       " 33,\n",
       " 96,\n",
       " 49,\n",
       " 13,\n",
       " 40,\n",
       " 48,\n",
       " 59,\n",
       " 61,\n",
       " 71,\n",
       " 40,\n",
       " 47,\n",
       " 16,\n",
       " 21,\n",
       " 47,\n",
       " 3,\n",
       " 64,\n",
       " 38,\n",
       " 41,\n",
       " 30,\n",
       " 42,\n",
       " 24,\n",
       " 0,\n",
       " 62,\n",
       " 83,\n",
       " 25,\n",
       " 1]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_per_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:32:47 PST INFO Logging to ./outputs/logs/2023-12-18_18-32-47_Test.log and console...\n",
      "2023-12-18 18:32:48 PST INFO Using device: cuda:0\n",
      "2023-12-18 18:32:48 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 4,\n",
      "    \"VALIDATION_BATCH_SIZE\": 4,\n",
      "    \"TEST_BATCH_SIZE\": 4,\n",
      "    \"GRID_SAMPLER\": false,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.0003,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 2,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"PROJECTION_HEAD_NUM_LAYERS\": 2,\n",
      "    \"PROJECTION_HEAD_HIDDEN_DIM_SCALE_FACTOR\": 1,\n",
      "    \"FEATURE_FUSION\": \"concatenation\",\n",
      "    \"LABEL_EMBEDDING_POOLING_METHOD\": \"mean\",\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 3,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 4,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"FocalLoss\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"SUPCON_TEMP\": 0.07,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\": 0,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 1,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 0.001,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 0.001,\n",
      "    \"TEST_SUBSET_FRACTION\": 0.001,\n",
      "    \"SHUFFLE_LABELS\": false\n",
      "}\n",
      "2023-12-18 18:32:51 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-12-18 18:32:51 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-12-18 18:32:51 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n",
      "2023-12-18 18:32:57 PST INFO Subsetting 0.1% of the train set...\n",
      "2023-12-18 18:32:58 PST INFO Removing 58 duplicate sequences from ./data/swissprot/proteinfer_splits/random/train_GO.fasta...\n",
      "2023-12-18 18:33:09 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n",
      "2023-12-18 18:33:10 PST INFO Subsetting 0.1% of the validation set...\n",
      "2023-12-18 18:33:10 PST INFO Removing 10 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-12-18 18:33:22 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n",
      "2023-12-18 18:33:22 PST INFO Subsetting 0.1% of the test set...\n",
      "2023-12-18 18:33:22 PST INFO Removing 13 duplicate sequences from ./data/swissprot/proteinfer_splits/random/test_GO.fasta...\n",
      "2023-12-18 18:33:33 PST INFO Loaded label embeddings from ./data/embeddings/frozen_BioGPT_label_embeddings_mean.pkl\n",
      "2023-12-18 18:33:33 PST INFO ################## 2023-12-18_18-32-47 RUNNING main.py ##################\n",
      "2023-12-18 18:33:34 PST INFO Loaded sequence embeddings from ./data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SETUP ###\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if master process\n",
    "is_master = True\n",
    "config = \"configs/base_config.yaml\"\n",
    "name = \"Test\"\n",
    "train_path_name = \"TRAIN_DATA_PATH\"\n",
    "validation_path_name = \"VAL_DATA_PATH\"\n",
    "test_paths_names = [\"TEST_DATA_PATH\"]\n",
    "amlt = False\n",
    "gpu=0\n",
    "rank=0\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path=config,\n",
    "    run_name=name,\n",
    "    overrides=[],\n",
    "    train_path_name=train_path_name,\n",
    "    val_path_name=validation_path_name,\n",
    "    test_paths_names=test_paths_names,\n",
    "    amlt=amlt,\n",
    "    is_master=is_master,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n",
    "\n",
    "# Set the GPU device, if using\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Log the params\n",
    "logger.info(json.dumps(params, indent=4))\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "if params[\"GRADIENT_CHECKPOINTING\"]:\n",
    "    raise NotImplementedError(\n",
    "        \"Gradient checkpointing is not yet implemented.\")\n",
    "\n",
    "if params[\"LORA\"]:\n",
    "    for layer in label_encoder.layers:\n",
    "        in_features, out_features = 1024, 1024\n",
    "        layer.self_attn.q_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.v_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.k_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.out_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "    # Mark only the LoRA parameters as trainable\n",
    "    lora.mark_only_lora_as_trainable(label_encoder)\n",
    "\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    paths_list=config['dataset_paths_list'],\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    vocabularies=vocabularies,\n",
    "    require_train_label_idxs=params['GRID_SAMPLER'],\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"],\n",
    "    },\n",
    "    deduplicate=params[\"DEDUPLICATE\"],\n",
    ")\n",
    "\n",
    "# Seed everything so we don't go crazy\n",
    "seed_everything(params[\"SEED\"], device)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING main.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    \"train\": params[\"TRAIN_LABEL_SAMPLE_SIZE\"],\n",
    "    \"validation\": params[\"VALIDATION_LABEL_SAMPLE_SIZE\"],\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    shuffle_labels=params['SHUFFLE_LABELS'],\n",
    "    in_batch_sampling=params['IN_BATCH_SAMPLING'],\n",
    "    grid_sampler=params['GRID_SAMPLER'],\n",
    "    num_workers=params[\"NUM_WORKERS\"],\n",
    "    world_size=1,\n",
    "    rank=rank,\n",
    ")\n",
    "\n",
    "if params[\"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\"]==0:\n",
    "    # Move the label encoder to CPU\n",
    "    label_encoder = label_encoder.cpu()\n",
    "\n",
    "# Initialize ProteInfer\n",
    "sequence_encoder = ProteInfer.from_pretrained(\n",
    "    weights_path=paths[\"PROTEINFER_WEIGHTS_PATH\"],\n",
    "    num_labels=config[\"embed_sequences_params\"][\"PROTEINFER_NUM_LABELS\"],\n",
    "    input_channels=config[\"embed_sequences_params\"][\"INPUT_CHANNELS\"],\n",
    "    output_channels=config[\"embed_sequences_params\"][\"OUTPUT_CHANNELS\"],\n",
    "    kernel_size=config[\"embed_sequences_params\"][\"KERNEL_SIZE\"],\n",
    "    activation=torch.nn.ReLU,\n",
    "    dilation_base=config[\"embed_sequences_params\"][\"DILATION_BASE\"],\n",
    "    num_resnet_blocks=config[\"embed_sequences_params\"][\"NUM_RESNET_BLOCKS\"],\n",
    "    bottleneck_factor=config[\"embed_sequences_params\"][\"BOTTLENECK_FACTOR\"],\n",
    ")\n",
    "\n",
    "# Generate all sequence embeddings upfront, if not training the sequence encoder\n",
    "sequence_embedding_df = None\n",
    "if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "    sequence_embedding_df = get_or_generate_sequence_embeddings(\n",
    "        paths,\n",
    "        device,\n",
    "        sequence_encoder,\n",
    "        datasets,\n",
    "        params,\n",
    "        logger,\n",
    "    )\n",
    "    sequence_encoder = sequence_encoder.to('cpu')\n",
    "\n",
    "# Loop through all the datasets and set the sequence embedding df\n",
    "for dataset in datasets.values():\n",
    "    for subset in dataset:\n",
    "        if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "            subset.set_sequence_embedding_df(sequence_embedding_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProTCL(\n",
    "    # Parameters\n",
    "    protein_embedding_dim=params[\"PROTEIN_EMBEDDING_DIM\"],\n",
    "    label_embedding_dim=params[\"LABEL_EMBEDDING_DIM\"],\n",
    "    latent_dim=params[\"LATENT_EMBEDDING_DIM\"],\n",
    "    label_embedding_pooling_method=params[\"LABEL_EMBEDDING_POOLING_METHOD\"],\n",
    "\n",
    "    # Encoders\n",
    "    label_encoder=label_encoder,\n",
    "    sequence_encoder=sequence_encoder,\n",
    "\n",
    "    # Output Layer\n",
    "    output_mlp_hidden_dim_scale_factor=params[\"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\"],\n",
    "    output_mlp_num_layers=params[\"OUTPUT_MLP_NUM_LAYERS\"],\n",
    "    output_neuron_bias=sigmoid_bias_from_prob(params[\"OUTPUT_NEURON_PROBABILITY_BIAS\"]) if params[\"OUTPUT_NEURON_PROBABILITY_BIAS\"] is not None else None,\n",
    "    outout_mlp_add_batchnorm=params[\"OUTPUT_MLP_BATCHNORM\"],\n",
    "    projection_head_num_layers=params[\"PROJECTION_HEAD_NUM_LAYERS\"],\n",
    "    projection_head_hidden_dim_scale_factor=params[\"PROJECTION_HEAD_HIDDEN_DIM_SCALE_FACTOR\"],\n",
    "\n",
    "    # Training options\n",
    "    label_encoder_num_trainable_layers=params[\"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\"],\n",
    "    train_sequence_encoder=params[\"TRAIN_SEQUENCE_ENCODER\"],\n",
    "\n",
    "    # Batch size limits\n",
    "    label_batch_size_limit=params[\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"],\n",
    "    sequence_batch_size_limit=params[\"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\"],\n",
    "\n",
    "    #\n",
    "    feature_fusion=config[\"params\"][\"FEATURE_FUSION\"],\n",
    "    temperature=config[\"params\"][\"SUPCON_TEMP\"]\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6,7,8,9,10]\n",
    "a[1:10:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from src.utils.data import load_gz_json, log_gpu_memory_usage, save_checkpoint, load_model\n",
    "from src.utils.evaluation import EvalMetrics,metric_collection_to_dict_float,save_evaluation_results\n",
    "from src.utils.losses import BatchWeightedBCE, FocalLoss, RGDBCE, WeightedBCE,SupCon, CBLoss\n",
    "from torchmetrics import MetricCollection, Metric\n",
    "from src.utils.proteinfer import normalize_confidences\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from transformers import BatchEncoding\n",
    "from src.utils.models import generate_label_embeddings_from_text, biogpt_train_last_n_layers\n",
    "from tqdm import tqdm\n",
    "from torcheval.metrics import MultilabelAUPRC, BinaryAUPRC\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "def print_checkpoint(checkpoint):\n",
    "    \n",
    "    print(\"weights_sum\",sum([i.sum() for i in checkpoint['model_state_dict'].values()]))\n",
    "    print('epoch',checkpoint['epoch'])\n",
    "    print('best_val_metric',checkpoint['best_val_metric'])\n",
    "\n",
    "    max_step = max(checkpoint['optimizer_state_dict']['state'].keys())\n",
    "    print('optimizer param groups',checkpoint['optimizer_state_dict']['param_groups'])\n",
    "    print('optimizer max step',checkpoint['optimizer_state_dict']['state'][max_step])\n",
    "\n",
    "def load_model(trainer, checkpoint_path, from_checkpoint=False):\n",
    "    \"\"\"\n",
    "    Load the model's state from a given checkpoint.\n",
    "\n",
    "    This function is designed to handle checkpoints from both Data Distributed Parallel (DDP) wrapped \n",
    "    and non-DDP models. If the checkpoint originates from a DDP-wrapped model, the function will adjust \n",
    "    the state dictionary keys accordingly before loading.\n",
    "\n",
    "    Parameters:\n",
    "    - trainer (object): An instance of the trainer containing the model, optimizer, and other training attributes.\n",
    "    - checkpoint_path (str): The path to the checkpoint file to be loaded.\n",
    "    - from_checkpoint (bool, optional): If True, the function will also load the optimizer's state, \n",
    "      epoch number, and best validation metric from the checkpoint. Defaults to False.\n",
    "\n",
    "    Note:\n",
    "    The function assumes that the model in the trainer object is DDP-wrapped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the entire checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Check if the state_dict is from a DDP-wrapped model\n",
    "    if list(state_dict.keys())[0].startswith('module.'):\n",
    "        # Remove the \"module.\" prefix\n",
    "        new_state_dict = OrderedDict()\n",
    "        for k, v in state_dict.items():\n",
    "            name = k[7:]  # remove 'module.' prefix\n",
    "            new_state_dict[name] = v\n",
    "        state_dict = new_state_dict\n",
    "\n",
    "    # Load the state_dict into the model\n",
    "    trainer.model.module.load_state_dict(state_dict)\n",
    "\n",
    "    # Load the optimizer state and epoch number if they exist in the checkpoint\n",
    "    if 'optimizer_state_dict' in checkpoint and from_checkpoint:\n",
    "        trainer.optimizer.load_state_dict(\n",
    "            checkpoint['optimizer_state_dict'])\n",
    "    if 'epoch' in checkpoint and from_checkpoint:\n",
    "        trainer.epoch = checkpoint['epoch']\n",
    "    if 'best_val_metric' in checkpoint and from_checkpoint:\n",
    "        trainer.best_val_metric = checkpoint['best_val_metric']\n",
    "\n",
    "    print_checkpoint(checkpoint)\n",
    "    # Delete the checkpoint to save memory\n",
    "    del checkpoint\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, best_val_metric, model_path):\n",
    "    \"\"\"\n",
    "    Save model and optimizer states as a checkpoint.\n",
    "\n",
    "    Args:\n",
    "    - model (torch.nn.Module): The model whose state we want to save.\n",
    "    - optimizer (torch.optim.Optimizer): The optimizer whose state we want to save.\n",
    "    - epoch (int): The current training epoch.\n",
    "    - model_path (str): The path where the checkpoint will be saved.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_val_metric': best_val_metric,\n",
    "    }\n",
    "\n",
    "    print_checkpoint(checkpoint)\n",
    "\n",
    "    torch.save(checkpoint, model_path)\n",
    "\n",
    "\n",
    "def load_checkpoint(trainer, checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load the model's state dict, optimizer's state, and epoch number from the checkpoint.\n",
    "\n",
    "    This function handles both DDP-wrapped and non-DDP checkpoints.\n",
    "\n",
    "    :param model: The model into which the checkpoint's state dict should be loaded.\n",
    "    :param trainer: The trainer instance containing the optimizer and epoch attributes.\n",
    "    :param checkpoint_path: Path to the checkpoint file.\n",
    "    \"\"\"\n",
    "    print_checkpoint(checkpoint)\n",
    "    # Load the entire checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "    # Extract the state_dict from the checkpoint\n",
    "    model_state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Check if the state_dict is from a DDP-wrapped model\n",
    "    if list(model_state_dict.keys())[0].startswith('module.'):\n",
    "        # Remove the \"module.\" prefix\n",
    "        new_model_state_dict = OrderedDict()\n",
    "        for k, v in model_state_dict.items():\n",
    "            name = k[7:]  # remove 'module.' prefix\n",
    "            new_model_state_dict[name] = v\n",
    "        model_state_dict = new_model_state_dict\n",
    "\n",
    "    # Load the state_dict into the model\n",
    "    trainer.model.module.load_state_dict(model_state_dict)\n",
    "\n",
    "    # Load the optimizer state and epoch number if they exist in the checkpoint\n",
    "    if 'optimizer_state_dict' in checkpoint:\n",
    "        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if 'epoch' in checkpoint:\n",
    "        trainer.epoch = checkpoint['epoch']\n",
    "    if 'best_val_metric' in checkpoint:\n",
    "        trainer.best_val_metric = checkpoint['best_val_metric']\n",
    "\n",
    "\n",
    "class ProTCLTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        device: str,\n",
    "        config: dict,\n",
    "        vocabularies: dict,\n",
    "        logger: logging.Logger,\n",
    "        timestamp: str,\n",
    "        run_name: str,\n",
    "        use_wandb: bool = False,\n",
    "        bce_pos_weight: torch.Tensor = None,\n",
    "        label_weights: torch.Tensor = None,\n",
    "        is_master: bool = True,\n",
    "        starting_epoch: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): The PyTorch model to train.\n",
    "            device (str): The device to use for training (e.g., 'cpu' or 'cuda').\n",
    "            logger (logging.Logger): The logger to use for logging training progress.\n",
    "            timestamp (str): The timestamp to use for naming log files and checkpoints.\n",
    "            run_name (str): The name of the current training run.\n",
    "            use_wandb (bool, optional): Whether to use Weights & Biases for logging. Defaults to False.\n",
    "            bce_pos_weight (torch.Tensor, optional): The positive weight for binary cross-entropy loss. Defaults to None.\n",
    "            is_master (bool, optional): Whether the current process is the master process. Defaults to True.\n",
    "            starting_epoch (int, optional): The starting epoch number. Defaults to 1. Used for resuming training.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        self.is_master = is_master\n",
    "        self.device = device\n",
    "        self.run_name = run_name\n",
    "        self.logger = logger\n",
    "        self.timestamp = timestamp\n",
    "        self.use_wandb = use_wandb\n",
    "        self.num_epochs = config[\"params\"][\"NUM_EPOCHS\"]\n",
    "        self.train_sequence_encoder = config[\"params\"][\"TRAIN_SEQUENCE_ENCODER\"]\n",
    "        self.label_encoder_num_trainable_layers = config[\"params\"][\"LABEL_ENCODER_NUM_TRAINABLE_LAYERS\"]\n",
    "        self.train_projection_head = config[\"params\"][\"TRAIN_PROJECTION_HEAD\"]\n",
    "\n",
    "        self.normalize_probabilities = config[\"params\"][\"NORMALIZE_PROBABILITIES\"]\n",
    "        self.validations_per_epoch = config[\"params\"][\"VALIDATIONS_PER_EPOCH\"]\n",
    "        self.gradient_accumulation_steps = config[\"params\"][\"GRADIENT_ACCUMULATION_STEPS\"]\n",
    "        self.clip_value = config[\"params\"][\"CLIP_VALUE\"]\n",
    "        self.vocabularies = vocabularies\n",
    "        self.label_normalizer = load_gz_json(\n",
    "            config[\"paths\"][\"PARENTHOOD_LIB_PATH\"]\n",
    "        )\n",
    "        self.output_model_dir = config[\"paths\"][\"OUTPUT_MODEL_DIR\"]\n",
    "        self.lora_params = {'rank':config[\"params\"][\"LORA_RANK\"],\n",
    "                            'in_features':config[\"params\"][\"LABEL_EMBEDDING_DIM\"],\n",
    "                            'out_features':config[\"params\"][\"LABEL_EMBEDDING_DIM\"],\n",
    "                            'device':self.device\n",
    "                            } if config[\"params\"][\"LORA\"] else None\n",
    "        \n",
    "        self._set_optimizer(opt_name = config[\"params\"][\"OPTIMIZER\"],\n",
    "                            lr = config[\"params\"][\"LEARNING_RATE\"])\n",
    "        \n",
    "        self.bce_pos_weight = bce_pos_weight\n",
    "        self.label_weights=label_weights\n",
    "        self.loss_fn = self._get_loss_fn(config)\n",
    "        self.model_path = self._get_saved_model_path()\n",
    "        self.best_val_metric = 0.0\n",
    "        self.scaler = GradScaler()\n",
    "        self.starting_epoch = starting_epoch\n",
    "        self.epoch = starting_epoch\n",
    "        self.config = config\n",
    "        self.tb = SummaryWriter(f\"runs/{self.run_name}_{self.timestamp}\") if self.is_master else None\n",
    "\n",
    "    def _get_saved_model_path(self):\n",
    "        # Save model to OUTPUT_MODEL_DIR. Create path if it doesn't exist.\n",
    "        if not os.path.exists(self.output_model_dir) and self.is_master:\n",
    "            os.makedirs(self.output_model_dir)\n",
    "\n",
    "        model_name = (\n",
    "            self.run_name if self.run_name else \"best_ProTCL.pt\"\n",
    "        )\n",
    "        model_path = os.path.join(\n",
    "            self.output_model_dir, f\"{self.timestamp}_{model_name}.pt\"\n",
    "        )\n",
    "        return model_path\n",
    "\n",
    "    def _get_loss_fn(self, config):\n",
    "        if config[\"params\"][\"LOSS_FN\"] == \"BCE\":\n",
    "            assert self.bce_pos_weight is not None, \"bce_pos_weight must be provided for BCE loss\"\n",
    "            return torch.nn.BCEWithLogitsLoss(reduction='mean', pos_weight=self.bce_pos_weight)\n",
    "        elif (config[\"params\"][\"LOSS_FN\"] == \"WeightedBCE\"):\n",
    "            assert self.label_weights is not None, \"label_weights must be provided for WeightedBCE Loss\"\n",
    "            return WeightedBCE(label_weights = self.label_weights)\n",
    "        elif (config[\"params\"][\"LOSS_FN\"] == \"CBLoss\"):\n",
    "            assert self.label_weights is not None, \"label_weights must be provided for CBLoss\"\n",
    "            return CBLoss(label_weights = self.label_weights)\n",
    "        elif config[\"params\"][\"LOSS_FN\"] == \"BatchWeightedBCE\":\n",
    "            return BatchWeightedBCE()\n",
    "        elif config[\"params\"][\"LOSS_FN\"] == \"FocalLoss\":\n",
    "            assert (config[\"params\"][\"FOCAL_LOSS_GAMMA\"] is not None)\\\n",
    "                & (config[\"params\"][\"FOCAL_LOSS_ALPHA\"] is not None), \"gamma and gamma must be provided for FocalLoss\"\n",
    "            return FocalLoss(gamma=config[\"params\"][\"FOCAL_LOSS_GAMMA\"], alpha=config[\"params\"][\"FOCAL_LOSS_ALPHA\"])\n",
    "        elif config[\"params\"][\"LOSS_FN\"] == \"RGDBCE\":\n",
    "            return RGDBCE(temp=config[\"params\"][\"RGDBCE_TEMP\"])\n",
    "        elif config[\"params\"][\"LOSS_FN\"] == \"SupCon\":\n",
    "            return SupCon(temperature=config[\"params\"][\"SUPCON_TEMP\"])\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown loss function {config['params']['LOSS_FN']}\")\n",
    "\n",
    "    def _to_device(self, *args):\n",
    "        processed_args = []\n",
    "        for item in args:\n",
    "            if isinstance(item, torch.Tensor):\n",
    "                processed_args.append(item.to(self.device))\n",
    "            elif isinstance(item, BatchEncoding) or isinstance(item, dict):\n",
    "                processed_dict = {k: v.to(self.device) if isinstance(\n",
    "                    v, torch.Tensor) else v for k, v in item.items()}\n",
    "                processed_args.append(processed_dict)\n",
    "            else:\n",
    "                processed_args.append(item)\n",
    "        return processed_args\n",
    "\n",
    "    def _set_optimizer(self, opt_name, lr):\n",
    "        trainable_params = []\n",
    "        trainable_params_names = []\n",
    "\n",
    "        # Use to unfreeze last n layers. 0 means entire model frozen.\n",
    "        biogpt_train_last_n_layers(self.model.module.label_encoder,\n",
    "                                   self.label_encoder_num_trainable_layers,\n",
    "                                   lora_params=self.lora_params\n",
    "                                   )\n",
    "        \n",
    "        for name, param in self.model.module.named_parameters():\n",
    "            if name.startswith('sequence_encoder') and (not self.train_sequence_encoder):\n",
    "                param.requires_grad = False\n",
    "\n",
    "            if (name.startswith('W_p.weight') or name.startswith('W_l.weight')) and (not self.train_projection_head):\n",
    "                param.requires_grad = False\n",
    "\n",
    "            if name.startswith('output_layer') and (not self.train_projection_head):\n",
    "                param.requires_grad = False\n",
    "\n",
    "            if param.requires_grad:\n",
    "                trainable_params.append(param)\n",
    "                trainable_params_names.append(name)\n",
    "\n",
    "        self.trainable_params_names = trainable_params_names\n",
    "\n",
    "        if opt_name == 'Adam':\n",
    "            opt = torch.optim.Adam\n",
    "        elif opt_name == 'SGD':\n",
    "            opt = torch.optim.SGD\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported optimizer name\")\n",
    "\n",
    "        self.optimizer = opt(\n",
    "            trainable_params, lr=lr\n",
    "        )\n",
    "\n",
    "    def evaluation_step(self, batch) -> tuple:\n",
    "        \"\"\"Perform a single evaluation step.\n",
    "\n",
    "        :param batch: _description_\n",
    "        :type batch: _type_\n",
    "        :return: batch loss, logits and labels\n",
    "        :rtype: tuple\n",
    "        \"\"\"\n",
    "\n",
    "        # Unpack the validation or testing batch\n",
    "        sequence_onehots, sequence_embeddings, sequence_lengths, sequence_ids, label_multihots, tokenized_labels, label_embeddings = (\n",
    "            batch[\"sequence_onehots\"],\n",
    "            batch[\"sequence_embeddings\"],\n",
    "            batch[\"sequence_lengths\"],\n",
    "            batch[\"sequence_ids\"],\n",
    "            batch[\"label_multihots\"],\n",
    "            batch[\"tokenized_labels\"],\n",
    "            batch[\"label_embeddings\"]\n",
    "        )\n",
    "\n",
    "        # Move all unpacked batch elements to GPU, if available\n",
    "        sequence_onehots, sequence_embeddings, sequence_lengths, label_multihots, tokenized_labels, label_embeddings = self._to_device(\n",
    "            sequence_onehots, sequence_embeddings, sequence_lengths, label_multihots, tokenized_labels, label_embeddings)\n",
    "\n",
    "        # Forward pass\n",
    "        inputs = {\n",
    "            \"sequence_onehots\": sequence_onehots,\n",
    "            \"sequence_embeddings\": sequence_embeddings,\n",
    "            \"sequence_lengths\": sequence_lengths,\n",
    "            \"tokenized_labels\": tokenized_labels,\n",
    "            \"label_embeddings\": label_embeddings\n",
    "        }\n",
    "        with autocast():\n",
    "            logits = self.model(**inputs)\n",
    "            # Compute validation loss for the batch\n",
    "            loss = self.loss_fn(logits, label_multihots.float())\n",
    "\n",
    "        return loss.item(), logits, label_multihots, sequence_ids\n",
    "\n",
    "    def validate(self,\n",
    "                 val_loader: torch.utils.data.DataLoader,\n",
    "                 eval_metrics: MetricCollection,\n",
    "                 val_optimization_metric_name: str\n",
    "                 ):\n",
    "\n",
    "        self.logger.info(\"Running validation...\")\n",
    "\n",
    "        prefix = 'validation'\n",
    "\n",
    "        val_metrics = self.evaluate(data_loader=val_loader,\n",
    "                                       eval_metrics=eval_metrics,\n",
    "                                       metrics_prefix=prefix)\n",
    "        val_optimization_metric_name = f'{prefix}_{val_optimization_metric_name}'\n",
    "\n",
    "        \n",
    "        self.logger.info(\"+-------------------------------- Validation Results --------------------------------+\")\n",
    "        # Print memory consumption\n",
    "        if self.is_master:\n",
    "            log_gpu_memory_usage(self.logger, 0)\n",
    "        self.logger.info(\n",
    "            f\"Validation metrics:\\n{json.dumps(val_metrics, indent=4)}\")\n",
    "\n",
    "        if self.use_wandb and self.is_master:\n",
    "            try:\n",
    "                if self.use_wandb and self.is_master:\n",
    "                    wandb.log(val_metrics,\n",
    "                              step=self.training_step\n",
    "                              )\n",
    "\n",
    "            except Exception as e:\n",
    "                self.logger.warning(\n",
    "                    f\"Failed to log validation metrics to wandb: {e}\")\n",
    "\n",
    "        # Save the model if it has the best validation loss so far (only on master node)\n",
    "        if self.is_master and val_metrics[val_optimization_metric_name] > self.best_val_metric and self.epoch==2:\n",
    "            self.logger.info(\n",
    "                f\"New best {val_optimization_metric_name}: {val_metrics[val_optimization_metric_name]}. Saving model...\"\n",
    "            )\n",
    "            self.best_val_metric = val_metrics[val_optimization_metric_name]\n",
    "\n",
    "            save_checkpoint(\n",
    "                model=self.model.module,\n",
    "                optimizer=self.optimizer,\n",
    "                epoch=self.epoch,\n",
    "                best_val_metric=self.best_val_metric,\n",
    "                model_path=self.model_path\n",
    "            )\n",
    "            self.logger.info(f\"Saved model to {self.model_path}.\")\n",
    "\n",
    "            if self.use_wandb:\n",
    "                wandb.save(f\"{self.timestamp}_best_ProTCL.pt\")\n",
    "        \n",
    "        self.logger.info(\"+------------------------------------------------------------------------------------+\") \n",
    "\n",
    "        return val_metrics\n",
    "\n",
    "    def find_optimal_threshold(\n",
    "        self, data_loader: torch.utils.data.DataLoader, optimization_metric_name: str\n",
    "    ) -> tuple[float, float]:\n",
    "        \"\"\"Find the optimal threshold for the given data loader.\n",
    "\n",
    "        :param data_loader: _description_\n",
    "        :type data_loader: torch.utils.data.DataLoader\n",
    "        :param average: _description_\n",
    "        :type average: Literal[&#39;micro&#39;, &#39;macro&#39;, &#39;weighted&#39;]\n",
    "        :param optimization_metric_name: _description_\n",
    "        :type optimization_metric_name: str\n",
    "        :return: _description_\n",
    "        :rtype: tuple[float, float]\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger.info(\"Finding optimal threshold...\")\n",
    "        self.model.eval()\n",
    "\n",
    "        best_th = 0.0\n",
    "        best_score = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_probabilities = []\n",
    "            all_label_multihots = []\n",
    "            for batch in data_loader:\n",
    "                _, logits, label_multihots, _ = self.evaluation_step(\n",
    "                    batch=batch)\n",
    "\n",
    "                # Apply sigmoid to get the probabilities for multi-label classification\n",
    "                probabilities = torch.sigmoid(logits)\n",
    "\n",
    "                if self.normalize_probabilities:\n",
    "                    probabilities = self._normalize_probabilities(probabilities)\n",
    "\n",
    "                all_probabilities.append(probabilities)\n",
    "                all_label_multihots.append(label_multihots)\n",
    "\n",
    "            all_probabilities = torch.cat(all_probabilities)\n",
    "            all_label_multihots = torch.cat(all_label_multihots)\n",
    "\n",
    "        for th in np.arange(0.1, 1, 0.01):\n",
    "            optimization_metric = EvalMetrics(device=self.device)\\\n",
    "                .get_metric_by_name(name=optimization_metric_name,\n",
    "                                    threshold=th,\n",
    "                                    num_labels=label_multihots.shape[-1])\n",
    "\n",
    "            optimization_metric(all_probabilities, all_label_multihots)\n",
    "            score = optimization_metric.compute().item()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_th = th\n",
    "            self.logger.info(\"TH: {:.3f}, F1: {:.3f}\".format(th, score))\n",
    "\n",
    "        best_score = best_score\n",
    "        self.logger.info(\n",
    "            f\"Best validation score: {best_score}, Best val threshold: {best_th}\"\n",
    "        )\n",
    "        self.model.train()\n",
    "        return best_th, best_score\n",
    "\n",
    "    def _normalize_probabilities(self,probabilities):\n",
    "        # TODO: Using original normalize_confidences implemented with numpy,\n",
    "                    # but this is slow. Should be able to do this with torch tensors.\n",
    "        return torch.tensor(\n",
    "                    normalize_confidences(\n",
    "                        predictions=probabilities.detach().cpu().numpy(),\n",
    "                        label_vocab=self.vocabularies[\"GO_label_vocab\"],\n",
    "                        applicable_label_dict=self.label_normalizer,\n",
    "                    ),\n",
    "                    device=self.device,\n",
    "                )\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        data_loader: torch.utils.data.DataLoader,\n",
    "        eval_metrics: MetricCollection = None,\n",
    "        save_results: bool = False,\n",
    "        metrics_prefix = None\n",
    "    ) -> tuple[dict, dict]:\n",
    "        \"\"\"Evaluate the model on the given data loader.\n",
    "        :param data_loader: pytorch data loader\n",
    "        :type data_loader: torch.utils.data.DataLoader\n",
    "        :param eval_metrics: an eval metrics class to calculate metrics like F1 score, defaults to None\n",
    "        :type eval_metrics: EvalMetrics, optional\n",
    "        :return: dictionary with evaluation metrics. Always return avg_loss and if eval_metrics is not None, it will return the metrics from eval_metrics.compute()\n",
    "        :rtype: dict\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Compute all label embeddings upfront, since we're not training\n",
    "        if data_loader.dataset.label_embedding_matrix is None:\n",
    "            logging.info(\n",
    "                \"Computing label embeddings for evaluation...\")\n",
    "            with torch.no_grad():\n",
    "                label_embedding_matrix = generate_label_embeddings_from_text(\n",
    "                    data_loader.dataset.label_text_list,\n",
    "                    data_loader.dataset.label_tokenizer,\n",
    "                    self.model.module.label_encoder,\n",
    "                    self.config[\"params\"][\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"],\n",
    "                ).cpu()\n",
    "            data_loader.dataset.set_label_embedding_matrix(\n",
    "                label_embedding_matrix)\n",
    "            logging.info(\"Done computing label embeddings.\")\n",
    "\n",
    "        total_loss = 0\n",
    "        test_results = defaultdict(list)\n",
    "\n",
    "        if eval_metrics is not None:\n",
    "            eval_metrics.reset()\n",
    "\n",
    "        mAP_micro = BinaryAUPRC(device='cpu')\n",
    "        mAP_macro = MultilabelAUPRC(device='cpu',num_labels=len(self.vocabularies[\"GO_label_vocab\"]))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(data_loader,total=len(data_loader)):\n",
    "                loss, logits, labels, sequence_ids = self.evaluation_step(\n",
    "                    batch=batch)\n",
    "                if eval_metrics is not None:\n",
    "                    # Apply sigmoid to get the probabilities for multi-label classification\n",
    "                    probabilities = torch.sigmoid(logits)\n",
    "\n",
    "                    if self.normalize_probabilities:\n",
    "                        probabilities = self._normalize_probabilities()\n",
    "\n",
    "                    # Update eval metrics\n",
    "                    eval_metrics(probabilities, labels)\n",
    "\n",
    "                    mAP_micro.update(probabilities.cpu().flatten(), labels.cpu().flatten())\n",
    "                    mAP_macro.update(probabilities.cpu(), labels.cpu())\n",
    "\n",
    "                    # No need to save results everytime. Only need it for final evaluation.\n",
    "                    if save_results:\n",
    "                        test_results[\"sequence_ids\"].append(sequence_ids)\n",
    "                        test_results[\"logits\"].append(logits.cpu())\n",
    "                        test_results[\"labels\"].append(labels.cpu())\n",
    "\n",
    "                # Accumulate loss\n",
    "                total_loss += loss\n",
    "\n",
    "            if save_results:\n",
    "                for key in test_results.keys():\n",
    "                    if key == \"sequence_ids\":\n",
    "                        test_results[key] = (\n",
    "                            np.array(\n",
    "                                [j for i in test_results[\"sequence_ids\"] for j in i])\n",
    "                        )\n",
    "                    else:\n",
    "                        test_results[key] = (\n",
    "                            torch.cat(test_results[key]).numpy()\n",
    "                        )\n",
    "                \n",
    "                self.logger.info(\"Saving validation results...\")\n",
    "                if self.is_master:\n",
    "                    save_evaluation_results(results=test_results,\n",
    "                                            label_vocabulary=self.vocabularies[\"GO_label_vocab\"],\n",
    "                                            run_name=self.run_name,\n",
    "                                            output_dir=self.config[\"paths\"][\"RESULTS_DIR\"],\n",
    "                                            data_split_name=metrics_prefix\n",
    "                                            )\n",
    "\n",
    "            # Compute average validation loss\n",
    "            avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "            final_metrics = eval_metrics.compute() if eval_metrics is not None else {}\n",
    "            final_metrics.update({\"loss\": avg_loss,\n",
    "                                  \"map_micro\":mAP_micro.compute(),\n",
    "                                  \"map_macro\":mAP_macro.compute()\n",
    "                                  })\n",
    "\n",
    "            final_metrics = metric_collection_to_dict_float(\n",
    "                final_metrics,\n",
    "                prefix=metrics_prefix)           \n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        return final_metrics\n",
    "\n",
    "    def train_one_epoch(self,\n",
    "                        train_loader: torch.utils.data.DataLoader,\n",
    "                        eval_metrics: MetricCollection\n",
    "        ):\n",
    "        \n",
    "        avg_loss = 0\n",
    "        avg_probs = 0\n",
    "        avg_gt = 0\n",
    "        eval_metrics.reset()\n",
    "        \n",
    "        ####### TRAINING LOOP #######\n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            \n",
    "            self.training_step += 1\n",
    "\n",
    "            # Unpack the training batch\n",
    "            sequence_onehots, sequence_embeddings, sequence_lengths, label_multihots, tokenized_labels, label_embeddings = (\n",
    "                batch[\"sequence_onehots\"],\n",
    "                batch[\"sequence_embeddings\"],\n",
    "                batch[\"sequence_lengths\"],\n",
    "                batch[\"label_multihots\"],\n",
    "                batch[\"tokenized_labels\"],\n",
    "                batch[\"label_embeddings\"]\n",
    "            )\n",
    "\n",
    "            # Move all unpacked batch elements to GPU, if available\n",
    "            sequence_onehots, sequence_embeddings, sequence_lengths, label_multihots, tokenized_labels, label_embeddings = self._to_device(\n",
    "                sequence_onehots, sequence_embeddings, sequence_lengths, label_multihots, tokenized_labels, label_embeddings)\n",
    "\n",
    "            # Forward pass\n",
    "            inputs = {\n",
    "                \"sequence_onehots\": sequence_onehots,\n",
    "                \"sequence_embeddings\": sequence_embeddings,\n",
    "                \"sequence_lengths\": sequence_lengths,\n",
    "                \"tokenized_labels\": tokenized_labels,\n",
    "                \"label_embeddings\": label_embeddings\n",
    "            }\n",
    "\n",
    "            with autocast():\n",
    "                logits = self.model(**inputs)\n",
    "\n",
    "                # Compute loss, normalized by the number of gradient accumulation steps\n",
    "                loss = self.loss_fn(logits, label_multihots.float()) / \\\n",
    "                    self.gradient_accumulation_steps\n",
    "        \n",
    "            # Backward pass with mixed precision\n",
    "            self.scaler.scale(loss).backward()\n",
    "        \n",
    "\n",
    "            # Gradient accumulation every GRADIENT_ACCUMULATION_STEPS\n",
    "            if (self.training_step % self.gradient_accumulation_steps == 0) or (batch_idx + 1 == len(train_loader)):     \n",
    "                # Unscales the gradients of optimizer's assigned params in-place\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                \n",
    "                # Apply gradient clipping\n",
    "                if self.clip_value is not None:\n",
    "                    clip_grad_norm_(self.model.module.parameters(),\n",
    "                                    max_norm=self.clip_value)\n",
    "                \n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "\n",
    "                #Log at this point to TB to have weights and gradients after a full epoch\n",
    "                if (batch_idx + 1 == len(train_loader)) & self.is_master:\n",
    "                    for name, weight in self.model.module.named_parameters():\n",
    "                        if weight.requires_grad:\n",
    "                            self.tb.add_histogram(name,weight, self.epoch)\n",
    "                            self.tb.add_histogram(f'{name}.grad',weight.grad, self.epoch)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss+=loss.item()\n",
    "            avg_probs += torch.mean(torch.sigmoid(logits).detach())\n",
    "            avg_gt += torch.mean(label_multihots.float().detach())\n",
    "\n",
    "            eval_metrics(logits, label_multihots)\n",
    "            \n",
    "            if self.use_wandb:\n",
    "                wandb.log({\"per_batch_train_loss\": loss.item()},\n",
    "                          step=self.training_step\n",
    "                          )\n",
    "\n",
    "            # Print memory consumption after first batch (to get the max memory consumption during training)\n",
    "            if batch_idx == 1 and self.is_master:\n",
    "                self.logger.info(\"+----------------- Train GPU Memory Usage -----------------+\")\n",
    "                log_gpu_memory_usage(self.logger, 0)\n",
    "                self.logger.info(\"+----------------------------------------------------------+\")\n",
    "\n",
    "        avg_loss = avg_loss/len(train_loader) if len(train_loader)> 0 else avg_loss\n",
    "        avg_probs_gt_ration = avg_probs/avg_gt\n",
    "\n",
    "        train_metrics = eval_metrics.compute() if eval_metrics is not None else {}\n",
    "        train_metrics.update({\"loss\": avg_loss,\n",
    "                              \"avg_probabilities_ground_truth_ratio\":avg_probs_gt_ration,\n",
    "                                })\n",
    "        train_metrics = metric_collection_to_dict_float(train_metrics,prefix='train')\n",
    "        \n",
    "        if self.use_wandb:\n",
    "            wandb.log(train_metrics,\n",
    "                      step=self.training_step\n",
    "                      )\n",
    "\n",
    "        \n",
    "        return train_metrics\n",
    "        \n",
    "        \n",
    "    def train(\n",
    "        self,\n",
    "        train_loader: torch.utils.data.DataLoader,\n",
    "        val_loader: torch.utils.data.DataLoader,\n",
    "        train_eval_metrics: MetricCollection,\n",
    "        val_eval_metrics: MetricCollection,\n",
    "        val_optimization_metric_name: str\n",
    "    ):\n",
    "        \"\"\"Train model\n",
    "        :param train_loader: _description_\n",
    "        :type train_loader: torch.utils.data.DataLoader\n",
    "        :param val_loader: _description_\n",
    "        :type val_loader: torch.utils.data.DataLoader\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        # Watch the model\n",
    "        if self.use_wandb:\n",
    "            wandb.watch(self.model)\n",
    "\n",
    "        # Compute total number of training steps\n",
    "        self.training_step = 0\n",
    "        num_training_steps = len(train_loader) * self.num_epochs\n",
    "        \n",
    "        self.logger.info(f\"{'='*100}\")\n",
    "        self.logger.info(\n",
    "            f\"Starting training. Total number of training steps: {num_training_steps}\")\n",
    "        self.logger.info(f\"{'='*100}\")\n",
    "\n",
    "        for epoch in range(self.starting_epoch, self.starting_epoch + self.num_epochs):\n",
    "            self.logger.info(\n",
    "                f\"Starting epoch {epoch}/{self.starting_epoch + self.num_epochs - 1}...\")\n",
    "            self.epoch = epoch\n",
    "\n",
    "            # Set distributed loader epoch to shuffle data\n",
    "            if hasattr(train_loader.sampler, \"set_epoch\"):\n",
    "                train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "            train_metrics = self.train_one_epoch(train_loader=train_loader,\n",
    "                                                 eval_metrics=train_eval_metrics)\n",
    "                \n",
    "\n",
    "            if (epoch % self.validations_per_epoch == 0):\n",
    "                ####### VALIDATION LOOP #######\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                # Run validation\n",
    "                self.validate(val_loader=val_loader,\n",
    "                                            eval_metrics=val_eval_metrics,\n",
    "                                            val_optimization_metric_name=val_optimization_metric_name\n",
    "                                            )\n",
    "\n",
    "                if self.label_encoder_num_trainable_layers>0:\n",
    "                    # Clear the label embedding matrix\n",
    "                    val_loader.dataset.set_label_embedding_matrix(None)\n",
    "\n",
    "                self.logger.info(\n",
    "                    f\"Epoch {epoch}/{self.starting_epoch + self.num_epochs - 1}, Batch {self.training_step}, Training Loss: {train_metrics['train_loss']}\"\n",
    "                )\n",
    "\n",
    "        if self.is_master:\n",
    "            self.logger.info(\"Restoring model to best validation map_micro...\")\n",
    "            load_model(trainer=self,\n",
    "                            checkpoint_path=self.model_path)\n",
    "\n",
    "        \n",
    "        self.tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"checkpoint_path =  'data/models/ProTCL/2023-11-27_17-07-08_FL_Experiments_mlp_scale2_2layer_projection_head_gamma2_lr3e4_bs32.pt'\\ncheckpoint = torch.load(checkpoint_path)\\nstate_dict = checkpoint['model_state_dict']\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''checkpoint_path =  'data/models/ProTCL/2023-11-27_17-07-08_FL_Experiments_mlp_scale2_2layer_projection_head_gamma2_lr3e4_bs32.pt'\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "state_dict = checkpoint['model_state_dict']\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize EvalMetrics\n",
    "eval_metrics = EvalMetrics(device=device)\n",
    "\n",
    "label_sample_sizes = {k:(v if v is not None else len(vocabularies['GO_label_vocab'])) \n",
    "                        for k,v in label_sample_sizes.items()}\n",
    "\n",
    "# Log sizes of all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:33:35 PST INFO ====================================================================================================\n",
      "2023-12-18 18:33:35 PST INFO Starting training. Total number of training steps: 270\n",
      "2023-12-18 18:33:35 PST INFO ====================================================================================================\n",
      "2023-12-18 18:33:35 PST INFO Starting epoch 1/3...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:33:37 PST INFO +----------------- Train GPU Memory Usage -----------------+\n",
      "2023-12-18 18:33:37 PST INFO GPU memory occupied: 7568 MB (9.34% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:33:37 PST INFO +----------------------------------------------------------+\n",
      "2023-12-18 18:33:46 PST INFO Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:33:52 PST INFO +-------------------------------- Validation Results --------------------------------+\n",
      "2023-12-18 18:33:52 PST INFO GPU memory occupied: 5082 MB (6.28% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:33:52 PST INFO Validation metrics:\n",
      "{\n",
      "    \"validation_f1_macro\": 0.0001800328609533608,\n",
      "    \"validation_f1_micro\": 0.10717897117137909,\n",
      "    \"validation_loss\": 0.0019823297043330967,\n",
      "    \"validation_map_micro\": 0.269729882478714,\n",
      "    \"validation_map_macro\": 0.007014680188149214\n",
      "}\n",
      "2023-12-18 18:33:52 PST INFO +------------------------------------------------------------------------------------+\n",
      "2023-12-18 18:33:52 PST INFO Epoch 1/3, Batch 90, Training Loss: 0.019239841347249844\n",
      "2023-12-18 18:33:52 PST INFO Starting epoch 2/3...\n",
      "2023-12-18 18:33:52 PST INFO +----------------- Train GPU Memory Usage -----------------+\n",
      "2023-12-18 18:33:52 PST INFO GPU memory occupied: 7592 MB (9.37% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:33:52 PST INFO +----------------------------------------------------------+\n",
      "2023-12-18 18:34:01 PST INFO Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:34:06 PST INFO +-------------------------------- Validation Results --------------------------------+\n",
      "2023-12-18 18:34:06 PST INFO GPU memory occupied: 5082 MB (6.28% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:34:06 PST INFO Validation metrics:\n",
      "{\n",
      "    \"validation_f1_macro\": 0.00027972637326456606,\n",
      "    \"validation_f1_micro\": 0.1364092230796814,\n",
      "    \"validation_loss\": 0.0018234694376587867,\n",
      "    \"validation_map_micro\": 0.35204529762268066,\n",
      "    \"validation_map_macro\": 0.008132383227348328\n",
      "}\n",
      "2023-12-18 18:34:06 PST INFO New best validation_map_micro: 0.35204529762268066. Saving model...\n",
      "weights_sum tensor(6714471.5000, device='cuda:0')\n",
      "epoch 2\n",
      "best_val_metric 0.35204529762268066\n",
      "optimizer param groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]\n",
      "optimizer max step {'step': tensor(179.), 'exp_avg': tensor([-0.0002], device='cuda:0'), 'exp_avg_sq': tensor([1.0297e-05], device='cuda:0')}\n",
      "2023-12-18 18:34:07 PST INFO Saved model to ./outputs/checkpoints/2023-12-18_18-32-47_debugging.pt.\n",
      "2023-12-18 18:34:07 PST INFO +------------------------------------------------------------------------------------+\n",
      "2023-12-18 18:34:07 PST INFO Epoch 2/3, Batch 180, Training Loss: 0.002180853038508859\n",
      "2023-12-18 18:34:07 PST INFO Starting epoch 3/3...\n",
      "2023-12-18 18:34:08 PST INFO +----------------- Train GPU Memory Usage -----------------+\n",
      "2023-12-18 18:34:08 PST INFO GPU memory occupied: 7592 MB (9.37% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:34:08 PST INFO +----------------------------------------------------------+\n",
      "2023-12-18 18:34:16 PST INFO Running validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 18:34:22 PST INFO +-------------------------------- Validation Results --------------------------------+\n",
      "2023-12-18 18:34:22 PST INFO GPU memory occupied: 5082 MB (6.28% of total memory 80994 MB). Device 0 [Name: NVIDIA A100 80GB PCIe]\n",
      "2023-12-18 18:34:22 PST INFO Validation metrics:\n",
      "{\n",
      "    \"validation_f1_macro\": 0.0006659323698841035,\n",
      "    \"validation_f1_micro\": 0.3161482810974121,\n",
      "    \"validation_loss\": 0.0016556073911488055,\n",
      "    \"validation_map_micro\": 0.3641241788864136,\n",
      "    \"validation_map_macro\": 0.009125406853854656\n",
      "}\n",
      "2023-12-18 18:34:22 PST INFO +------------------------------------------------------------------------------------+\n",
      "2023-12-18 18:34:22 PST INFO Epoch 3/3, Batch 270, Training Loss: 0.0018972821669497838\n",
      "2023-12-18 18:34:22 PST INFO Restoring model to best validation map_micro...\n",
      "weights_sum tensor(6714471.5000, device='cuda:0')\n",
      "epoch 2\n",
      "best_val_metric 0.35204529762268066\n",
      "optimizer param groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]\n",
      "optimizer max step {'step': tensor(179.), 'exp_avg': tensor([-0.0002], device='cuda:0'), 'exp_avg_sq': tensor([1.0297e-05], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# Load the entire checkpoint\n",
    "\n",
    "\n",
    "Trainer = ProTCLTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    vocabularies=vocabularies,\n",
    "    logger=logger,\n",
    "    timestamp=timestamp,\n",
    "    run_name='debugging',\n",
    "    use_wandb=False,\n",
    "    bce_pos_weight=None,\n",
    "    label_weights=None,\n",
    "    is_master=is_master,\n",
    ")\n",
    "\n",
    "Trainer.train(train_loader=loaders[\"train\"][0],\n",
    "    val_loader=loaders[\"validation\"][0],\n",
    "    train_eval_metrics=eval_metrics.get_metric_collection_with_regex(pattern=\"f1_m.*\",\n",
    "                                                                        threshold=0.5,\n",
    "                                                                num_labels=label_sample_sizes[\"train\"] if (params['IN_BATCH_SAMPLING'] or params['GRID_SAMPLER']) is False else None\n",
    "                                                                ),\n",
    "    val_eval_metrics=eval_metrics.get_metric_collection_with_regex(pattern=\"f1_m.*\", threshold=0.5,\n",
    "                                                        num_labels=label_sample_sizes[\"validation\"]\n",
    "                                                        ),\n",
    "    val_optimization_metric_name=params[\"OPTIMIZATION_METRIC_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights_sum tensor(6714471.5000, device='cuda:0')\n",
      "epoch 2\n",
      "best_val_metric 0.35204529762268066\n",
      "optimizer param groups [{'lr': 0.0003, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]}]\n",
      "optimizer max step {'step': tensor(179.), 'exp_avg': tensor([-0.0002], device='cuda:0'), 'exp_avg_sq': tensor([1.0297e-05], device='cuda:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  9.55it/s]\n"
     ]
    }
   ],
   "source": [
    "Trainer = ProTCLTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    config=config,\n",
    "    vocabularies=vocabularies,\n",
    "    logger=logger,\n",
    "    timestamp=timestamp,\n",
    "    run_name='debugging',\n",
    "    use_wandb=False,\n",
    "    bce_pos_weight=None,\n",
    "    label_weights=None,\n",
    "    is_master=is_master,\n",
    ")\n",
    "\n",
    "\n",
    "load_model(\n",
    "    trainer=Trainer,\n",
    "    checkpoint_path=os.path.join(config[\"DATA_PATH\"], '../outputs/checkpoints/2023-12-18_18-32-47_debugging.pt'),\n",
    "    from_checkpoint=False\n",
    ")\n",
    "\n",
    "\n",
    "validation_metrics = Trainer.evaluate(\n",
    "            data_loader=loaders['validation'][0],\n",
    "            eval_metrics=eval_metrics.get_metric_collection_with_regex(pattern=\"f1_m.*\",\n",
    "                                                                    threshold=0.5,\n",
    "                                                                    num_labels=label_sample_sizes[\"validation\"]\n",
    "                                                            ),\n",
    "            save_results=False,\n",
    "            metrics_prefix='final_validation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'final_validation_f1_macro': 0.00027972637326456606,\n",
       " 'final_validation_f1_micro': 0.1364092230796814,\n",
       " 'final_validation_loss': 0.0018234694376587867,\n",
       " 'final_validation_map_micro': 0.35204529762268066,\n",
       " 'final_validation_map_macro': 0.008132383227348328}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"validation_f1_macro\": 0.00027972637326456606,\n",
    "    \"validation_f1_micro\": 0.1364092230796814,\n",
    "    \"validation_loss\": 0.0018234694376587867,\n",
    "    \"validation_map_micro\": 0.35204529762268066,\n",
    "    \"validation_map_macro\": 0.008132383227348328\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [03:56<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 06:07:15 PST INFO Saving validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_validation_loss': 0.0011481819404962755,\n",
       " 'final_validation_map_micro': 0.8853791952133179,\n",
       " 'final_validation_map_macro': 0.30902916193008423}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = EvalMetrics(device=device)\n",
    "\n",
    "trainer.evaluate(\n",
    "            data_loader=loaders[\"validation\"][0],\n",
    "            eval_metrics=eval_metrics.get_metric_collection_with_regex(pattern=\"f1_m.*\",\n",
    "                                                                    threshold=0.5,\n",
    "                                                                    num_labels=label_sample_sizes[\"validation\"]\n",
    "                                                            ),\n",
    "            save_results=True,\n",
    "            metrics_prefix='final_validation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [03:27<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-18 05:47:21 PST INFO Saving validation results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'final_validation_loss': 0.0011481819404962755,\n",
       " 'final_validation_map_micro': 0.8853791952133179,\n",
       " 'final_validation_map_macro': 0.30902916193008423}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = EvalMetrics(device=device)\n",
    "\n",
    "trainer.evaluate(\n",
    "            data_loader=loaders[\"validation\"][0],\n",
    "            eval_metrics=eval_metrics.get_metric_collection_with_regex(pattern=\"f1_m.*\",\n",
    "                                                                    threshold=0.5,\n",
    "                                                                    num_labels=label_sample_sizes[\"validation\"]\n",
    "                                                            ),\n",
    "            save_results=True,\n",
    "            metrics_prefix='final_validation'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataLoader' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "loaders[\"validation\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Literal\n",
    "from torchdata.datapipes.iter import FileLister, FileOpener\n",
    "import argparse\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "from tqdm import tqdm\n",
    "from src.utils.data import read_fasta\n",
    "\n",
    "\n",
    "def process_sequence_tfrecord(record: dict, annotation_types: list):\n",
    "\n",
    "    sequence = record['sequence'][0].decode()\n",
    "    id = record['id'][0].decode()\n",
    "\n",
    "    labels = set()\n",
    "\n",
    "    # Some rows have no lavel column\n",
    "    if 'label' not in record:\n",
    "        return None\n",
    "\n",
    "    # Add all labels from desired annotation types\n",
    "    for l in record['label']:\n",
    "        label = l.decode()\n",
    "        label_type = label.split(':')[0]\n",
    "\n",
    "        if (label_type in annotation_types):\n",
    "            labels.add(label)\n",
    "\n",
    "    # Sequence with no annotation from selected types\n",
    "    if not labels:\n",
    "        return None\n",
    "\n",
    "    return id, (sequence, list(labels))\n",
    "\n",
    "\n",
    "def process_tfrecords(input_dir: str,\n",
    "                      output_dir: str,\n",
    "                      annotation_types: list,\n",
    "                      pattern: str,\n",
    "                      pattern_name: str\n",
    "                      ):\n",
    "    # Load all tfrecords from desired data split\n",
    "    datapipe1 = FileLister(input_dir, pattern)\n",
    "    datapipe2 = FileOpener(datapipe1, mode=\"b\")\n",
    "    tfrecord_loader_dp = datapipe2.load_from_tfrecord()\n",
    "\n",
    "    records = []\n",
    "    # Iterate over records, process and write to a fasta file\n",
    "    for _, record in tqdm(enumerate(tfrecord_loader_dp)):\n",
    "        processed_sequence = process_sequence_tfrecord(\n",
    "            record, annotation_types)\n",
    "\n",
    "        # Skipping sequence with no labels from desired annotations\n",
    "        if processed_sequence is None:\n",
    "            continue\n",
    "\n",
    "        id, (sequence, labels) = processed_sequence\n",
    "\n",
    "        description = \" \".join(labels)\n",
    "        record = SeqRecord(Seq(sequence), id=f\"{id}\", description=description)\n",
    "        records.append(record)\n",
    "\n",
    "    with open(os.path.join(output_dir, f\"{pattern_name}_{'_'.join(annotation_types)}.fasta\"), \"w\") as output_handle:\n",
    "        SeqIO.write(records, output_handle, \"fasta\")\n",
    "\n",
    "\n",
    "dirname = os.path.dirname(__file__)\n",
    "# TODO: This paths should be in config or something\n",
    "# input_dir = os.path.join(dirname, 'data/swissprot/proteinfer_splits/random/')\n",
    "# output_dir = os.path.join(dirname, 'data/swissprot/proteinfer_splits/random/')\n",
    "\n",
    "patterns = {'train': 'train*.tfrecord',\n",
    "            'dev': 'dev*.tfrecord',\n",
    "            'test': 'test*.tfrecord',\n",
    "            'full': '*.tfrecord'}\n",
    "\n",
    "for pattern_name, pattern in patterns.items():\n",
    "    logging.info(f'Processing {pattern_name}')\n",
    "    process_tfrecords(input_dir=args.input_dir,\n",
    "                        output_dir=args.output_dir,\n",
    "                        annotation_types=args.annotation_types,\n",
    "                        pattern=pattern,\n",
    "                        pattern_name=pattern_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Obtaining dependency information for torchdata from https://files.pythonhosted.org/packages/39/18/6f0d33df4b9fe4d44a779c2c7cc7cb042535a336f051bb0e5b5387844ee6/torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting urllib3>=1.25 (from torchdata)\n",
      "  Obtaining dependency information for urllib3>=1.25 from https://files.pythonhosted.org/packages/96/94/c31f58c7a7f470d5665935262ebd7455c7e4c7782eb525658d3dbf4b9403/urllib3-2.1.0-py3-none-any.whl.metadata\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting requests (from torchdata)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting torch>=2 (from torchdata)\n",
      "  Obtaining dependency information for torch>=2 from https://files.pythonhosted.org/packages/96/82/0966469ded5946cb4c18dd11b04eac78c943269fc79d290740d6477005e8/torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl.metadata (25 kB)\n",
      "Collecting filelock (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/81/54/84d42a0bee35edba99dee7b59a8d4970eccdd44b99fe728ed912106fc781/filelock-3.13.1-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typing-extensions (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for typing-extensions from https://files.pythonhosted.org/packages/24/21/7d397a4b7934ff4028987914ac1044d3b7d52712f30e2ac7a2ae5bc86dd0/typing_extensions-4.8.0-py3-none-any.whl.metadata\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch>=2->torchdata)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/d5/f0/8fbc882ca80cf077f1b246c0e3c3465f7f415439bdea6b899f6b19f61f70/networkx-3.2.1-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=2->torchdata)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/67/32/9276db0647d8142da3d9ec1af536522081813005a9d7aaebbdba082967c1/fsspec-2023.12.0-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.12.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==8.9.2.26 from https://files.pythonhosted.org/packages/ff/74/a2e2be7fb83aaedec84f391f082cf765dfb635e7caa9b49065f73e4835d8/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0 (from torch>=2->torchdata)\n",
      "  Obtaining dependency information for triton==2.1.0 from https://files.pythonhosted.org/packages/4d/22/91a8af421c8a8902dde76e6ef3db01b258af16c53d81e8c0d0dc13900a9e/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/1e/07/bf730d44c2fe1b676ad9cc2be5f5f861eb5d153fb6951987a2d6a96379a9/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchdata)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/da/f1/3702ba2a7470666a62fd81c58a4c40be00670e5006a67f4d626e57f013ae/charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->torchdata)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/c2/e7/a82b05cf63a603df6e68d59ae6a68bf5064484a0718ea5033660af4b54a9/idna-3.6-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchdata)\n",
      "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/64/62/428ef076be88fa93716b576e4a01f919d25968913e817077a386fcbe4f42/certifi-2023.11.17-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2->torchdata)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/12/b3/d9ed2c0971e1435b8a62354b18d3060b66c8cb1d368399ec0b9baa7c0ee5/MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=2->torchdata)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.1.1-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m666.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m560.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2023.12.0-py3-none-any.whl (168 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Using cached MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, urllib3, typing-extensions, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchdata\n",
      "  Attempting uninstall: mpmath\n",
      "    Found existing installation: mpmath 1.3.0\n",
      "    Uninstalling mpmath-1.3.0:\n",
      "      Successfully uninstalled mpmath-1.3.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.16\n",
      "    Uninstalling urllib3-1.26.16:\n",
      "      Successfully uninstalled urllib3-1.26.16\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.11.1\n",
      "    Uninstalling sympy-1.11.1:\n",
      "      Successfully uninstalled sympy-1.11.1\n",
      "  Attempting uninstall: networkx\n",
      "    Found existing installation: networkx 3.1\n",
      "    Uninstalling networkx-3.1:\n",
      "      Successfully uninstalled networkx-3.1\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.1\n",
      "    Uninstalling MarkupSafe-2.1.1:\n",
      "      Successfully uninstalled MarkupSafe-2.1.1\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.9.2\n",
      "    Uninstalling fsspec-2023.9.2:\n",
      "      Successfully uninstalled fsspec-2023.9.2\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.9.0\n",
      "    Uninstalling filelock-3.9.0:\n",
      "      Successfully uninstalled filelock-3.9.0\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.7.22\n",
      "    Uninstalling certifi-2023.7.22:\n",
      "      Successfully uninstalled certifi-2023.7.22\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 2.0.0\n",
      "    Uninstalling triton-2.0.0:\n",
      "      Successfully uninstalled triton-2.0.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.29.0\n",
      "    Uninstalling requests-2.29.0:\n",
      "      Successfully uninstalled requests-2.29.0\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.2\n",
      "    Uninstalling Jinja2-3.1.2:\n",
      "      Successfully uninstalled Jinja2-3.1.2\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.0.1\n",
      "    Uninstalling torch-2.0.1:\n",
      "      Successfully uninstalled torch-2.0.1\n",
      "  Attempting uninstall: torchdata\n",
      "    Found existing installation: torchdata 0.4.1+f9ecd8b\n",
      "    Uninstalling torchdata-0.4.1+f9ecd8b:\n",
      "      Successfully uninstalled torchdata-0.4.1+f9ecd8b\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "azureml-core 1.52.0 requires urllib3<2.0.0,>=1.23, but you have urllib3 2.1.0 which is incompatible.\n",
      "azureml-dataset-runtime 1.53.0 requires numpy!=1.19.3,<1.24; sys_platform == \"linux\", but you have numpy 1.26.1 which is incompatible.\n",
      "azureml-k8s-mt 1.0.6b1 requires requests<2.30,>=2.22, but you have requests 2.31.0 which is incompatible.\n",
      "datasets 2.14.5 requires fsspec[http]<2023.9.0,>=2023.1.0, but you have fsspec 2023.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.3 certifi-2023.11.17 charset-normalizer-3.3.2 filelock-3.13.1 fsspec-2023.12.0 idna-3.6 jinja2-3.1.2 mpmath-1.3.0 networkx-3.2.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 requests-2.31.0 sympy-1.12 torch-2.1.1 torchdata-0.7.1 triton-2.1.0 typing-extensions-4.8.0 urllib3-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchdata --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y160sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m \u001b[39mimport\u001b[39;00m FileLister, FileOpener\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y160sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m datapipe1 \u001b[39m=\u001b[39m FileLister(\u001b[39m'\u001b[39m\u001b[39mdata/swissprot/proteinfer_splits/random/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mdev*.tfrecord\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y160sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m datapipe2 \u001b[39m=\u001b[39m FileOpener(datapipe1, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torchdata/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m datapipes\n\u001b[1;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__  \u001b[39m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torchdata/datapipes/__init__.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright (c) Meta Platforms, Inc. and affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# All rights reserved.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m# This source code is licensed under the BSD-style license found in the\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataChunk, functional_datapipe\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39miter\u001b[39m, \u001b[39mmap\u001b[39m, utils\n\u001b[1;32m     11\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mDataChunk\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfunctional_datapipe\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39miter\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torchdata/datapipes/iter/__init__.py:64\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39ms3io\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m     S3FileListerIterDataPipe \u001b[39mas\u001b[39;00m S3FileLister,\n\u001b[1;32m     57\u001b[0m     S3FileLoaderIterDataPipe \u001b[39mas\u001b[39;00m S3FileLoader,\n\u001b[1;32m     58\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbucketbatcher\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     60\u001b[0m     BucketBatcherIterDataPipe \u001b[39mas\u001b[39;00m BucketBatcher,\n\u001b[1;32m     61\u001b[0m     InBatchShufflerIterDataPipe \u001b[39mas\u001b[39;00m InBatchShuffler,\n\u001b[1;32m     62\u001b[0m     MaxTokenBucketizerIterDataPipe \u001b[39mas\u001b[39;00m MaxTokenBucketizer,\n\u001b[1;32m     63\u001b[0m )\n\u001b[0;32m---> 64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransform\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallable\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     65\u001b[0m     BatchMapperIterDataPipe \u001b[39mas\u001b[39;00m BatchMapper,\n\u001b[1;32m     66\u001b[0m     FlatMapperIterDataPipe \u001b[39mas\u001b[39;00m FlatMapper,\n\u001b[1;32m     67\u001b[0m )\n\u001b[1;32m     68\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbz2fileloader\u001b[39;00m \u001b[39mimport\u001b[39;00m Bz2FileLoaderIterDataPipe \u001b[39mas\u001b[39;00m Bz2FileLoader\n\u001b[1;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39miter\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcacheholder\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m     70\u001b[0m     EndOnDiskCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m EndOnDiskCacheHolder,\n\u001b[1;32m     71\u001b[0m     InMemoryCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m InMemoryCacheHolder,\n\u001b[1;32m     72\u001b[0m     OnDiskCacheHolderIterDataPipe \u001b[39mas\u001b[39;00m OnDiskCacheHolder,\n\u001b[1;32m     73\u001b[0m )\n",
      "File \u001b[0;32m/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torchdata/datapipes/iter/transform/callable.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Callable, Iterator, List, TypeVar\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m functional_datapipe, IterDataPipe\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatapipes\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m _check_lambda_fn\n\u001b[1;32m     12\u001b[0m T_co \u001b[39m=\u001b[39m TypeVar(\u001b[39m\"\u001b[39m\u001b[39mT_co\u001b[39m\u001b[39m\"\u001b[39m, covariant\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     15\u001b[0m \u001b[39m@functional_datapipe\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mmap_batches\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mBatchMapperIterDataPipe\u001b[39;00m(IterDataPipe[T_co]):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_check_lambda_fn' from 'torch.utils.data.datapipes.utils.common' (/anaconda/envs/protein_functions_310/lib/python3.10/site-packages/torch/utils/data/datapipes/utils/common.py)"
     ]
    }
   ],
   "source": [
    "from torchdata.datapipes.iter import FileLister, FileOpener\n",
    "\n",
    "datapipe1 = FileLister('data/swissprot/proteinfer_splits/random/', 'dev*.tfrecord')\n",
    "datapipe2 = FileOpener(datapipe1, mode=\"b\")\n",
    "tfrecord_loader_dp = datapipe2.load_from_tfrecord()\n",
    "\n",
    "records = []\n",
    "# Iterate over records, process and write to a fasta file\n",
    "for _, record in tqdm(enumerate(tfrecord_loader_dp)):\n",
    "    processed_sequence = process_sequence_tfrecord(\n",
    "        record, \"GO\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from src.utils.data import read_fasta, read_json, get_vocab_mappings, read_pickle\n",
    "from src.utils.models import tokenize_labels, get_label_embeddings\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import List\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from src.utils.main_utils import get_or_generate_label_embeddings\n",
    "\n",
    "\n",
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for protein sequences with GO annotations.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_paths: dict,\n",
    "        config: dict,\n",
    "        vocabularies: dict,\n",
    "        label_tokenizer=None,\n",
    "        label_encoder=None,\n",
    "        logger=None,\n",
    "        subset_fraction: float = 1.0,\n",
    "        deduplicate: bool = False,\n",
    "        is_master: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        paths (dict): Dictionary containing paths to the data and vocabularies.\n",
    "            data_path (str): Path to the FASTA file containing the protein sequences and corresponding GO annotations\n",
    "            dataset_type (str): One of 'train', 'validation', or 'test'\n",
    "            go_descriptions_path (str): Path to the pickled file containing the GO term descriptions mapped to GO term IDs\n",
    "        deduplicate (bool): Whether to remove duplicate sequences (default: False)\n",
    "        \"\"\"\n",
    "        # Error handling: check for missing keys and invalid dataset types\n",
    "        required_keys = [\"data_path\", \"dataset_type\"]\n",
    "        for key in required_keys:\n",
    "            if key not in data_paths:\n",
    "                raise ValueError(\n",
    "                    f\"Missing required key in paths dictionary: {key}\")\n",
    "\n",
    "        assert data_paths[\"dataset_type\"] in [\n",
    "            \"train\",\n",
    "            \"validation\",\n",
    "            \"test\",\n",
    "        ], \"dataset_type must be one of 'train', 'val', or 'test'\"\n",
    "\n",
    "        # Set the dataset type and data path\n",
    "        self.dataset_type = data_paths[\"dataset_type\"]\n",
    "        self.data_path = data_paths[\"data_path\"]\n",
    "\n",
    "        # Set and process the vocabularies\n",
    "        self.amino_acid_vocabulary = vocabularies[\"amino_acid_vocab\"]\n",
    "        self.label_vocabulary = vocabularies[\"GO_label_vocab\"]\n",
    "        self.sequence_id_vocabulary = vocabularies[\"sequence_id_vocab\"]\n",
    "        self._process_vocab()\n",
    "\n",
    "        # Initialize class variables\n",
    "        self.data = read_fasta(data_paths[\"data_path\"])\n",
    "        self.label_embedding_matrix = self.sequence_embedding_df = None\n",
    "\n",
    "        # Subset the data if subset_fraction is provided\n",
    "        if subset_fraction < 1.0:\n",
    "            logging.info(\n",
    "                f\"Subsetting {subset_fraction*100}% of the {self.dataset_type} set...\"\n",
    "            )\n",
    "            self.data = self.data[:int(subset_fraction * len(self.data))]\n",
    "\n",
    "        # Deduplicate the data if deduplicate is True\n",
    "        if deduplicate:\n",
    "            self._remove_duplicates()\n",
    "\n",
    "        # Load the map from alphanumeric label id to text label\n",
    "        self.label_annotation_map = {key: value['label'] for key, value in read_pickle(\n",
    "            data_paths[\"go_annotations_path\"]).to_dict(orient='index').items()}\n",
    "\n",
    "        # Create ordered list of labels\n",
    "        label_text_list = []\n",
    "        for label_id in self.label_vocabulary:\n",
    "            label_text_list.append(self.label_annotation_map[label_id])\n",
    "        self.label_text_list = label_text_list\n",
    "\n",
    "        # Loop through the label IDs and tokenize the labels if a label tokenizer is provided\n",
    "        self.tokenized_labels = None\n",
    "        self.label_tokenizer = None\n",
    "        if label_tokenizer is not None:\n",
    "            self.label_tokenizer = label_tokenizer\n",
    "            self.tokenized_labels = tokenize_labels(\n",
    "                label_text_list, label_tokenizer)\n",
    "\n",
    "        # If a label encoder is provided, encode the labels\n",
    "        # TODO: Move back to main to remove warning\n",
    "        self.label_embedding_matrix = None\n",
    "        self.label_encoder = None\n",
    "        if label_encoder is not None and not config[\"params\"][\"TRAIN_LABEL_ENCODER\"]:\n",
    "            self.label_encoder = label_encoder\n",
    "            label_embedding_matrix = get_or_generate_label_embeddings(\n",
    "                label_annotations=self.label_text_list,\n",
    "                label_tokenizer=label_tokenizer,\n",
    "                label_encoder=label_encoder,\n",
    "                label_embedding_path=config[\"paths\"][\"LABEL_EMBEDDING_PATH\"],\n",
    "                logger=logger,\n",
    "                batch_size_limit=config[\"params\"][\"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\"],\n",
    "                is_master=is_master,\n",
    "            )\n",
    "            self.label_embedding_matrix = label_embedding_matrix\n",
    "\n",
    "    # Helper functions for setting embedding dictionaries\n",
    "    def set_sequence_embedding_df(self, embedding_df: pd.DataFrame):\n",
    "        self.sequence_embedding_df = embedding_df\n",
    "\n",
    "    def set_label_embedding_matrix(self, embedding_matrix: torch.Tensor):\n",
    "        self.label_embedding_matrix = embedding_matrix\n",
    "\n",
    "    def _remove_duplicates(self):\n",
    "        \"\"\"\n",
    "        Remove duplicate sequences from self.data, keeping only the first instance of each sequence\n",
    "        Use pandas to improve performance\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert self.data to a DataFrame\n",
    "        df = pd.DataFrame(self.data, columns=[\"sequence\", \"labels\"])\n",
    "\n",
    "        # Drop duplicate rows based on the 'sequence' column, keeping the first instance\n",
    "        df = df.drop_duplicates(subset=\"sequence\", keep=\"first\")\n",
    "\n",
    "        # Log the number of duplicate sequences removed\n",
    "        num_duplicates = len(self.data) - len(df)\n",
    "        logging.info(\n",
    "            f\"Removing {num_duplicates} duplicate sequences from {self.data_path}...\")\n",
    "\n",
    "        # Convert the DataFrame back to the list of tuples format\n",
    "        self.data = list(df.itertuples(index=False, name=None))\n",
    "\n",
    "    # Helper functions for processing and loading vocabularies\n",
    "    def _process_vocab(self):\n",
    "        self._process_amino_acid_vocab()\n",
    "        self._process_label_vocab()\n",
    "        self._process_sequence_id_vocab()\n",
    "\n",
    "    def _process_amino_acid_vocab(self):\n",
    "        self.aminoacid2int, self.int2aminoacid = get_vocab_mappings(\n",
    "            self.amino_acid_vocabulary\n",
    "        )\n",
    "\n",
    "    def _process_label_vocab(self):\n",
    "        self.label2int, self.int2label = get_vocab_mappings(\n",
    "            self.label_vocabulary)\n",
    "\n",
    "    def _process_sequence_id_vocab(self):\n",
    "        self.sequence_id2int, self.int2sequence_id = get_vocab_mappings(\n",
    "            self.sequence_id_vocabulary\n",
    "        )\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "\n",
    "    def process_example(self, sequence: str, labels: list[str]) -> dict:\n",
    "        sequence_id_alphanumeric, labels = labels[0], labels[1:]\n",
    "\n",
    "        # Convert the sequence and labels to integers for one-hot encoding\n",
    "        amino_acid_ints = torch.tensor(\n",
    "            [self.aminoacid2int[aa] for aa in sequence], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        labels_ints = torch.tensor(\n",
    "            [self.label2int[label] for label in labels], dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # Get the length of the sequence\n",
    "        sequence_length = torch.tensor(len(amino_acid_ints))\n",
    "\n",
    "        # Get multi-hot encoding of sequence and labels\n",
    "        sequence_onehots = torch.nn.functional.one_hot(\n",
    "            amino_acid_ints, num_classes=len(self.amino_acid_vocabulary)\n",
    "        ).permute(1, 0)\n",
    "        label_multihots = torch.nn.functional.one_hot(\n",
    "            labels_ints, num_classes=len(self.label_vocabulary)\n",
    "        ).sum(dim=0)\n",
    "\n",
    "        # Set the label embeddings, if provided\n",
    "        label_embeddings = self.label_embedding_matrix if self.label_embedding_matrix is not None else None\n",
    "\n",
    "        # Get the sequence embedding, if provided\n",
    "        sequence_embedding = None\n",
    "        # TODO: Remove this check\n",
    "        if self.sequence_embedding_df is not None:\n",
    "            sequence_embedding = torch.tensor(\n",
    "                self.sequence_embedding_df.loc[sequence_id_alphanumeric].values)\n",
    "\n",
    "        # Get the tokenized labels, if provided\n",
    "        tokenized_labels = self.tokenized_labels if self.tokenized_labels is not None else None\n",
    "\n",
    "        # Return a dict containing the processed example\n",
    "        return {\n",
    "            \"sequence_onehots\": sequence_onehots,\n",
    "            \"sequence_id\": sequence_id_alphanumeric,\n",
    "            \"sequence_embedding\": sequence_embedding,\n",
    "            \"sequence_length\": sequence_length,\n",
    "            \"label_multihots\": label_multihots,\n",
    "            \"tokenized_labels\": tokenized_labels,\n",
    "            \"label_embeddings\": label_embeddings,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx) -> tuple:\n",
    "        sequence, labels = self.data[idx]\n",
    "        return self.process_example(sequence, labels)\n",
    "\n",
    "    @classmethod\n",
    "    def create_multiple_datasets(\n",
    "        cls,\n",
    "        paths_list: List[Dict[str, str]],\n",
    "        config: dict,\n",
    "        vocabularies: dict,\n",
    "        subset_fractions: dict = None,\n",
    "        label_tokenizer=None,\n",
    "        label_encoder=None,\n",
    "        logger=None,\n",
    "        deduplicate: bool = False,\n",
    "    ) -> List[Dataset]:\n",
    "        \"\"\"\n",
    "        paths_list (List[Dict[str, str]]): List of dictionaries, each containing paths to the data and vocabularies.\n",
    "        subset_fractions (dict): Dictionary containing the subset fraction for each dataset type (default: None)\n",
    "        \"\"\"\n",
    "        datasets = defaultdict(list)\n",
    "        subset_fractions = subset_fractions or {}\n",
    "        for data_paths in paths_list:\n",
    "            datasets[data_paths[\"dataset_type\"]].append(\n",
    "                cls(\n",
    "                    data_paths,\n",
    "                    config,\n",
    "                    vocabularies,\n",
    "                    label_tokenizer=label_tokenizer,\n",
    "                    label_encoder=label_encoder,\n",
    "                    logger=logger,\n",
    "                    subset_fraction=subset_fractions.get(\n",
    "                        data_paths[\"dataset_type\"], 1.0),\n",
    "                    deduplicate=deduplicate\n",
    "                )\n",
    "            )\n",
    "        return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 08:41:37 PST INFO Logging to ./outputs/logs/2023-12-03_08-41-37_Test.log and console...\n",
      "2023-12-03 08:41:37 PST INFO Logging to ./outputs/logs/2023-12-03_08-41-37_Test.log and console...\n",
      "2023-12-03 08:41:37 PST INFO Logging to ./outputs/logs/2023-12-03_08-41-37_Test.log and console...\n",
      "2023-12-03 08:41:37 PST INFO Using device: cuda:0\n",
      "2023-12-03 08:41:37 PST INFO Using device: cuda:0\n",
      "2023-12-03 08:41:37 PST INFO Using device: cuda:0\n",
      "2023-12-03 08:41:37 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 32,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.0003,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 2,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"PROJECTION_HEAD_NUM_LAYERS\": 2,\n",
      "    \"FEATURE_FUSION\": \"concatenation\",\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 8,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"FocalLoss\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"SUPCON_TEMP\": 0.07,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"TRAIN_LABEL_ENCODER\": false,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 1,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": true\n",
      "}\n",
      "2023-12-03 08:41:37 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 32,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.0003,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 2,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"PROJECTION_HEAD_NUM_LAYERS\": 2,\n",
      "    \"FEATURE_FUSION\": \"concatenation\",\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 8,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"FocalLoss\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"SUPCON_TEMP\": 0.07,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"TRAIN_LABEL_ENCODER\": false,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 1,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": true\n",
      "}\n",
      "2023-12-03 08:41:37 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 32,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.0003,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 2,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"PROJECTION_HEAD_NUM_LAYERS\": 2,\n",
      "    \"FEATURE_FUSION\": \"concatenation\",\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 8,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"FocalLoss\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"SUPCON_TEMP\": 0.07,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"TRAIN_LABEL_ENCODER\": false,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 1,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": true\n",
      "}\n",
      "2023-12-03 08:41:39 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n",
      "2023-12-03 08:41:39 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### SETUP ###\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if master process\n",
    "is_master = True\n",
    "config = \"configs/base_config.yaml\"\n",
    "name = \"Test\"\n",
    "train_path_name = \"TRAIN_DATA_PATH\"\n",
    "validation_path_name = \"VAL_DATA_PATH\"\n",
    "test_paths_names = [\"TEST_DATA_PATH\"]\n",
    "amlt = False\n",
    "gpu=0\n",
    "rank=0\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path=config,\n",
    "    run_name=name,\n",
    "    overrides=[],\n",
    "    train_path_name=train_path_name,\n",
    "    val_path_name=validation_path_name,\n",
    "    test_paths_names=test_paths_names,\n",
    "    amlt=amlt,\n",
    "    is_master=is_master,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n",
    "\n",
    "# Set the GPU device, if using\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Log the params\n",
    "logger.info(json.dumps(params, indent=4))\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "if params[\"GRADIENT_CHECKPOINTING\"]:\n",
    "    raise NotImplementedError(\n",
    "        \"Gradient checkpointing is not yet implemented.\")\n",
    "\n",
    "if params[\"LORA\"]:\n",
    "    for layer in label_encoder.layers:\n",
    "        in_features, out_features = 1024, 1024\n",
    "        layer.self_attn.q_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.v_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.k_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.out_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "    # Mark only the LoRA parameters as trainable\n",
    "    lora.mark_only_lora_as_trainable(label_encoder)\n",
    "\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'vocabularies_dir': './data/vocabularies/proteinfer',\n",
       "  'go_annotations_path': './data/annotations/go_annotations_2019_07_01.pkl',\n",
       "  'data_path': './data/swissprot/proteinfer_splits/random/train_GO.fasta',\n",
       "  'dataset_type': 'train'},\n",
       " {'vocabularies_dir': './data/vocabularies/proteinfer',\n",
       "  'go_annotations_path': './data/annotations/go_annotations_2019_07_01.pkl',\n",
       "  'data_path': './data/swissprot/proteinfer_splits/random/dev_GO.fasta',\n",
       "  'dataset_type': 'validation'},\n",
       " {'vocabularies_dir': './data/vocabularies/proteinfer',\n",
       "  'go_annotations_path': './data/annotations/go_annotations_2019_07_01.pkl',\n",
       "  'data_path': './data/swissprot/proteinfer_splits/random/test_GO.fasta',\n",
       "  'dataset_type': 'test'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['dataset_paths_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sequence_onehots\": sequence_onehots,\n",
    "\"sequence_id\": sequence_id_alphanumeric,\n",
    "\"sequence_embedding\": sequence_embedding,\n",
    "\"sequence_length\": sequence_length,\n",
    "\"label_multihots\": label_multihots,\n",
    "\"tokenized_labels\": tokenized_labels,\n",
    "\"label_embeddings\": label_embeddings,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def flatten_single_seq(idx):\n",
    "    seq = d.data[idx]\n",
    "    pos_labels = seq[1][1:]\n",
    "    neg_labels = set(d.label_vocabulary) - set(pos_labels)\n",
    "    labels = [(i,1) for i in pos_labels] + [(i,0) for i in neg_labels]\n",
    "    flattened = defaultdict(list)\n",
    "\n",
    "    for label,y in labels:\n",
    "        flattened['sequence'].append(seq[0])\n",
    "        flattened['sequence_id'].append(seq[1][0])\n",
    "        flattened['label'].append(label)\n",
    "        flattened['y'].append(y)\n",
    "\n",
    "    return flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m flattened \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(d\u001b[39m.\u001b[39mdata)):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     single_flattend \u001b[39m=\u001b[39m flatten_single_seq(i)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m single_flattend\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         flattened[k]\u001b[39m.\u001b[39mextend(v)\n",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pos_labels \u001b[39m=\u001b[39m seq[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m neg_labels \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(d\u001b[39m.\u001b[39mlabel_vocabulary) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(pos_labels)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m labels \u001b[39m=\u001b[39m [(i,\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pos_labels] \u001b[39m+\u001b[39m [(i,\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m neg_labels]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m flattened \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m label,y \u001b[39min\u001b[39;00m labels:\n",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m pos_labels \u001b[39m=\u001b[39m seq[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m:]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m neg_labels \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(d\u001b[39m.\u001b[39mlabel_vocabulary) \u001b[39m-\u001b[39m \u001b[39mset\u001b[39m(pos_labels)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m labels \u001b[39m=\u001b[39m [(i,\u001b[39m1\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pos_labels] \u001b[39m+\u001b[39m [(i,\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m neg_labels]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m flattened \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y155sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m label,y \u001b[39min\u001b[39;00m labels:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flattened = defaultdict(list)\n",
    "for i in range(len(d.data)):\n",
    "    single_flattend = flatten_single_seq(i)\n",
    "    for k,v in single_flattend.items():\n",
    "        flattened[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-25 03:45:01 PST INFO Logging to ./outputs/logs/2023-11-25_03-45-01_Test.log and console...\n",
      "2023-11-25 03:45:01 PST INFO Using device: cuda:0\n",
      "2023-11-25 03:45:01 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 64,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.001,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 1,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 8,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"WeightedBCE\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"TRAIN_LABEL_ENCODER\": false,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 100,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": true\n",
      "}\n",
      "2023-11-25 03:45:05 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-11-25 03:45:05 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-11-25 03:45:05 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n",
      "2023-11-25 03:45:11 PST INFO Removing 66586 duplicate sequences from ./data/swissprot/proteinfer_splits/random/train_GO.fasta...\n",
      "2023-11-25 03:45:24 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:25 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-11-25 03:45:36 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:37 PST INFO Removing 8176 duplicate sequences from ./data/swissprot/proteinfer_splits/random/test_GO.fasta...\n",
      "2023-11-25 03:45:48 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:48 PST INFO ################## 2023-11-25_03-45-01 RUNNING main.py ##################\n",
      "2023-11-25 03:45:49 PST INFO Loaded sequence embeddings from ./data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f639c7977c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### SETUP ###\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if master process\n",
    "is_master = True\n",
    "config = \"configs/base_config.yaml\"\n",
    "name = \"Test\"\n",
    "train_path_name = \"TRAIN_DATA_PATH\"\n",
    "validation_path_name = \"VAL_DATA_PATH\"\n",
    "test_paths_names = [\"TEST_DATA_PATH\"]\n",
    "amlt = False\n",
    "gpu=0\n",
    "rank=0\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path=config,\n",
    "    run_name=name,\n",
    "    overrides=[],\n",
    "    train_path_name=train_path_name,\n",
    "    val_path_name=validation_path_name,\n",
    "    test_paths_names=test_paths_names,\n",
    "    amlt=amlt,\n",
    "    is_master=is_master,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n",
    "\n",
    "# Set the GPU device, if using\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Log the params\n",
    "logger.info(json.dumps(params, indent=4))\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "if params[\"GRADIENT_CHECKPOINTING\"]:\n",
    "    raise NotImplementedError(\n",
    "        \"Gradient checkpointing is not yet implemented.\")\n",
    "\n",
    "if params[\"LORA\"]:\n",
    "    for layer in label_encoder.layers:\n",
    "        in_features, out_features = 1024, 1024\n",
    "        layer.self_attn.q_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.v_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.k_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.out_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "    # Mark only the LoRA parameters as trainable\n",
    "    lora.mark_only_lora_as_trainable(label_encoder)\n",
    "\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    paths_list=config['dataset_paths_list'],\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    vocabularies=vocabularies,\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"],\n",
    "    },\n",
    "    deduplicate=params[\"DEDUPLICATE\"],\n",
    ")\n",
    "\n",
    "# Seed everything so we don't go crazy\n",
    "seed_everything(params[\"SEED\"], device)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING main.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    \"train\": params[\"TRAIN_LABEL_SAMPLE_SIZE\"],\n",
    "    \"validation\": params[\"VALIDATION_LABEL_SAMPLE_SIZE\"],\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    shuffle_labels=params['SHUFFLE_LABELS'],\n",
    "    in_batch_sampling=params['IN_BATCH_SAMPLING'],\n",
    "    num_workers=params[\"NUM_WORKERS\"],\n",
    "    world_size=1,\n",
    "    rank=rank,\n",
    ")\n",
    "\n",
    "if not params[\"TRAIN_LABEL_ENCODER\"]:\n",
    "    # Move the label encoder to CPU\n",
    "    label_encoder = label_encoder.cpu()\n",
    "\n",
    "# Initialize ProteInfer\n",
    "sequence_encoder = ProteInfer.from_pretrained(\n",
    "    weights_path=paths[\"PROTEINFER_WEIGHTS_PATH\"],\n",
    "    num_labels=config[\"embed_sequences_params\"][\"PROTEINFER_NUM_LABELS\"],\n",
    "    input_channels=config[\"embed_sequences_params\"][\"INPUT_CHANNELS\"],\n",
    "    output_channels=config[\"embed_sequences_params\"][\"OUTPUT_CHANNELS\"],\n",
    "    kernel_size=config[\"embed_sequences_params\"][\"KERNEL_SIZE\"],\n",
    "    activation=torch.nn.ReLU,\n",
    "    dilation_base=config[\"embed_sequences_params\"][\"DILATION_BASE\"],\n",
    "    num_resnet_blocks=config[\"embed_sequences_params\"][\"NUM_RESNET_BLOCKS\"],\n",
    "    bottleneck_factor=config[\"embed_sequences_params\"][\"BOTTLENECK_FACTOR\"],\n",
    ")\n",
    "\n",
    "# Generate all sequence embeddings upfront, if not training the sequence encoder\n",
    "sequence_embedding_df = None\n",
    "if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "    sequence_embedding_df = get_or_generate_sequence_embeddings(\n",
    "        paths,\n",
    "        device,\n",
    "        sequence_encoder,\n",
    "        datasets,\n",
    "        params,\n",
    "        logger,\n",
    "    )\n",
    "    sequence_encoder = sequence_encoder.to('cpu')\n",
    "\n",
    "# Loop through all the datasets and set the sequence embedding df\n",
    "for dataset in datasets.values():\n",
    "    for subset in dataset:\n",
    "        if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "            subset.set_sequence_embedding_df(sequence_embedding_df)\n",
    "\n",
    "\n",
    "loaders[\"train\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 08:44:55 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-12-03 08:44:55 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-12-03 08:44:55 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-12-03 08:45:08 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-12-03 08:45:08 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-12-03 08:45:08 PST INFO Loaded label embeddings from ./data/None\n"
     ]
    }
   ],
   "source": [
    "d = ProteinDataset(config=config,\n",
    "        logger=logger,\n",
    "        label_tokenizer=label_tokenizer,\n",
    "        label_encoder=label_encoder,\n",
    "        vocabularies=vocabularies,\n",
    "        data_paths=config['dataset_paths_list'][1],\n",
    "        deduplicate=params[\"DEDUPLICATE\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"sequence_onehots\": sequence_onehots,\n",
    "\"sequence_id\": sequence_id_alphanumeric,\n",
    "\"sequence_embedding\": sequence_embedding,\n",
    "\"sequence_length\": sequence_length,\n",
    "\"label_multihots\": label_multihots,\n",
    "\"tokenized_labels\": tokenized_labels,\n",
    "\"label_embeddings\": label_embeddings,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_onehots_list = []\n",
    "sequence_id_alphanumeric_list = []\n",
    "sequence_embedding_list = []\n",
    "sequence_length_list = []\n",
    "label_multihots_list = []\n",
    "\n",
    "flattened = defaultdict(list)\n",
    "for i in range(10):\n",
    "\n",
    "    sequence,labels = d.data[i]\n",
    "\n",
    "    sequence_id_alphanumeric, labels = labels[0], labels[1:]\n",
    "\n",
    "    # Convert the sequence and labels to integers for one-hot encoding\n",
    "    amino_acid_ints = torch.tensor(\n",
    "        [d.aminoacid2int[aa] for aa in sequence], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    labels_ints = torch.tensor(\n",
    "        [d.label2int[label] for label in labels], dtype=torch.long\n",
    "    )\n",
    "\n",
    "    # Get the length of the sequence\n",
    "    sequence_length = torch.tensor(len(amino_acid_ints))\n",
    "\n",
    "    # Get multi-hot encoding of sequence and labels\n",
    "    sequence_onehots = torch.nn.functional.one_hot(\n",
    "        amino_acid_ints, num_classes=len(d.amino_acid_vocabulary)\n",
    "    ).permute(1, 0)\n",
    "    label_multihots = torch.nn.functional.one_hot(\n",
    "        labels_ints, num_classes=len(d.label_vocabulary)\n",
    "    ).sum(dim=0)\n",
    "\n",
    "    # Get the sequence embedding, if provided\n",
    "    sequence_embedding = None\n",
    "    # TODO: Remove this check\n",
    "    if d.sequence_embedding_df is not None:\n",
    "        sequence_embedding = torch.tensor(\n",
    "            d.sequence_embedding_df.loc[sequence_id_alphanumeric].values)\n",
    "        \n",
    "\n",
    "    sequence_onehots_list.extend([sequence_onehots.clone() for _ in range(len(d.label_vocabulary))])\n",
    "    sequence_id_alphanumeric_list.append([sequence_id_alphanumeric for _ in range(len(d.label_vocabulary))])\n",
    "    sequence_embedding_list.append([sequence_embedding.clone() if sequence_embedding is not None else None for _ in range(len(d.label_vocabulary))])\n",
    "    sequence_length_list.append(sequence_length)\n",
    "    label_multihots_list.append(label_multihots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " tensor([[0, 0, 0,  ..., 0, 0, 1],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 1, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]])]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 155])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_onehots.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the label embeddings, if provided\n",
    "label_embeddings = d.label_embedding_matrix if d.label_embedding_matrix is not None else None\n",
    "\n",
    "# Get the tokenized labels, if provided\n",
    "tokenized_labels = d.tokenized_labels if d.tokenized_labels is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25390)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1939, -0.8254,  0.0120,  ..., -0.0249, -0.1879,  0.3359])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embeddings[labels_ints[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0434, -1.4430, -0.3326,  ..., -0.6947,  0.2159, -0.7041],\n",
       "        [-1.3708, -0.5248,  0.2347,  ..., -0.1317,  0.0789,  0.1895],\n",
       "        [-1.0705, -1.2185, -0.5712,  ..., -0.3016, -0.3092, -0.1650],\n",
       "        ...,\n",
       "        [-1.1631, -0.5377,  0.3100,  ..., -0.2499,  0.4125,  0.2718],\n",
       "        [-0.7390, -0.1916,  1.3233,  ..., -0.3388,  0.4596,  0.3480],\n",
       "        [-1.1939, -0.8254,  0.0120,  ..., -0.0249, -0.1879,  0.3359]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_embeddings[labels_ints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_weights_v2(label_weights, target):\n",
    "    \"\"\"\n",
    "    Computes the weights for each sample in the batch based on the target labels\n",
    "    using broadcasting.\n",
    "    \n",
    "    Args:\n",
    "        label_weights: torch.tensor of size [no_of_classes] with the weight of each label.\n",
    "        target: torch.tensor of size [batch, no_of_classes].\n",
    "\n",
    "    Returns:\n",
    "        weights_for_samples: torch.tensor of size [batch, no_of_classes].\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure label_weights is a float tensor for correct broadcasting and computation\n",
    "    label_weights = label_weights.float()\n",
    "\n",
    "    # Multiply weights with target labels using broadcasting\n",
    "    # This step applies the specific class weights to the corresponding labels in the target.\n",
    "    weighted_targets = label_weights * target\n",
    "\n",
    "    # Sum the weighted targets along the class dimension to get a single weight per sample\n",
    "    weights_for_samples = weighted_targets.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Use broadcasting again for expanding weights across the class dimension\n",
    "    # No need to repeat the tensor explicitly.\n",
    "    weights_for_samples = weights_for_samples.expand_as(target)\n",
    "\n",
    "    return weights_for_samples\n",
    "\n",
    "\n",
    "class CBLoss(torch.nn.Module):\n",
    "    def __init__(self, label_weights, beta=0.99):\n",
    "        super().__init__()\n",
    "\n",
    "        self.label_weights = label_weights\n",
    "        self.beta=beta\n",
    "\n",
    "    def forward(self, input,target):\n",
    "        no_of_classes = len(self.label_weights)\n",
    "        effective_num = 1.0 - torch.pow(self.beta, self.label_weights)\n",
    "\n",
    "        # Replace zeros in effective_num with 'inf' (infinity) to avoid division by zero\n",
    "        effective_num = torch.where(effective_num == 0, torch.tensor(float('inf')), effective_num)\n",
    "\n",
    "        weights = (1.0 - self.beta) / effective_num\n",
    "        weights = weights / torch.sum(weights) * no_of_classes\n",
    "\n",
    "        weights = get_batch_weights_v2(weights,target)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def CB_loss(labels_one_hot, samples_per_cls, no_of_classes,  beta=0.99):\n",
    "    \"\"\"Compute the Class Balanced Loss between `logits` and the ground truth `labels`.\n",
    "\n",
    "    Class Balanced Loss: ((1-beta)/(1-beta^n))*Loss(labels, logits)\n",
    "    where Loss is one of the standard losses used for Neural Networks.\n",
    "\n",
    "    Args:\n",
    "      labels: A int tensor of size [batch].\n",
    "      logits: A float tensor of size [batch, no_of_classes].\n",
    "      samples_per_cls: A python list of size [no_of_classes].\n",
    "      no_of_classes: total number of classes. int\n",
    "      loss_type: string. One of \"sigmoid\", \"focal\", \"softmax\".\n",
    "      beta: float. Hyperparameter for Class balanced loss.\n",
    "      gamma: float. Hyperparameter for Focal loss.\n",
    "\n",
    "    Returns:\n",
    "      cb_loss: A float tensor representing class balanced loss\n",
    "    \"\"\"\n",
    "    effective_num = 1.0 - np.power(beta, samples_per_cls)\n",
    "    weights = (1.0 - beta) / np.array(effective_num)\n",
    "    weights = weights / np.sum(weights) * no_of_classes\n",
    "\n",
    "\n",
    "    weights = torch.tensor(weights).float()\n",
    "    weights = weights.unsqueeze(0)\n",
    "    weights = weights.repeat(labels_one_hot.shape[0],1) * labels_one_hot\n",
    "    weights = weights.sum(1)\n",
    "    weights = weights.unsqueeze(1)\n",
    "    weights = weights.repeat(1,no_of_classes)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = (torch.rand(size=(10,100))>0.4)*1.0\n",
    "preds = torch.rand(size=labels.shape)*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_cls = labels.sum(axis=0\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7., 6., 6., 6., 4., 7., 6., 6., 6., 7., 6., 6., 6., 2., 7., 6., 4., 5.,\n",
       "        7., 5., 7., 7., 6., 4., 7., 5., 5., 7., 5., 8., 6., 6., 6., 5., 5., 6.,\n",
       "        5., 7., 3., 5., 4., 7., 5., 4., 5., 8., 7., 1., 7., 6., 6., 6., 5., 5.,\n",
       "        6., 5., 6., 9., 5., 6., 9., 6., 4., 6., 7., 6., 6., 7., 4., 5., 5., 5.,\n",
       "        8., 6., 7., 6., 6., 7., 4., 8., 5., 4., 7., 2., 6., 5., 6., 6., 6., 4.,\n",
       "        5., 7., 7., 9., 6., 7., 8., 8., 3., 6.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_per_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_original = CB_loss(labels, samples_per_cls, len(samples_per_cls),  beta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(53.7565), tensor(53756.4570))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_original.mean(),w_original.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "cb=CBLoss(samples_per_cls,beta=0.9)\n",
    "w_mine=cb(None,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(53.7565), tensor(53756.4609))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_mine.mean(),w_mine.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_mine = CB_loss(labels, samples_per_cls, len(samples_per_cls),  beta=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bsz=3\n",
    "features = torch.randint(0,10,(bsz,2,1))\n",
    "labels = torch.Tensor([1,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature=0.07\n",
    "contrast_mode='all'\n",
    "base_temperature=0.07\n",
    "\n",
    "device = (torch.device('cuda')\n",
    "            if features.is_cuda\n",
    "            else torch.device('cpu'))\n",
    "\n",
    "features = features.view(features.shape[0], features.shape[1], -1)\n",
    "\n",
    "batch_size = features.shape[0]\n",
    "labels = labels.contiguous().view(-1, 1)\n",
    "if labels.shape[0] != batch_size:\n",
    "    raise ValueError('Num of labels does not match num of features')\n",
    "mask = torch.eq(labels, labels.T).float().to(device)\n",
    "\n",
    "\n",
    "contrast_count = features.shape[1]\n",
    "contrast_feature = torch.cat(torch.unbind(features, dim=1), dim=0)\n",
    "if contrast_mode == 'one':\n",
    "    anchor_feature = features[:, 0]\n",
    "    anchor_count = 1\n",
    "elif contrast_mode == 'all':\n",
    "    anchor_feature = contrast_feature\n",
    "    anchor_count = contrast_count\n",
    "else:\n",
    "    raise ValueError('Unknown mode: {}'.format(contrast_mode))\n",
    "\n",
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(\n",
    "    torch.matmul(anchor_feature, contrast_feature.T),\n",
    "    temperature)\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# tile mask\n",
    "mask = mask.repeat(anchor_count, contrast_count)\n",
    "# mask-out self-contrast cases\n",
    "logits_mask = torch.scatter(\n",
    "    torch.ones_like(mask),\n",
    "    1,\n",
    "    torch.arange(batch_size * anchor_count).view(-1, 1).to(device),\n",
    "    0\n",
    ")\n",
    "mask = mask * logits_mask\n",
    "\n",
    "# compute log_prob\n",
    "exp_logits = torch.exp(logits) * logits_mask\n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "# compute mean of log-likelihood over positive\n",
    "mean_log_prob_pos = (mask * log_prob).sum(1) / mask.sum(1)\n",
    "\n",
    "# loss\n",
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "\n",
    "loss = loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1.],\n",
       "        [1., 0., 1., 1., 1., 1.],\n",
       "        [1., 1., 0., 1., 1., 1.],\n",
       "        [1., 1., 1., 0., 1., 1.],\n",
       "        [1., 1., 1., 1., 0., 1.],\n",
       "        [1., 1., 1., 1., 1., 0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -573.2203,    -1.7918,  -716.0775,  -716.0775,  -916.0775, -1144.6489])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask * log_prob).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(126.7857), tensor(126.7857))"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.view(anchor_count, batch_size).mean(),loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.view(anchor_count, batch_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "del anchor_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels_multihot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m log_prob \u001b[39m=\u001b[39m logits \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mlog(exp_logits\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# compute mean of log-likelihood over positive\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m mean_log_prob_pos \u001b[39m=\u001b[39m (labels_multihot \u001b[39m*\u001b[39m log_prob)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m mask\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#Y112sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m (temperature \u001b[39m/\u001b[39m base_temperature) \u001b[39m*\u001b[39m mean_log_prob_pos\n",
      "\u001b[0;31mNameError\u001b[0m: name 'labels_multihot' is not defined"
     ]
    }
   ],
   "source": [
    "temperature=0.07\n",
    "base_temperature=0.07\n",
    "\n",
    "# compute logits\n",
    "anchor_dot_contrast = torch.div(anchor_dot_contrast,temperature)\n",
    "# for numerical stability\n",
    "logits_max, _ = torch.max(anchor_dot_contrast, dim=1, keepdim=True)\n",
    "logits = anchor_dot_contrast - logits_max.detach()\n",
    "\n",
    "# compute log_prob\n",
    "exp_logits = torch.exp(logits) \n",
    "log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "\n",
    "# compute mean of log-likelihood over positive\n",
    "mean_log_prob_pos = (labels_multihot * log_prob).sum(1) / labels_multihot.sum(1)\n",
    "\n",
    "# loss\n",
    "loss = - (temperature / base_temperature) * mean_log_prob_pos\n",
    "loss = loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(88.9891)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7918],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.logsumexp(logits,dim=1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -1.7918,   -1.7918,   -1.7918,   -1.7918,   -1.7918,   -1.7918],\n",
       "        [-214.2857,  -85.7143,  -42.8571, -128.5714, -128.5714,    0.0000],\n",
       "        [-285.7143, -114.2857,  -57.1429, -171.4286, -171.4286,    0.0000],\n",
       "        [-142.8571,  -57.1429,  -28.5714,  -85.7143,  -85.7143,    0.0000],\n",
       "        [-142.8571,  -57.1429,  -28.5714,  -85.7143,  -85.7143,    0.0000],\n",
       "        [-357.1429, -142.8571,  -71.4286, -214.2857, -214.2857,    0.0000]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
       "        [  0.0000, 128.5714, 171.4286,  85.7143,  85.7143, 214.2857],\n",
       "        [  0.0000, 171.4286, 228.5714, 114.2857, 114.2857, 285.7143],\n",
       "        [  0.0000,  85.7143, 114.2857,  57.1429,  57.1429, 142.8571],\n",
       "        [  0.0000,  85.7143, 114.2857,  57.1429,  57.1429, 142.8571],\n",
       "        [  0.0000, 214.2857, 285.7143, 142.8571, 142.8571, 357.1429]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0],\n",
       "         [2]],\n",
       "\n",
       "        [[3],\n",
       "         [2]],\n",
       "\n",
       "        [[4],\n",
       "         [5]]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [2],\n",
       "        [5]])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contrast_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  71.4286,  157.1429,  342.8571,  128.5714],\n",
       "        [ 157.1429,  371.4286,  685.7143,  214.2857],\n",
       "        [ 342.8571,  685.7143, 1828.5714,  800.0000],\n",
       "        [ 128.5714,  214.2857,  800.0000,  414.2857]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_dot_contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0.],\n",
       "        [0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0.],\n",
       "        [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=MLP(1000,[10,10],bias=False,norm_layer=torch.nn.BatchNorm1d,activation_layer=torch.nn.Identity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 3])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e=torch.nn.Embedding(100,3)\n",
    "\n",
    "e(torch.arange(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 4.1412e-01,  2.0729e+00,  3.3877e-01],\n",
       "        [ 7.4035e-01, -1.0129e+00,  1.0684e+00],\n",
       "        [-7.6563e-01, -1.6943e-01, -7.2646e-01],\n",
       "        [ 3.0629e-01, -5.6680e-01,  6.6975e-01],\n",
       "        [-4.6175e-03, -4.8004e-01,  1.1684e+00],\n",
       "        [-1.5192e-01,  4.9175e-01, -1.0614e+00],\n",
       "        [-1.7002e-01,  1.8095e-01,  4.0745e-01],\n",
       "        [-1.0855e+00,  1.6527e+00,  1.1391e+00],\n",
       "        [ 7.1451e-01,  2.7505e+00,  5.0293e-01],\n",
       "        [-7.2259e-01, -6.9784e-01,  6.9926e-01],\n",
       "        [-8.0408e-01, -1.9509e+00,  1.9277e+00],\n",
       "        [-1.6251e-01, -1.7948e-01,  6.0711e-01],\n",
       "        [ 1.4911e-01,  3.4602e-01, -1.4749e+00],\n",
       "        [-1.1428e-01,  4.2197e-01, -1.1637e+00],\n",
       "        [-6.9847e-01,  1.1591e+00,  1.7230e-01],\n",
       "        [-4.1416e-01, -1.2346e+00, -1.1913e+00],\n",
       "        [-4.8150e-01,  1.1232e+00,  2.1309e+00],\n",
       "        [ 4.2791e-01,  2.0048e+00,  1.1230e+00],\n",
       "        [ 2.1412e-01,  9.4107e-01, -3.6250e-01],\n",
       "        [ 3.0476e-01, -2.9366e-02,  7.1577e-01],\n",
       "        [ 1.1111e+00,  3.2859e+00, -3.8700e-01],\n",
       "        [-2.6780e-01,  4.5223e-01, -1.5561e+00],\n",
       "        [ 1.7492e+00, -1.2961e+00,  6.7495e-01],\n",
       "        [-1.9354e-01, -1.4452e+00,  7.4329e-01],\n",
       "        [-2.3749e-01, -1.4156e+00,  5.3666e-01],\n",
       "        [ 2.0051e-01,  7.0806e-01, -1.2866e+00],\n",
       "        [ 1.2120e+00, -9.9553e-02, -8.1010e-01],\n",
       "        [-6.4451e-01, -2.9391e-01,  6.8143e-01],\n",
       "        [ 8.9433e-02,  3.1906e-02, -2.2157e-01],\n",
       "        [-6.5695e-01, -2.8275e-01, -1.1841e+00],\n",
       "        [ 3.1109e-01, -7.3270e-02,  1.4882e+00],\n",
       "        [-2.3157e-01,  9.7753e-01, -1.1180e+00],\n",
       "        [ 4.2236e-01,  2.6887e-02, -5.6748e-02],\n",
       "        [-1.1166e+00, -7.5366e-01,  5.2445e-01],\n",
       "        [-7.4717e-01, -2.8172e+00,  1.5306e+00],\n",
       "        [ 2.3401e-01,  2.4709e-01, -5.5698e-01],\n",
       "        [ 5.8064e-02,  2.1018e-01, -1.2149e+00],\n",
       "        [ 2.8053e-01,  6.0376e-01, -2.9387e-01],\n",
       "        [-1.4951e+00,  1.0801e+00,  7.2832e-01],\n",
       "        [ 1.5595e-01, -1.3675e+00, -1.4642e-01],\n",
       "        [-1.2973e-01, -9.1049e-01, -4.4614e-01],\n",
       "        [ 2.3575e-03, -4.3549e-01, -4.4367e-01],\n",
       "        [-3.2981e-01, -6.9884e-01, -1.2228e+00],\n",
       "        [-1.0822e+00, -5.2295e-01,  5.4517e-01],\n",
       "        [ 6.9984e-01,  1.0657e-01,  6.3149e-01],\n",
       "        [-7.8317e-01,  8.6421e-01, -1.1066e+00],\n",
       "        [-5.2400e-01, -7.0290e-01,  1.3407e+00],\n",
       "        [ 3.5794e-01,  1.5427e+00, -2.6545e+00],\n",
       "        [-5.8823e-01, -3.9011e-01, -2.1362e-01],\n",
       "        [ 2.8433e-01, -2.6775e-01, -1.8570e-01],\n",
       "        [ 9.2063e-01,  8.1245e-01,  1.0596e-01],\n",
       "        [ 4.9543e-01,  2.5520e-02,  7.2264e-01],\n",
       "        [-1.5956e+00,  2.1024e+00,  2.3348e+00],\n",
       "        [ 5.0753e-02,  4.4679e-01,  1.1090e+00],\n",
       "        [ 3.6281e-01,  1.5246e+00, -6.1091e-01],\n",
       "        [-2.0431e-03, -3.3988e-01,  5.0113e-01],\n",
       "        [ 3.6552e-01,  1.6496e+00,  1.3314e-01],\n",
       "        [-2.4609e-01, -6.5868e-01,  1.9157e-01],\n",
       "        [-1.1504e+00, -8.1888e-01,  4.3923e-01],\n",
       "        [ 4.3039e-01,  7.1507e-02, -3.7653e-01],\n",
       "        [-1.0791e+00, -2.4048e-01,  4.2345e-01],\n",
       "        [ 1.9567e+00,  2.2771e-01, -1.5723e-01],\n",
       "        [ 4.9176e-01,  8.1366e-01, -7.2535e-01],\n",
       "        [-2.6337e-01,  4.0416e-02, -5.9337e-01],\n",
       "        [ 3.0209e-01,  2.1540e-01, -4.7287e-01],\n",
       "        [-1.0621e+00,  1.6018e+00, -7.7593e-01],\n",
       "        [ 2.1298e-02,  4.7223e-01, -1.9026e-01],\n",
       "        [ 8.6005e-01,  9.4706e-01,  7.5838e-01],\n",
       "        [-6.0646e-01, -3.9555e-01,  8.4763e-01],\n",
       "        [-8.4835e-01,  6.0907e-01,  1.5922e+00],\n",
       "        [ 4.8856e-01,  1.0006e+00, -1.4859e-01],\n",
       "        [ 4.2223e-01, -1.4388e+00, -1.0208e+00],\n",
       "        [ 1.0276e+00, -1.3861e+00, -4.3115e-01],\n",
       "        [-1.0761e-02, -9.9919e-02,  8.8677e-01],\n",
       "        [ 3.9599e-01, -6.0225e-01, -1.0776e+00],\n",
       "        [ 5.0202e-01, -3.1856e-01,  1.5525e+00],\n",
       "        [-6.3705e-01, -9.2683e-01, -7.6245e-01],\n",
       "        [ 9.3045e-01, -9.2651e-01,  3.0462e-02],\n",
       "        [-1.1075e+00,  1.3727e+00, -2.0777e-01],\n",
       "        [ 1.5397e+00,  2.7782e-01, -2.4375e-01],\n",
       "        [ 6.1954e-01,  3.7248e-01,  1.3380e+00],\n",
       "        [-1.6130e-01,  5.3568e-01,  1.9587e+00],\n",
       "        [-1.8537e+00, -1.1964e-01, -6.6178e-01],\n",
       "        [ 1.4381e+00,  4.4171e-01,  1.7309e+00],\n",
       "        [ 1.8737e+00, -2.3101e-01,  5.5219e-01],\n",
       "        [ 8.4100e-02, -7.0921e-01,  6.6169e-01],\n",
       "        [-8.8450e-01,  1.0118e+00, -1.0265e-01],\n",
       "        [ 7.7554e-01, -6.7150e-01,  6.8879e-01],\n",
       "        [-2.2528e+00,  4.5865e-02,  2.2675e-01],\n",
       "        [-7.1048e-01,  1.5160e+00, -3.9299e-01],\n",
       "        [ 1.7056e-01, -2.1750e-01, -1.9245e-01],\n",
       "        [-1.6822e+00, -4.4930e-01,  1.4788e+00],\n",
       "        [ 2.4178e+00,  1.7688e+00, -6.7901e-01],\n",
       "        [ 1.9154e+00,  5.7157e-01,  1.5687e+00],\n",
       "        [-2.1726e+00, -1.4869e+00, -1.2950e+00],\n",
       "        [ 7.0746e-01,  1.3852e+00, -3.9818e-01],\n",
       "        [-2.6641e-01, -2.3976e+00, -6.6122e-01],\n",
       "        [-2.9886e-01, -1.4048e-01, -1.0224e-01],\n",
       "        [-7.1825e-01, -3.1057e-01, -1.4953e+00],\n",
       "        [ 2.6664e-01,  1.0835e+00,  7.4522e-01]], requires_grad=True)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4141,  2.0729,  0.3388],\n",
       "        [ 0.7403, -1.0129,  1.0684],\n",
       "        [-0.7656, -0.1694, -0.7265],\n",
       "        [ 0.3063, -0.5668,  0.6697],\n",
       "        [-0.0046, -0.4800,  1.1684],\n",
       "        [-0.1519,  0.4917, -1.0614],\n",
       "        [-0.1700,  0.1810,  0.4074],\n",
       "        [-1.0855,  1.6527,  1.1391],\n",
       "        [ 0.7145,  2.7505,  0.5029],\n",
       "        [-0.7226, -0.6978,  0.6993]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-25 03:45:01 PST INFO Logging to ./outputs/logs/2023-11-25_03-45-01_Test.log and console...\n",
      "2023-11-25 03:45:01 PST INFO Using device: cuda:0\n",
      "2023-11-25 03:45:01 PST INFO {\n",
      "    \"TRAIN_BATCH_SIZE\": 64,\n",
      "    \"VALIDATION_BATCH_SIZE\": 64,\n",
      "    \"TEST_BATCH_SIZE\": 64,\n",
      "    \"IN_BATCH_SAMPLING\": false,\n",
      "    \"TRAIN_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"VALIDATION_LABEL_SAMPLE_SIZE\": null,\n",
      "    \"LABEL_BATCH_SIZE_LIMIT_NO_GRAD\": 1500,\n",
      "    \"SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD\": 128,\n",
      "    \"LEARNING_RATE\": 0.001,\n",
      "    \"OPTIMIZER\": \"Adam\",\n",
      "    \"PROTEIN_EMBEDDING_DIM\": 1100,\n",
      "    \"LABEL_EMBEDDING_DIM\": 1024,\n",
      "    \"LATENT_EMBEDDING_DIM\": 1024,\n",
      "    \"OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR\": 1,\n",
      "    \"OUTPUT_MLP_NUM_LAYERS\": 2,\n",
      "    \"OUTPUT_NEURON_PROBABILITY_BIAS\": null,\n",
      "    \"OUTPUT_MLP_BATCHNORM\": true,\n",
      "    \"OPTIMIZATION_METRIC_NAME\": \"map_micro\",\n",
      "    \"DECISION_TH_METRIC_NAME\": \"f1_micro\",\n",
      "    \"NUM_EPOCHS\": 15,\n",
      "    \"GRADIENT_ACCUMULATION_STEPS\": 1,\n",
      "    \"GRADIENT_CHECKPOINTING\": false,\n",
      "    \"LORA\": false,\n",
      "    \"LORA_RANK\": 8,\n",
      "    \"CLIP_VALUE\": 1,\n",
      "    \"LOSS_FN\": \"WeightedBCE\",\n",
      "    \"FOCAL_LOSS_GAMMA\": 2,\n",
      "    \"FOCAL_LOSS_ALPHA\": -1,\n",
      "    \"BCE_POS_WEIGHT\": 1,\n",
      "    \"RGDBCE_TEMP\": 0.12,\n",
      "    \"TRAIN_SEQUENCE_ENCODER\": false,\n",
      "    \"TRAIN_LABEL_ENCODER\": false,\n",
      "    \"DISTRIBUTE_LABELS\": false,\n",
      "    \"TRAIN_PROJECTION_HEAD\": true,\n",
      "    \"LABEL_ENCODER_CHECKPOINT\": \"microsoft/biogpt\",\n",
      "    \"DEDUPLICATE\": true,\n",
      "    \"NORMALIZE_PROBABILITIES\": false,\n",
      "    \"SEED\": 42,\n",
      "    \"VALIDATIONS_PER_EPOCH\": 100,\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"DECISION_TH\": null,\n",
      "    \"TRAIN_SUBSET_FRACTION\": 1,\n",
      "    \"VALIDATION_SUBSET_FRACTION\": 1,\n",
      "    \"TEST_SUBSET_FRACTION\": 1,\n",
      "    \"SHUFFLE_LABELS\": true\n",
      "}\n",
      "2023-11-25 03:45:05 PST INFO Loaded amino_acid_vocab vocabulary from ./data/vocabularies/proteinfer/amino_acid_vocab.json\n",
      "2023-11-25 03:45:05 PST INFO Loaded GO_label_vocab vocabulary from ./data/vocabularies/proteinfer/GO_label_vocab.json\n",
      "2023-11-25 03:45:05 PST INFO Loaded sequence_id_vocab vocabulary from ./data/vocabularies/proteinfer/sequence_id_vocab.json\n",
      "2023-11-25 03:45:11 PST INFO Removing 66586 duplicate sequences from ./data/swissprot/proteinfer_splits/random/train_GO.fasta...\n",
      "2023-11-25 03:45:24 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:25 PST INFO Removing 8479 duplicate sequences from ./data/swissprot/proteinfer_splits/random/dev_GO.fasta...\n",
      "2023-11-25 03:45:36 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:37 PST INFO Removing 8176 duplicate sequences from ./data/swissprot/proteinfer_splits/random/test_GO.fasta...\n",
      "2023-11-25 03:45:48 PST INFO Loaded label embeddings from ./data/None\n",
      "2023-11-25 03:45:48 PST INFO ################## 2023-11-25_03-45-01 RUNNING main.py ##################\n",
      "2023-11-25 03:45:49 PST INFO Loaded sequence embeddings from ./data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f639c7977c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "### SETUP ###\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check if master process\n",
    "is_master = True\n",
    "config = \"configs/base_config.yaml\"\n",
    "name = \"Test\"\n",
    "train_path_name = \"TRAIN_DATA_PATH\"\n",
    "validation_path_name = \"VAL_DATA_PATH\"\n",
    "test_paths_names = [\"TEST_DATA_PATH\"]\n",
    "amlt = False\n",
    "gpu=0\n",
    "rank=0\n",
    "\n",
    "# Unpack and process the config file\n",
    "config = get_setup(\n",
    "    config_path=config,\n",
    "    run_name=name,\n",
    "    overrides=[],\n",
    "    train_path_name=train_path_name,\n",
    "    val_path_name=validation_path_name,\n",
    "    test_paths_names=test_paths_names,\n",
    "    amlt=amlt,\n",
    "    is_master=is_master,\n",
    ")\n",
    "params, paths, timestamp, logger = config[\"params\"], config[\n",
    "    \"paths\"], config[\"timestamp\"], config[\"logger\"]\n",
    "\n",
    "# Set the GPU device, if using\n",
    "torch.cuda.set_device(gpu)\n",
    "device = torch.device('cuda:' + str(gpu)\n",
    "                        if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Log the params\n",
    "logger.info(json.dumps(params, indent=4))\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "if params[\"GRADIENT_CHECKPOINTING\"]:\n",
    "    raise NotImplementedError(\n",
    "        \"Gradient checkpointing is not yet implemented.\")\n",
    "\n",
    "if params[\"LORA\"]:\n",
    "    for layer in label_encoder.layers:\n",
    "        in_features, out_features = 1024, 1024\n",
    "        layer.self_attn.q_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.v_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.k_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "        layer.self_attn.out_proj = lora.Linear(\n",
    "            in_features, out_features, r=params[\"LORA_RANK\"])\n",
    "    # Mark only the LoRA parameters as trainable\n",
    "    lora.mark_only_lora_as_trainable(label_encoder)\n",
    "\n",
    "label_encoder = label_encoder.to(device)\n",
    "\n",
    "# Load or generate the vocabularies\n",
    "vocabularies = get_or_generate_vocabularies(\n",
    "    paths[\"FULL_DATA_PATH\"], paths[\"VOCABULARIES_DIR\"], logger)\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(\n",
    "    paths_list=config['dataset_paths_list'],\n",
    "    config=config,\n",
    "    logger=logger,\n",
    "    label_tokenizer=label_tokenizer,\n",
    "    label_encoder=label_encoder,\n",
    "    vocabularies=vocabularies,\n",
    "    subset_fractions={\n",
    "        \"train\": params[\"TRAIN_SUBSET_FRACTION\"],\n",
    "        \"validation\": params[\"VALIDATION_SUBSET_FRACTION\"],\n",
    "        \"test\": params[\"TEST_SUBSET_FRACTION\"],\n",
    "    },\n",
    "    deduplicate=params[\"DEDUPLICATE\"],\n",
    ")\n",
    "\n",
    "# Seed everything so we don't go crazy\n",
    "seed_everything(params[\"SEED\"], device)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING main.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    \"train\": params[\"TRAIN_LABEL_SAMPLE_SIZE\"],\n",
    "    \"validation\": params[\"VALIDATION_LABEL_SAMPLE_SIZE\"],\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    shuffle_labels=params['SHUFFLE_LABELS'],\n",
    "    in_batch_sampling=params['IN_BATCH_SAMPLING'],\n",
    "    num_workers=params[\"NUM_WORKERS\"],\n",
    "    world_size=1,\n",
    "    rank=rank,\n",
    ")\n",
    "\n",
    "if not params[\"TRAIN_LABEL_ENCODER\"]:\n",
    "    # Move the label encoder to CPU\n",
    "    label_encoder = label_encoder.cpu()\n",
    "\n",
    "# Initialize ProteInfer\n",
    "sequence_encoder = ProteInfer.from_pretrained(\n",
    "    weights_path=paths[\"PROTEINFER_WEIGHTS_PATH\"],\n",
    "    num_labels=config[\"embed_sequences_params\"][\"PROTEINFER_NUM_LABELS\"],\n",
    "    input_channels=config[\"embed_sequences_params\"][\"INPUT_CHANNELS\"],\n",
    "    output_channels=config[\"embed_sequences_params\"][\"OUTPUT_CHANNELS\"],\n",
    "    kernel_size=config[\"embed_sequences_params\"][\"KERNEL_SIZE\"],\n",
    "    activation=torch.nn.ReLU,\n",
    "    dilation_base=config[\"embed_sequences_params\"][\"DILATION_BASE\"],\n",
    "    num_resnet_blocks=config[\"embed_sequences_params\"][\"NUM_RESNET_BLOCKS\"],\n",
    "    bottleneck_factor=config[\"embed_sequences_params\"][\"BOTTLENECK_FACTOR\"],\n",
    ")\n",
    "\n",
    "# Generate all sequence embeddings upfront, if not training the sequence encoder\n",
    "sequence_embedding_df = None\n",
    "if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "    sequence_embedding_df = get_or_generate_sequence_embeddings(\n",
    "        paths,\n",
    "        device,\n",
    "        sequence_encoder,\n",
    "        datasets,\n",
    "        params,\n",
    "        logger,\n",
    "    )\n",
    "    sequence_encoder = sequence_encoder.to('cpu')\n",
    "\n",
    "# Loop through all the datasets and set the sequence embedding df\n",
    "for dataset in datasets.values():\n",
    "    for subset in dataset:\n",
    "        if not params[\"TRAIN_SEQUENCE_ENCODER\"]:\n",
    "            subset.set_sequence_embedding_df(sequence_embedding_df)\n",
    "\n",
    "\n",
    "loaders[\"train\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast\n",
    "def tokenize_labels(text, tokenizer, max_length=1024):\n",
    "    \"\"\"\n",
    "    Tokenize a list of text strings.\n",
    "\n",
    "    Args:\n",
    "        text (list): The list of text strings.\n",
    "        tokenizer (transformers.PreTrainedTokenizer): The tokenizer.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing tokenized labels as 'input_ids' and 'attention_mask'.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        text, padding='longest', truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_mean_hidden_states(last_hidden_states, attention_mask):\n",
    "    \"\"\"Compute the mean of the last hidden state for only the relevant tokens.\"\"\"\n",
    "    # Compute the number of relevant tokens for each sequence\n",
    "    num_relevant_tokens = attention_mask.sum(dim=1, keepdim=True)\n",
    "    # Mask the last_hidden_state tensor and compute the sum\n",
    "    sum_hidden_states = (last_hidden_states *\n",
    "                         attention_mask.unsqueeze(-1)).sum(dim=1)\n",
    "    # Compute the mean of the last hidden state\n",
    "    return sum_hidden_states / num_relevant_tokens\n",
    "\n",
    "\n",
    "def get_label_embeddings(tokenized_labels, model, batch_size_limit=1000):\n",
    "    \"\"\"\n",
    "    Get embeddings for a list of tokenized labels.\n",
    "    Assumes that tokenized_labels and model are on the same device, ideally GPU.\n",
    "    \"\"\"\n",
    "    total_labels = tokenized_labels[\"input_ids\"].shape[0]\n",
    "\n",
    "    if total_labels <= batch_size_limit:\n",
    "        with autocast():\n",
    "            last_hidden_states = model(\n",
    "                input_ids=tokenized_labels[\"input_ids\"],\n",
    "                attention_mask=tokenized_labels[\"attention_mask\"]\n",
    "            ).last_hidden_state\n",
    "        output = compute_mean_hidden_states(\n",
    "            last_hidden_states, tokenized_labels[\"attention_mask\"])\n",
    "        del last_hidden_states\n",
    "        return output\n",
    "\n",
    "    else:\n",
    "        # Convert dictionary values to tensors\n",
    "        tensors = [tokenized_labels[\"input_ids\"],\n",
    "                   tokenized_labels[\"attention_mask\"]]\n",
    "        # Create TensorDataset and DataLoader\n",
    "        dataset = TensorDataset(*tensors)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size_limit,\n",
    "                                shuffle=False, pin_memory=False, num_workers=0)\n",
    "\n",
    "        all_label_embeddings = []\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask = batch\n",
    "            with autocast():\n",
    "                last_hidden_states = model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "            mean_hidden_states = compute_mean_hidden_states(\n",
    "                last_hidden_states, attention_mask)\n",
    "            all_label_embeddings.append(mean_hidden_states)\n",
    "            del last_hidden_states, mean_hidden_states\n",
    "        # Concatenate all the label embeddings\n",
    "        return torch.cat(all_label_embeddings, dim=0)\n",
    "\n",
    "\n",
    "def generate_label_embeddings_from_text(label_annotations, label_tokenizer, label_encoder, batch_size_limit=1000):\n",
    "    \"\"\"Tokenize the labels and generate label embeddings.\"\"\"\n",
    "    tokenized_labels = tokenize_labels(label_annotations, label_tokenizer)\n",
    "\n",
    "    # Move to GPU\n",
    "    tokenized_labels[\"input_ids\"] = tokenized_labels[\"input_ids\"].to(\n",
    "        label_encoder.device)\n",
    "    tokenized_labels[\"attention_mask\"] = tokenized_labels[\"attention_mask\"].to(\n",
    "        label_encoder.device)\n",
    "\n",
    "    # Generate label embeddings\n",
    "    return get_label_embeddings(tokenized_labels, label_encoder, batch_size_limit=batch_size_limit)\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")\n",
    "\n",
    "# Initialize label encoder\n",
    "label_encoder = AutoModel.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import read_pickle\n",
    "annot=read_pickle('data/annotations/go_annotations_2019_07_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GO:0070327',\n",
       " 'The directed movement of thyroid hormone into, out of or within a cell, or between cells, by means of some agent such as a transporter or pore.')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=32000\n",
    "annot.index[i],annot.iloc[i]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22605"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0].label2int[annot.index[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8438,  0.1259,  0.2046,  ...,  0.4670, -0.1736,  0.8953]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_label_embeddings_from_text([annot.iloc[i]['label']],label_tokenizer=label_tokenizer,label_encoder=label_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_iter = iter(loaders[\"train\"][0])\n",
    "data_iter = iter(datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_batch = next(data_iter)\n",
    "loader_batch=next(loader_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8445,  0.1256,  0.2044,  ...,  0.4676, -0.1743,  0.8949])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader_batch['label_embeddings'][22605]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13652"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0].label2int['GO:0035639']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([datasets[\"train\"][0].label2int[i] for i in datasets[\"train\"][0].data[0][1][1:]])==torch.where(data_batch['label_multihots']==1)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GO:0035639',\n",
       " 'GO:0032553',\n",
       " 'GO:0005524',\n",
       " 'GO:0017076',\n",
       " 'GO:0005737',\n",
       " 'GO:1901265',\n",
       " 'GO:1901363',\n",
       " 'GO:0043168',\n",
       " 'GO:0044424',\n",
       " 'GO:0030554',\n",
       " 'GO:0005488',\n",
       " 'GO:0043167',\n",
       " 'GO:0042026',\n",
       " 'GO:0032559',\n",
       " 'GO:0005515',\n",
       " 'GO:0051082',\n",
       " 'GO:0032555',\n",
       " 'GO:0005575',\n",
       " 'GO:0008144',\n",
       " 'GO:0009987',\n",
       " 'GO:0097159',\n",
       " 'GO:0006457',\n",
       " 'GO:0000166',\n",
       " 'GO:0008150',\n",
       " 'GO:0036094',\n",
       " 'GO:0003674',\n",
       " 'GO:0044464',\n",
       " 'GO:0097367']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0].data[0][1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_onehots': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'sequence_id': 'P60545',\n",
       " 'sequence_embedding': tensor([-0.0553, -0.3441, -0.2825,  ...,  0.4497, -0.0895, -0.1504]),\n",
       " 'sequence_length': tensor(538),\n",
       " 'label_multihots': tensor([0, 0, 0,  ..., 0, 0, 0]),\n",
       " 'tokenized_labels': {'input_ids': tensor([[   2,   18,  569,  ...,    1,    1,    1],\n",
       "         [   2,   18, 1900,  ...,    1,    1,    1],\n",
       "         [   2,   18,  371,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   2,   18,  919,  ...,    1,    1,    1],\n",
       "         [   2,   18,  919,  ...,    1,    1,    1],\n",
       "         [   2,   18,  919,  ...,    1,    1,    1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'label_embeddings': tensor([[-1.3426e+00,  1.9259e-01,  4.5337e-01,  ..., -6.7419e-02,\n",
       "           1.7350e-01,  8.9762e-01],\n",
       "         [-5.8517e-01,  2.5346e-03,  9.9431e-01,  ...,  7.3632e-01,\n",
       "           1.3791e+00,  1.2030e+00],\n",
       "         [-4.8449e-01, -2.6923e-01,  1.7874e-01,  ..., -3.5807e-01,\n",
       "           8.9524e-01,  8.7176e-01],\n",
       "         ...,\n",
       "         [-2.0514e-01, -1.0103e+00,  1.2279e+00,  ...,  3.6141e-01,\n",
       "          -3.4265e-01,  5.1903e-01],\n",
       "         [-8.9557e-01, -5.3069e-01,  9.3757e-01,  ..., -1.8156e-01,\n",
       "          -2.4020e-02, -9.7481e-04],\n",
       "         [-7.9217e-01, -9.6587e-01,  1.2481e+00,  ...,  4.1990e-01,\n",
       "          -3.4655e-01,  1.0383e-02]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = torch.load('data/embeddings/frozen_BioGPT_label_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3426e+00,  1.9259e-01,  4.5337e-01,  ..., -6.7419e-02,\n",
       "          1.7350e-01,  8.9762e-01],\n",
       "        [-5.8517e-01,  2.5346e-03,  9.9431e-01,  ...,  7.3632e-01,\n",
       "          1.3791e+00,  1.2030e+00],\n",
       "        [-4.8449e-01, -2.6923e-01,  1.7874e-01,  ..., -3.5807e-01,\n",
       "          8.9524e-01,  8.7176e-01],\n",
       "        ...,\n",
       "        [-2.0514e-01, -1.0103e+00,  1.2279e+00,  ...,  3.6141e-01,\n",
       "         -3.4265e-01,  5.1903e-01],\n",
       "        [-8.9557e-01, -5.3069e-01,  9.3757e-01,  ..., -1.8156e-01,\n",
       "         -2.4020e-02, -9.7481e-04],\n",
       "        [-7.9217e-01, -9.6587e-01,  1.2481e+00,  ...,  4.1990e-01,\n",
       "         -3.4655e-01,  1.0383e-02]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3426e+00,  1.9259e-01,  4.5337e-01,  ..., -6.7419e-02,\n",
       "          1.7350e-01,  8.9762e-01],\n",
       "        [-5.8517e-01,  2.5346e-03,  9.9431e-01,  ...,  7.3632e-01,\n",
       "          1.3791e+00,  1.2030e+00],\n",
       "        [-4.8449e-01, -2.6923e-01,  1.7874e-01,  ..., -3.5807e-01,\n",
       "          8.9524e-01,  8.7176e-01],\n",
       "        ...,\n",
       "        [-2.0514e-01, -1.0103e+00,  1.2279e+00,  ...,  3.6141e-01,\n",
       "         -3.4265e-01,  5.1903e-01],\n",
       "        [-8.9557e-01, -5.3069e-01,  9.3757e-01,  ..., -1.8156e-01,\n",
       "         -2.4020e-02, -9.7481e-04],\n",
       "        [-7.9217e-01, -9.6587e-01,  1.2481e+00,  ...,  4.1990e-01,\n",
       "         -3.4655e-01,  1.0383e-02]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['label_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_e = a['sequence_embeddings']\n",
    "L_e = a['label_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:15<00:00,  4.23it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "joint = []\n",
    "for i in tqdm(P_e):\n",
    "    for j in L_e:\n",
    "        joint.append(torch.concat([i,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'repeat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/samirchar/ProteinFunctions/notebooks/debugging copy.ipynb Cell 16\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbastion_tunnel/home/samirchar/ProteinFunctions/notebooks/debugging%20copy.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39;49mrepeat(\u001b[39m10\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch' has no attribute 'repeat'"
     ]
    }
   ],
   "source": [
    "torch.repe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 5448.56it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "joint = []\n",
    "joint_matrix = []\n",
    "for i in tqdm(range(10)):\n",
    "    joint_rows=[]\n",
    "    for j in range(11,15):\n",
    "        i_ = torch.tensor([i]*5)\n",
    "        j_ = torch.tensor([j]*7)\n",
    "        concat = torch.concat([i_,j_])\n",
    "        joint.append(concat)\n",
    "        joint_rows.append(concat)\n",
    "    joint_rows = torch.stack(joint_rows)\n",
    "    joint_matrix.append(joint_rows)\n",
    "\n",
    "#joint = torch.stack(joint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77,  84,  91,  98],\n",
       "        [ 82,  89,  96, 103],\n",
       "        [ 87,  94, 101, 108],\n",
       "        [ 92,  99, 106, 113],\n",
       "        [ 97, 104, 111, 118],\n",
       "        [102, 109, 116, 123],\n",
       "        [107, 114, 121, 128],\n",
       "        [112, 119, 126, 133],\n",
       "        [117, 124, 131, 138],\n",
       "        [122, 129, 136, 143]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(joint_matrix).sum(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(103)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(joint_matrix)[1][3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 77,  84,  91,  98],\n",
       "        [ 82,  89,  96, 103],\n",
       "        [ 87,  94, 101, 108],\n",
       "        [ 92,  99, 106, 113],\n",
       "        [ 97, 104, 111, 118],\n",
       "        [102, 109, 116, 123],\n",
       "        [107, 114, 121, 128],\n",
       "        [112, 119, 126, 133],\n",
       "        [117, 124, 131, 138],\n",
       "        [122, 129, 136, 143]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint.sum(axis=1).reshape(10,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 12])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-80840.0156)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint.sum(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = P_e.shape[0]\n",
    "num_labels = L_e.shape[0]\n",
    "sequence_embedding_dim = P_e.shape[1]\n",
    "label_embedding_dim = L_e.shape[1]\n",
    "\n",
    "# Use broadcasting so we don't have to expand the tensor dimensions\n",
    "joint_embeddings = torch.cat([\n",
    "    P_e[:, None, :].expand(\n",
    "        num_sequences, num_labels, sequence_embedding_dim),\n",
    "    L_e[None, :, :].expand(\n",
    "        num_sequences, num_labels, label_embedding_dim)\n",
    "], dim=2).reshape(-1, sequence_embedding_dim + label_embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-80840.0156)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joint_embeddings.sum(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 1],\n",
       "        [0, 1, 1],\n",
       "        [1, 1, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1,0,1,0,1,1,1,1,0]).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(\n",
    "    description=\"Train and/or Test the ProTCL model.\")\n",
    "parser.add_argument(\"--train-path-name\", type=str, default=None,\n",
    "                    help=\"Specify the desired train path name to train the model using names from config file. If not provided, model will not be trained. If provided, must also provide --val-path.\")\n",
    "\n",
    "parser.add_argument(\"--validation-path-name\", type=str, default=None,\n",
    "                    help=\"Specify the desired val path name to validate the model during training using names from config file. If not provided, model will not be trained. If provided, must also provide --train-path.\")\n",
    "\n",
    "parser.add_argument(\"--full-path-name\", type=str, default=None,\n",
    "                    help=\"Specify the desired full path name to define the vocabularies. Defaults to the full path name in the config file.\")\n",
    "\n",
    "parser.add_argument(\"--test-paths-names\", nargs=\"+\", type=str, default=None,\n",
    "                    help=\"Specify all the desired test paths names to test the model using names from config file to test. If not provided, model will not be tested.\")\n",
    "\n",
    "parser.add_argument(\"--use-wandb\", action=\"store_true\", default=False,\n",
    "                    help=\"Use Weights & Biases for logging. Default is False.\")\n",
    "\n",
    "parser.add_argument(\"--load-model\", type=str, default=None,\n",
    "                    help=\"(Relative) path to the model to be loaded. If not provided, a new model will be initialized.\")\n",
    "\n",
    "parser.add_argument('--from-checkpoint', action=\"store_true\", default=False,\n",
    "                    help=\"Continue training from a previous model checkpoint (including optimizer state and epoch). Default is False.\")\n",
    "\n",
    "parser.add_argument(\"--name\", type=str, default=\"ProTCL\",\n",
    "                    help=\"Name of the W&B run. If not provided, a name will be generated.\")\n",
    "\n",
    "parser.add_argument(\"--config\", type=str, default=\"configs/base_config.yaml\",\n",
    "                    help=\"(Relative) path to the configuration file.\")\n",
    "\n",
    "parser.add_argument(\"--amlt\", action=\"store_true\", default=False,\n",
    "                    help=\"Run job on Amulet. Default is False.\")\n",
    "\n",
    "parser.add_argument(\"--override\", nargs=\"*\",\n",
    "                    help=\"Override config parameters in key-value pairs.\")\n",
    "\n",
    "parser.add_argument(\"--save-prediction-results\", action=\"store_true\", default=False,\n",
    "                    help=\"Save predictions and ground truth dataframe for validation and/or test\")\n",
    "\n",
    "parser.add_argument('-n', '--nodes', default=1, type=int,\n",
    "                    metavar='N', help='Number of nodes (default: 1)')\n",
    "\n",
    "parser.add_argument('-g', '--gpus', default=1, type=int,\n",
    "                    help='Number of gpus per node (default: 1)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
