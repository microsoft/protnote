{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/min_lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from pathlib import Path\n",
    "from pynvml import *\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# seq_len, dataset_size = 512, 512\n",
    "# dummy_data = {\n",
    "#     \"input_ids\": np.random.randint(100, 30000, (dataset_size, seq_len)),\n",
    "#     \"labels\": np.random.randint(0, 1, (dataset_size)),\n",
    "# }\n",
    "# ds = Dataset.from_dict(dummy_data)\n",
    "# ds.set_format(\"pt\")\n",
    "\n",
    "# Load test dataset\n",
    "GO_ANNOTATIONS_PATH = \"/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\"\n",
    "go_annotations = read_pickle(GO_ANNOTATIONS_PATH)\n",
    "\n",
    "# Get first 1000 labels as a list\n",
    "text = go_annotations.iloc[:1000, 0].tolist()\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 47 MB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/min_lora/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "Some weights of BioGptForSequenceClassification were not initialized from the model checkpoint at microsoft/biogpt and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 2292 MB.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "print_gpu_utilization()\n",
    "checkpoint = 'microsoft/biogpt'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    # torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "print_gpu_utilization()\n",
    "\n",
    "default_args = {\n",
    "    \"output_dir\": \"tmp\",\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"do_eval\": False,\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"log_level\": \"error\",\n",
    "    \"report_to\": \"none\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loralib as lora\n",
    "for layer in model.biogpt.layers:\n",
    "    # This assumes in_features and out_features are 1024 as per the model details you shared\n",
    "    in_features, out_features = 1024, 1024\n",
    "    layer.self_attn.q_proj = lora.Linear(in_features, out_features, r=16)  # Choose rank r as per requirement\n",
    "    layer.self_attn.v_proj = lora.Linear(in_features, out_features, r=16)  # Choose rank r as per requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 348338176 || trainable%: 0.45\n"
     ]
    }
   ],
   "source": [
    "lora.mark_only_lora_as_trainable(model)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 348338176 || trainable%: 0.45\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Tokenize the go_annotations list\n",
    "tokenized_data = tokenizer(text, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Create random labels for the new dataset\n",
    "random_labels = np.random.randint(0, 1, (len(text)))\n",
    "\n",
    "# Create the dataset from the tokenized data and random labels\n",
    "dummy_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": tokenized_data[\"input_ids\"],\n",
    "    \"attention_mask\": tokenized_data[\"attention_mask\"], \n",
    "    \"labels\": random_labels\n",
    "})\n",
    "\n",
    "# Set the format to PyTorch tensors\n",
    "dummy_dataset.set_format(\"pt\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# config = LoraConfig(\n",
    "#     r=4,\n",
    "#     lora_alpha=8,\n",
    "#     target_modules=[\"k_proj\", \"v_proj\"],\n",
    "#     lora_dropout=0.5,\n",
    "#     bias=\"none\",\n",
    "#     modules_to_save=[\"classifier\"],\n",
    "# )\n",
    "# lora_model = get_peft_model(model, config)\n",
    "# print_trainable_parameters(lora_model)\n",
    "\n",
    "# # Set requires grad to true for all parameters\n",
    "# for param in lora_model.parameters():\n",
    "#     param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model on cuda: True\n",
      "{'train_runtime': 18.9111, 'train_samples_per_second': 52.879, 'train_steps_per_second': 0.264, 'train_loss': 1.65550537109375, 'epoch': 1.0}\n",
      "Time: 18.91\n",
      "Samples/second: 52.88\n",
      "GPU memory occupied: 34948 MB.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, logging\n",
    "import transformers\n",
    "import accelerate\n",
    "import peft\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "# model = lora_model\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "test_model = model\n",
    "\n",
    "# Check if model is on cuda\n",
    "print(f\"Model on cuda: {next(test_model.parameters()).is_cuda}\")\n",
    "\n",
    "batch_size = 150\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=1,\n",
    "    gradient_checkpointing=True,\n",
    "    **default_args\n",
    ")\n",
    "trainer = Trainer(model=test_model, args=training_args, train_dataset=dummy_dataset)\n",
    "result = trainer.train()\n",
    "print_summary(result)\n",
    "\n",
    "# 40 GB with batch size = 40, float16, trains in 5 seconds\n",
    "# 73 GB with batch size = 40, trains in 34 seconds\n",
    "\n",
    "# With LoRA, batch size = 40: 30974  MB\n",
    "# Without LoRA, batch size = 40: 39228 MB.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
