{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from src.utils.data import read_pickle\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/label_embeddings.pk1\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pk1\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, val_dataset, test_dataset = ProteinDataset\\\n",
    "    .create_multiple_datasets(data_paths=[TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH],\n",
    "                              sequence_vocabulary_path=AMINO_ACID_VOCAB_PATH)\n",
    "\n",
    "# Create label voculabary by merging sets from train_loader, val_loader, and test_loader\n",
    "master_label_vocabulary = list(set(train_dataset.label_vocabulary) | set(val_dataset.label_vocabulary) | set(test_dataset.label_vocabulary))\n",
    "\n",
    "# Save master_label_vocabulary to JSON file\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'w') as f:\n",
    "    json.dump(master_label_vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522607\n",
      "['A0A023GPI8', 'A0A023IWD9', 'A0A023IWE0', 'A0A023IWE1', 'A0A023IWE2', 'A0A023IWE3', 'A0A023IWG1', 'A0A023IWG2', 'A0A023IWG3', 'A0A023IWG4']\n",
      "True\n",
      "522607\n",
      "522607\n",
      "439540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "# Import read_pickle\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# Load /home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl\n",
    "sequence_id_map = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl')\n",
    "\n",
    "# Print the shape and column names\n",
    "print(len(sequence_id_map))\n",
    "\n",
    "# Print the first 10 keys\n",
    "print(list(sequence_id_map.keys())[:10])\n",
    "\n",
    "# Check if the keys are unique\n",
    "print(len(sequence_id_map) == len(set(sequence_id_map.keys())))\n",
    "\n",
    "# Load sequence embeddings \n",
    "sequence_embeddings = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_embeddings.pkl')\n",
    "print(len(sequence_embeddings))\n",
    "\n",
    "# Load old embeddings from /home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth with pytorch\n",
    "old_sequence_embeddings = torch.load('/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth')\n",
    "\n",
    "# Print number of old embeddings\n",
    "print(len(old_sequence_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new map from label ID to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000001', 'GO:0000002', 'GO:0000003', 'GO:0000006', 'GO:0000007', 'GO:0000009', 'GO:0000010', 'GO:0000011', 'GO:0000012', 'GO:0000014']\n"
     ]
    }
   ],
   "source": [
    "# Load vocab from /home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json\n",
    "import json\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'r') as f:\n",
    "    label_vocab = json.load(f)\n",
    "\n",
    "# Print the first 10 labels\n",
    "print(label_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from /home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\n",
    "from src.utils.data import read_pickle\n",
    "annotations = read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the annotations df to be only the labels in label_vocab. In annotations, the go id is the index\n",
    "annotations = annotations[annotations.index.isin(label_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT model\n",
    "from src.utils.models import load_PubMedBERT\n",
    "tokenizer, model = load_PubMedBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  2648,  5316,  2052,  1977,  4663,  2176,  1977,  3189,  1942,\n",
      "          6922,     5,    51,  2832,  4747,  9023,  2004,     5,     3],\n",
      "        [    2, 18208,  1023,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT tokenizer\n",
    "inputs = tokenizer([\"Test text This is what it is like to live! I will run forever!\", \"BLAH\"], return_tensors=\"pt\",\n",
    "                       truncation=True, padding=True, max_length=512)\n",
    "print(inputs)\n",
    "print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.loc[:, 'tokenized_label_text'] = annotations['label'].apply(lambda x: tokenizer(x, return_tensors=\"pt\", truncation=True, padding=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'tokenized_label_text', 'input_ids', 'attention_mask',\n",
       "       'token_type_ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all labels in the dataframe in a batched manner\n",
    "tokenized_outputs = tokenizer(annotations['label'].tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tokenized outputs to the dataframe\n",
    "annotations['input_ids'] = tokenized_outputs['input_ids']\n",
    "annotations['attention_mask'] = tokenized_outputs['attention_mask']\n",
    "if 'token_type_ids' in tokenized_outputs:\n",
    "    annotations['token_type_ids'] = tokenized_outputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Extract input_ids and attention_mask as tensors and create new columns\n",
    "annotations['input_ids'] = annotations['tokenized_label_text'].apply(lambda x: x['input_ids'].clone().detach())\n",
    "annotations['attention_mask'] = annotations['tokenized_label_text'].apply(lambda x: x['attention_mask'].clone().detach())\n",
    "annotations['token_type_ids'] = annotations['tokenized_label_text'].apply(lambda x: x.get('token_type_ids', torch.zeros_like(x['input_ids'])).clone().detach())\n",
    "\n",
    "# # Drop the tokenized_label_text column\n",
    "tokenized_annotations = annotations.drop(columns=['label', 'tokenized_label_text'])\n",
    "\n",
    "# Save tokenized_annotations\n",
    "tokenized_annotations.to_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/tokenized_go_annotations_2019_07_01.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/tokenized_go_annotations_2019_07_01.pkl')\n",
    "\n",
    "# Create dictionaries for each column using the DataFrame's index as the key\n",
    "\n",
    "input_ids_dict = df['input_ids'].to_dict()\n",
    "attention_mask_dict = df['attention_mask'].to_dict()\n",
    "token_type_ids_dict = df['token_type_ids'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "print(len(token_type_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,    51,    11,  3428,  2252, 15215,  1958,    43,  5897,  3312,\n",
      "          1941,  6016,  1972,  4914,  3057,  4292,  3308,    18,  4663,    43,\n",
      "         13104,    18, 26334,  1018,    18, 24422,  1011,  1012,    18,     3],\n",
      "        [    2,  2142,  1023,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2,    51, 24184,  2307,  3086,    51,  2112,  2485,  3119,  2176,\n",
      "         13065,    18,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life. What a relief. Wow. Crazy.\",\n",
    "    \"Meh\",\n",
    "    \"I wonder how many I can use before it breaks.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30, 768])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Go through the model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings\n",
    "embeddings = outputs.last_hidden_state[:,0,:]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "# Intiialize a simple ProteinDataset\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.utils.data import read_pickle, get_vocab_mappings, read_json\n",
    "\n",
    "label_vocab = read_json('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json')\n",
    "\n",
    "label2int, int2label = get_vocab_mappings(label_vocab)\n",
    "\n",
    "# Assert label2int values start at 0 and do not skip any integers\n",
    "assert len(label2int) == max(label2int.values()) + 1, \"label2int values must be sequential and start at 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import read_pickle, read_json\n",
    "\n",
    "# Load the label vocabulary and corresponding free-text annotations\n",
    "annotations = read_pickle(\n",
    "    '/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')\n",
    "\n",
    "# Filter the annotations df to be only the labels in label_vocab. In annotations, the go id is the index\n",
    "filtered_annotations = annotations[annotations.index.isin(label_vocab)].copy()\n",
    "\n",
    "\n",
    "# Add a new column 'numeric_id' to the dataframe based on the id_map\n",
    "filtered_annotations.loc[:, 'numeric_id'] = filtered_annotations.index.map(label2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by 'numeric_id'\n",
    "annotations_sorted = filtered_annotations.sort_values(by='numeric_id')\n",
    "\n",
    "# Extract the \"label\" column as a list\n",
    "sorted_labels = annotations_sorted['label'].tolist()\n",
    "\n",
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import load_model_and_tokenizer, tokenize_inputs, get_embeddings_from_tokens\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer(checkpoint, freeze_weights=True)\n",
    "model_inputs = tokenize_inputs(tokenizer, sorted_labels)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 200  # Try increasing the batch size if your GPU has enough memory\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset in batches\n",
    "dataloader = DataLoader(TensorDataset(*model_inputs.values()), batch_size=batch_size)\n",
    "\n",
    "embeddings = get_embeddings_from_tokens(model, dataloader, train_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tis: 0.9465829730033875\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tis: 0.9293946027755737\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 0.973195493221283\n",
      "Similarity between: \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tis: 0.9717705249786377\n",
      "Similarity between: \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 0.9410324692726135\n",
      "Similarity between: \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 0.9525930285453796\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "\n",
    "def test_protein_embeddings():\n",
    "    # Define several strings that describe proteins\n",
    "\n",
    "    protein_descriptions = [\n",
    "        \"This protein is involved in the process of cell signaling.\",\n",
    "        \"A protein that plays a role in cell signaling.\",\n",
    "        \"A protein responsible for muscle contraction.\",\n",
    "        \"This protein aids in muscle contraction.\"\n",
    "    ]\n",
    "\n",
    "    # protein_descriptions = [\n",
    "    #     \"This gene product is crucial for intracellular communication mediated by signaling molecules.\",\n",
    "    #     \"The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\",\n",
    "    #     \"The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\",\n",
    "    #     \"This gene's product is essential for repairing inaccuracies in the DNA helix.\"\n",
    "    # ]\n",
    "\n",
    "    # Load the model and tokenizer\n",
    "    checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\"\n",
    "    tokenizer, model = load_model_and_tokenizer(checkpoint)\n",
    "\n",
    "    # Tokenize the protein descriptions\n",
    "    tokens = tokenize_inputs(tokenizer, protein_descriptions)\n",
    "\n",
    "    # print(tokens)\n",
    "    # print(type(tokens))\n",
    "\n",
    "    # # Get embeddings for the tokenized descriptions\n",
    "    embeddings = get_embeddings_from_tokens(model, tokens)\n",
    "\n",
    "    # Convert the embeddings to numpy for cosine similarity computation\n",
    "    embeddings_matrix = embeddings.cpu().numpy()\n",
    "\n",
    "    # Compute the cosine similarity between the embeddings\n",
    "    cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "    # Print all the cosine similarities and their corresponding protein descriptions\n",
    "    for i in range(len(protein_descriptions)):\n",
    "        for j in range(i + 1, len(protein_descriptions)):\n",
    "            print(\"Similarity between: \\n\\t{} \\n\\t{} \\n\\tis: {}\".format(protein_descriptions[i],\n",
    "                                                                         protein_descriptions[j],\n",
    "                                                                         cosine_sim[i][j]))\n",
    "\n",
    "    # Check if strings that describe similar proteins have a high cosine similarity\n",
    "    # assert cosine_sim[0][1] > 0.8  # Similar descriptions about cell signaling\n",
    "    # assert cosine_sim[2][3] > 0.8  # Similar descriptions about muscle contraction\n",
    "    # assert cosine_sim[0][2] < 0.5  # Different descriptions\n",
    "    # assert cosine_sim[1][3] < 0.5  # Different descriptions\n",
    "\n",
    "test_protein_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
