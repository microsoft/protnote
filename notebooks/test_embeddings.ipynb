{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Logging to console...\n",
      "2023-09-23 18:20:42 PDT INFO Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-23 18:21:22 PDT INFO ################## 2023-09-23_18-20-42 RUNNING train.py ##################\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "import logging\n",
    "from src.utils.data import (\n",
    "    load_model_weights,\n",
    "    seed_everything,\n",
    "    read_pickle,\n",
    ")\n",
    "from src.data.datasets import ProteinDataset, calculate_pos_weight, create_multiple_loaders\n",
    "from src.models.ProTCLTrainer import ProTCLTrainer\n",
    "from src.models.ProTCL import ProTCL\n",
    "from src.models.protein_encoders import ProteInfer\n",
    "from src.utils.evaluation import EvalMetrics, save_evaluation_results\n",
    "from src.utils.models import count_parameters_by_layer, get_label_embeddings\n",
    "from src.utils.configs import get_setup\n",
    "import torch\n",
    "import wandb\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModel, BatchEncoding\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"ROOT_PATH\"] = \"/home/ncorley/protein/ProteinFunctions\"\n",
    "\n",
    "(config, params, paths, paths_list, timestamp, logger, device, ROOT_PATH) = get_setup(\n",
    "    config_path='/home/ncorley/protein/ProteinFunctions/configs/base_config.yaml',\n",
    "    log_to_console=True,\n",
    "    run_name=\"Test\",\n",
    "    overrides=None,\n",
    "    train_path_name=\"TRAIN_DATA_PATH\",\n",
    "    val_path_name=\"VAL_DATA_PATH\",\n",
    "    test_paths_names=[\"TEST_DATA_PATH\"],\n",
    ")\n",
    "\n",
    "# Initialize label tokenizer\n",
    "label_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    params['LABEL_ENCODER_CHECKPOINT'])\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "datasets = ProteinDataset.create_multiple_datasets(paths_list, label_tokenizer=label_tokenizer)\n",
    "\n",
    "# Initialize new run\n",
    "logger.info(\n",
    "    f\"################## {timestamp} RUNNING train.py ##################\")\n",
    "\n",
    "# Define label sample sizes for train, validation, and test loaders\n",
    "label_sample_sizes = {\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"train\": 2000,\n",
    "    # Assuming you have this parameter in your params dictionary\n",
    "    \"validation\": 100,\n",
    "    \"test\": None  # No sampling for the test set\n",
    "}\n",
    "\n",
    "# Define data loaders\n",
    "loaders = create_multiple_loaders(\n",
    "    datasets,\n",
    "    params,\n",
    "    label_sample_sizes=label_sample_sizes,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Perform basic testing on train dataset\n",
    "train_dataset = datasets[\"train\"][0]\n",
    "\n",
    "# Get one item from the dataset\n",
    "sample = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "train_loader = loaders['train'][0]\n",
    "val_loader = loaders['validation'][0]\n",
    "test_loader = loaders['test'][0]\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Get one batch from train loader\n",
    "result = next(iter(train_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence_onehots': tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 1.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 1., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 'sequence_embeddings': None,\n",
       " 'sequence_lengths': tensor([ 36, 311, 200, 126, 190, 109, 102, 385, 142, 188, 215, 301, 353, 348,\n",
       "         320,  67,  84, 251,  93, 324, 843, 527, 298, 201, 404, 424, 115,  60,\n",
       "         205, 106, 780, 711]),\n",
       " 'label_multihots': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]),\n",
       " 'tokenized_labels': {'input_ids': tensor([[    2,  9246, 18418,  ...,     1,     1,     1],\n",
       "         [    2,   480,   639,  ...,     1,     1,     1],\n",
       "         [    2, 16023,   370,  ...,     1,     1,     1],\n",
       "         ...,\n",
       "         [    2,    18,   919,  ...,     1,     1,     1],\n",
       "         [    2,    18,   919,  ...,     1,     1,     1],\n",
       "         [    2,    18,   427,  ...,     1,     1,     1]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]])},\n",
       " 'label_embeddings': None}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\xbc]H\\x15', b'd(C\\xa8E\\x06n\\xec\\xe8/X\\xbf \\xe1f\\x1d\\x8f\\xc9\\xc8\\x07\\x00\\xc05\\xb4iS\\x11\\x02B\\xf9\\\\\\x92\\xabw\\xab\\x90\\xfe\\xc0\\x9dK\\xfc\\xabaO\\xa9\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x00+\\x00\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003']\n",
      "Bad pipe message: %s [b'\\xe6\\x87[\\xb5\\xfe\\xc7\\x0c\\x86\\xc9\\xa9\\xb8\\xe0\\x98\\xaf_\\r\\xe6\\xdf \\xbb%\\xc3\\x03\\xcag\\xce8[4\"\\x83\\xf7\\xaa\\xa6\\xa7\\xd2\\xc6']\n",
      "Bad pipe message: %s [b\"\\xd7)\\x95\\xc7\\xe3jT\\xbe:\\x97nu\\x89\\xacgB\\xc1;\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\"]\n",
      "Bad pipe message: %s [b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xcd\\xe2;\\xd46-\\xf9\\xf4\\xf4\\x97\\xc7\\xb7l[\\x9a\\xb1\\x0f\\xca\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0', b\"/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00\"]\n",
      "Bad pipe message: %s [b'@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t']\n",
      "Bad pipe message: %s [b'7.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00']\n",
      "Bad pipe message: %s [b'\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06\\x01\\x03\\x03\\x02\\x03\\x03\\x01\\x02\\x01\\x03\\x02\\x02\\x02\\x04\\x02\\x05\\x02\\x06\\x02']\n",
      "Bad pipe message: %s [b'\\xe9\\x88\\xe4\\xff\\x05\\xa3\\xfcT^\\xf8\\xbd\\xca\\x9f\\xee\\xae\\x99\\x95\\xe2\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05']\n",
      "Bad pipe message: %s [b'\\xa7\\xde\\x88\\rq<\\x81\\xc6\\xb0q\\xb6\\x1f(\\x13\\xf7u\\xa5\\xc6\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15']\n",
      "Bad pipe message: %s [b'\\xbf~U<\\x08\\x9b\\xaby\\xce\\xed\\x8f\\xa7\\xe3\"x0(\\xdc\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0', b'\\x16\\x00\\x18\\xc0\\x0c\\xc0']\n",
      "Bad pipe message: %s [b'\\x05']\n",
      "Bad pipe message: %s [b'\\x191\\xd8\\x0e\\xf2\\xa4\\x07cM_\\x1a\\x88/\\xde\\x91\\xce\\x11\\x19\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A']\n",
      "Bad pipe message: %s [b\"\\xe3\\xc0\\xf4\\x82C\\xdd[l\\xa1\\x91n\\xe7}H7\\xad\\xc6\\x00\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\", b'\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12']\n",
      "Bad pipe message: %s [b'9\\xe5\\xf1\\x9d\\xd5\\x87\\x90\\xfb2']\n",
      "Bad pipe message: %s [b\"\\xb2\\xfa\\xb1S\\xd8\\xb3J\\x05\\xaf/\\xfe^{{\\xfd\\xfb\\xfd,\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\"]\n",
      "Bad pipe message: %s [b'\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\\t\\x08\\n\\x08']\n",
      "Bad pipe message: %s [b'\\x04\\x08\\x05\\x08\\x06\\x04\\x01\\x05\\x01\\x06']\n",
      "Bad pipe message: %s [b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b\"\\x05\\xf4\\xa9\\x03Q(\\x92\\x89\\x05\\t\\x8bb'\\xa2\\xda\\xee\\xb5\\xcf\\x00\\x00\\xa6\\xc0\"]\n",
      "Bad pipe message: %s [b'0\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e']\n",
      "Bad pipe message: %s [b\"\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\"]\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\x97\\x88hUT\\x0c\\n\\xd6\\xba/Z\\x83V\\x8e\\x0c\\x9c\\xab.\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00\\x06\\x00\\x17\\x00\\x03\\xc0\\x10\\xc0\\x06\\xc0\\x15\\xc0\\x0b\\xc0\\x01\\x00\\x02\\x00\\x01']\n",
      "Bad pipe message: %s [b'3vc\\xb4\\xe4\\xeb(\\x115\\x83\\xca7*\\x871\\x86\\xb0\\xa0\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007']\n",
      "Bad pipe message: %s [b'\\xc8\\xea6\\xee\\xf8y\\x83\\x98[\\x06.a\\x87|\\x16']\n",
      "Bad pipe message: %s [b\"*\\x9f\\x14cC\\xd1\\x98\\xb1\\x17U@a\\x90\\xdcY\\xed\\xf3O\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\"]\n",
      "Bad pipe message: %s [b'g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'\\x03']\n",
      "Bad pipe message: %s [b\"\\xb3Xd\\xe9\\xad\\xd5Inv`c|\\x97+\\xf1M\\x9d\\xf6\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x00\\xa6\\x00l\\x004\\x00\\x9b\\x00F\\xc01\\xc0-\\xc0\"]\n",
      "Bad pipe message: %s [b'%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00']\n",
      "Bad pipe message: %s [b'\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n",
      "torch.Size([2, 20, 315])\n",
      "torch.Size([2, 2000])\n",
      "torch.Size([2])\n",
      "torch.Size([2000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"\\xb7>\\x1d+\\xd0\\x9d\\x80~\\x00\\xff\\x02Un3\\xebd\\xf8\\x94\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t1\"]\n",
      "Bad pipe message: %s [b'.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04', b'\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b\"\\xc1\\xab\\xaad(\\xae6\\xeb\\xd5\\x9c\\xa1\\xfdn\\x13|\\xf7\\xa1\\xd2\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\"]\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'', b'\\x03\\x03']\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'', b'\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\x19\\x82\\xf0\\x82\\xdd\\x82\\xd0\\x9d-\\xcbW\\x91HGo\\xfd\\xa2\\x97\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r']\n",
      "Bad pipe message: %s [b'\\x0c\\xf7\\x1c\\xb2\\x8c\\x7f\\x15\\xc6\\x800\\xa6o\\xado\\xf0\\x03&\\x03\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11']\n",
      "Bad pipe message: %s [b'?\\x83\\xd6_\\x8f\\xbc\\x07\"\\xf2\\'\\x1d\\x9aC>\\xcc\\xc4g\\x8f\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0']\n",
      "Bad pipe message: %s [b'5\\x00\\x84\\xc0']\n",
      "Bad pipe message: %s [b'\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00']\n",
      "Bad pipe message: %s [b'D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00']\n",
      "Bad pipe message: %s [b'U\\x80*\\x9d\\xb4\\xc62\\xd9\\xebC2q\\xc2rY\\xbc\\x08\\x15\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x15\\x03\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'\\xc3\\t\\xf1V+\\xa5-\\xf4\\xb5\\xd8\\x9d\\x8cv\\x8c\\x90\\xb3\\x04\\x9c\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13']\n",
      "Bad pipe message: %s [b\"(|\\xb8\\xfb \\xe8\\xd2\\xc0\\xc64\\r%\\xdc@\\xc4\\xb3\\x1e\\xf7\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\"]\n",
      "Bad pipe message: %s [b\"\\xe9\\xa0\\xa3Xd\\xd8'U\\xfc~yl\\x04P[\\x01\\x882\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\"]\n"
     ]
    }
   ],
   "source": [
    "print(sequence_ids.shape)\n",
    "print(sequence_multihots.shape)\n",
    "print(label_multihots.shape)\n",
    "print(sequence_lengths.shape)\n",
    "print(sampled_label_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from src.utils.data import read_pickle\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/label_embeddings.pk1\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pk1\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, val_dataset, test_dataset = ProteinDataset\\\n",
    "    .create_multiple_datasets(data_paths=[TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH],\n",
    "                              sequence_vocabulary_path=AMINO_ACID_VOCAB_PATH)\n",
    "\n",
    "# Create label voculabary by merging sets from train_loader, val_loader, and test_loader\n",
    "master_label_vocabulary = list(set(train_dataset.label_vocabulary) | set(val_dataset.label_vocabulary) | set(test_dataset.label_vocabulary))\n",
    "\n",
    "# Save master_label_vocabulary to JSON file\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'w') as f:\n",
    "    json.dump(master_label_vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522607\n",
      "['A0A023GPI8', 'A0A023IWD9', 'A0A023IWE0', 'A0A023IWE1', 'A0A023IWE2', 'A0A023IWE3', 'A0A023IWG1', 'A0A023IWG2', 'A0A023IWG3', 'A0A023IWG4']\n",
      "True\n",
      "522607\n",
      "522607\n",
      "439540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "# Import read_pickle\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# Load /home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl\n",
    "sequence_id_map = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl')\n",
    "\n",
    "# Print the shape and column names\n",
    "print(len(sequence_id_map))\n",
    "\n",
    "# Print the first 10 keys\n",
    "print(list(sequence_id_map.keys())[:10])\n",
    "\n",
    "# Check if the keys are unique\n",
    "print(len(sequence_id_map) == len(set(sequence_id_map.keys())))\n",
    "\n",
    "# Load sequence embeddings \n",
    "sequence_embeddings = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_embeddings.pkl')\n",
    "print(len(sequence_embeddings))\n",
    "\n",
    "# Load old embeddings from /home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth with pytorch\n",
    "old_sequence_embeddings = torch.load('/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth')\n",
    "\n",
    "# Print number of old embeddings\n",
    "print(len(old_sequence_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new map from label ID to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000001', 'GO:0000002', 'GO:0000003', 'GO:0000006', 'GO:0000007', 'GO:0000009', 'GO:0000010', 'GO:0000011', 'GO:0000012', 'GO:0000014']\n"
     ]
    }
   ],
   "source": [
    "# Load vocab from /home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json\n",
    "import json\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'r') as f:\n",
    "    label_vocab = json.load(f)\n",
    "\n",
    "# Print the first 10 labels\n",
    "print(label_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from /home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\n",
    "from src.utils.data import read_pickle\n",
    "annotations = read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the annotations df to be only the labels in label_vocab. In annotations, the go id is the index\n",
    "annotations = annotations[annotations.index.isin(label_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT model\n",
    "from src.utils.models import load_PubMedBERT\n",
    "tokenizer, model = load_PubMedBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,  2648,  5316,  2052,  1977,  4663,  2176,  1977,  3189,  1942,\n",
      "          6922,     5,    51,  2832,  4747,  9023,  2004,     5,     3],\n",
      "        [    2, 18208,  1023,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT tokenizer\n",
    "inputs = tokenizer([\"Test text This is what it is like to live! I will run forever!\", \"BLAH\"], return_tensors=\"pt\",\n",
    "                       truncation=True, padding=True, max_length=512)\n",
    "print(inputs)\n",
    "print(type(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations.loc[:, 'tokenized_label_text'] = annotations['label'].apply(lambda x: tokenizer(x, return_tensors=\"pt\", truncation=True, padding=True, max_length=512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'tokenized_label_text', 'input_ids', 'attention_mask',\n",
       "       'token_type_ids'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all labels in the dataframe in a batched manner\n",
    "tokenized_outputs = tokenizer(annotations['label'].tolist(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "275"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the tokenized outputs to the dataframe\n",
    "annotations['input_ids'] = tokenized_outputs['input_ids']\n",
    "annotations['attention_mask'] = tokenized_outputs['attention_mask']\n",
    "if 'token_type_ids' in tokenized_outputs:\n",
    "    annotations['token_type_ids'] = tokenized_outputs['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Extract input_ids and attention_mask as tensors and create new columns\n",
    "annotations['input_ids'] = annotations['tokenized_label_text'].apply(lambda x: x['input_ids'].clone().detach())\n",
    "annotations['attention_mask'] = annotations['tokenized_label_text'].apply(lambda x: x['attention_mask'].clone().detach())\n",
    "annotations['token_type_ids'] = annotations['tokenized_label_text'].apply(lambda x: x.get('token_type_ids', torch.zeros_like(x['input_ids'])).clone().detach())\n",
    "\n",
    "# # Drop the tokenized_label_text column\n",
    "tokenized_annotations = annotations.drop(columns=['label', 'tokenized_label_text'])\n",
    "\n",
    "# Save tokenized_annotations\n",
    "tokenized_annotations.to_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/tokenized_go_annotations_2019_07_01.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the pickled DataFrame\n",
    "df = pd.read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/tokenized_go_annotations_2019_07_01.pkl')\n",
    "\n",
    "# Create dictionaries for each column using the DataFrame's index as the key\n",
    "\n",
    "input_ids_dict = df['input_ids'].to_dict()\n",
    "attention_mask_dict = df['attention_mask'].to_dict()\n",
    "token_type_ids_dict = df['token_type_ids'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32102\n"
     ]
    }
   ],
   "source": [
    "print(len(token_type_ids_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfomers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2,    51,    11,  3428,  2252, 15215,  1958,    43,  5897,  3312,\n",
      "          1941,  6016,  1972,  4914,  3057,  4292,  3308,    18,  4663,    43,\n",
      "         13104,    18, 26334,  1018,    18, 24422,  1011,  1012,    18,     3],\n",
      "        [    2,  2142,  1023,     3,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [    2,    51, 24184,  2307,  3086,    51,  2112,  2485,  3119,  2176,\n",
      "         13065,    18,     3,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transformers.tokenization_utils_base.BatchEncoding"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life. What a relief. Wow. Crazy.\",\n",
    "    \"Meh\",\n",
    "    \"I wonder how many I can use before it breaks.\"\n",
    "]\n",
    "\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "type(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 30, 768])\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Go through the model\n",
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 768])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embeddings\n",
    "embeddings = outputs.last_hidden_state[:,0,:]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "# Intiialize a simple ProteinDataset\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.utils.data import read_pickle, get_vocab_mappings, read_json\n",
    "\n",
    "label_vocab = read_json('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json')\n",
    "\n",
    "label2int, int2label = get_vocab_mappings(label_vocab)\n",
    "\n",
    "# Assert label2int values start at 0 and do not skip any integers\n",
    "assert len(label2int) == max(label2int.values()) + 1, \"label2int values must be sequential and start at 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data import read_pickle, read_json\n",
    "\n",
    "# Load the label vocabulary and corresponding free-text annotations\n",
    "annotations = read_pickle(\n",
    "    '/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')\n",
    "\n",
    "# Filter the annotations df to be only the labels in label_vocab. In annotations, the go id is the index\n",
    "filtered_annotations = annotations[annotations.index.isin(label_vocab)].copy()\n",
    "\n",
    "\n",
    "# Add a new column 'numeric_id' to the dataframe based on the id_map\n",
    "filtered_annotations.loc[:, 'numeric_id'] = filtered_annotations.index.map(label2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dataframe by 'numeric_id'\n",
    "annotations_sorted = filtered_annotations.sort_values(by='numeric_id')\n",
    "\n",
    "# Extract the \"label\" column as a list\n",
    "sorted_labels = annotations_sorted['label'].tolist()\n",
    "\n",
    "checkpoint = \"microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.models import load_model_and_tokenizer, tokenize_inputs, get_cls_embeddings_from_tokenss\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "tokenizer, model = load_model_and_tokenizer(checkpoint, freeze_weights=True)\n",
    "model_inputs = tokenize_inputs(tokenizer, sorted_labels)\n",
    "\n",
    "# Define the batch size\n",
    "batch_size = 200  # Try increasing the batch size if your GPU has enough memory\n",
    "\n",
    "# Create a DataLoader to iterate over the dataset in batches\n",
    "dataloader = DataLoader(TensorDataset(*model_inputs.values()), batch_size=batch_size)\n",
    "\n",
    "embeddings = get_cls_embeddings_from_tokenss(model, dataloader, train_model=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tis: 1.0000001192092896\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tis: 1.0000001192092896\n",
      "Similarity between: \n",
      "\tThis protein is involved in the process of cell signaling. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 1.0000001192092896\n",
      "Similarity between: \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tis: 1.0000001192092896\n",
      "Similarity between: \n",
      "\tA protein that plays a role in cell signaling. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 1.0000001192092896\n",
      "Similarity between: \n",
      "\tA protein responsible for muscle contraction. \n",
      "\tThis protein aids in muscle contraction. \n",
      "\tis: 1.0000001192092896\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from src.utils.models import get_cls_embeddings_from_tokens, load_HF_tokenizer, load_HF_model, tokenize_inputs\n",
    "\n",
    "protein_descriptions = [\n",
    "    \"This protein is involved in the process of cell signaling.\",\n",
    "    \"A protein that plays a role in cell signaling.\",\n",
    "    \"A protein responsible for muscle contraction.\",\n",
    "    \"This protein aids in muscle contraction.\"\n",
    "]\n",
    "\n",
    "# protein_descriptions = [\n",
    "#     \"This gene product is crucial for intracellular communication mediated by signaling molecules.\",\n",
    "#     \"The protein encoded by this gene plays a pivotal role in the transmission of signals within cells.\",\n",
    "#     \"The protein expressed from this locus is involved in the correction of mismatches in DNA sequences.\",\n",
    "#     \"This gene's product is essential for repairing inaccuracies in the DNA helix.\"\n",
    "# ]\n",
    "\n",
    "# Load the tokenizer\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "tokenizer = load_HF_tokenizer(checkpoint)\n",
    "\n",
    "# Load the model\n",
    "model = load_HF_model(checkpoint)\n",
    "\n",
    "# Tokenize the protein descriptions\n",
    "tokens = tokenize_inputs(tokenizer, protein_descriptions)\n",
    "\n",
    "# print(tokens)\n",
    "# print(type(tokens))\n",
    "\n",
    "# # Get embeddings for the tokenized descriptions\n",
    "embeddings = get_cls_embeddings_from_tokens(model, tokens)\n",
    "\n",
    "# Convert the embeddings to numpy for cosine similarity computation\n",
    "embeddings_matrix = embeddings.cpu().numpy()\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Print all the cosine similarities and their corresponding protein descriptions\n",
    "for i in range(len(protein_descriptions)):\n",
    "    for j in range(i + 1, len(protein_descriptions)):\n",
    "        print(\"Similarity between: \\n\\t{} \\n\\t{} \\n\\tis: {}\".format(protein_descriptions[i],\n",
    "                                                                        protein_descriptions[j],\n",
    "                                                                        cosine_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 42384])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BioGptTokenizer, BioGptForCausalLM\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BioGptTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "# Tokenize the input text\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "# Get the model's output\n",
    "output = model(**encoded_input)\n",
    "\n",
    "# Extract the logits (token embeddings) from the output\n",
    "logits = output.logits\n",
    "\n",
    "# Compute the average embedding\n",
    "average_embedding = logits.mean(dim=1)\n",
    "\n",
    "# The result is a tensor of shape (batch_size, hidden_size), where hidden_size is the size of the embeddings (e.g., 768 for base models).\n",
    "print(average_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GO:0000001</th>\n",
       "      <td>The distribution of mitochondria, including th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000002</th>\n",
       "      <td>The maintenance of the structure and integrity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000003</th>\n",
       "      <td>The production of new individuals that contain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000005</th>\n",
       "      <td>OBSOLETE. Assists in the correct assembly of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000006</th>\n",
       "      <td>Enables the transfer of zinc ions (Zn2+) from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000007</th>\n",
       "      <td>Enables the transfer of a solute or solutes fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000008</th>\n",
       "      <td>OBSOLETE. A small disulfide-containing redox p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000009</th>\n",
       "      <td>Catalysis of the transfer of a mannose residue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000010</th>\n",
       "      <td>Catalysis of the reaction: all-trans-hexapreny...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000011</th>\n",
       "      <td>The distribution of vacuoles into daughter cel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000012</th>\n",
       "      <td>The repair of single strand breaks in DNA. Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000014</th>\n",
       "      <td>Catalysis of the hydrolysis of ester linkages ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000015</th>\n",
       "      <td>A multimeric enzyme complex, usually a dimer o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000016</th>\n",
       "      <td>Catalysis of the reaction: lactose + H2O = D-g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000017</th>\n",
       "      <td>The directed movement of alpha-glucosides into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000018</th>\n",
       "      <td>Any process that modulates the frequency, rate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000019</th>\n",
       "      <td>Any process that modulates the frequency, rate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000020</th>\n",
       "      <td>OBSOLETE. Any process that stops, prevents, or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000022</th>\n",
       "      <td>The cell cycle process in which the distance i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GO:0000023</th>\n",
       "      <td>The chemical reactions and pathways involving ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        label\n",
       "GO:0000001  The distribution of mitochondria, including th...\n",
       "GO:0000002  The maintenance of the structure and integrity...\n",
       "GO:0000003  The production of new individuals that contain...\n",
       "GO:0000005  OBSOLETE. Assists in the correct assembly of r...\n",
       "GO:0000006  Enables the transfer of zinc ions (Zn2+) from ...\n",
       "GO:0000007  Enables the transfer of a solute or solutes fr...\n",
       "GO:0000008  OBSOLETE. A small disulfide-containing redox p...\n",
       "GO:0000009  Catalysis of the transfer of a mannose residue...\n",
       "GO:0000010  Catalysis of the reaction: all-trans-hexapreny...\n",
       "GO:0000011  The distribution of vacuoles into daughter cel...\n",
       "GO:0000012  The repair of single strand breaks in DNA. Rep...\n",
       "GO:0000014  Catalysis of the hydrolysis of ester linkages ...\n",
       "GO:0000015  A multimeric enzyme complex, usually a dimer o...\n",
       "GO:0000016  Catalysis of the reaction: lactose + H2O = D-g...\n",
       "GO:0000017  The directed movement of alpha-glucosides into...\n",
       "GO:0000018  Any process that modulates the frequency, rate...\n",
       "GO:0000019  Any process that modulates the frequency, rate...\n",
       "GO:0000020  OBSOLETE. Any process that stops, prevents, or...\n",
       "GO:0000022  The cell cycle process in which the distance i...\n",
       "GO:0000023  The chemical reactions and pathways involving ..."
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load /home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\n",
    "from src.utils.data import read_pickle\n",
    "annotations = read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')\n",
    "annotations.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "torch.Size([4, 1024])\n",
      "Similarity between: \n",
      "\tCatalysis of the reaction: lactose + H2O = D-glucose + D-galactose \n",
      "\tLactose galactohydrolase activity \n",
      "\tis: 0.622012197971344\n",
      "Similarity between: \n",
      "\tCatalysis of the reaction: lactose + H2O = D-glucose + D-galactose \n",
      "\tThe chemical reactions and pathways resulting in the formation of L-glutamate, the L enantiomer anion of 2-aminopentanedioic acid \n",
      "\tis: 0.5596150755882263\n",
      "Similarity between: \n",
      "\tCatalysis of the reaction: lactose + H2O = D-glucose + D-galactose \n",
      "\tL-glutamate anabolism \n",
      "\tis: 0.6240302920341492\n",
      "Similarity between: \n",
      "\tLactose galactohydrolase activity \n",
      "\tThe chemical reactions and pathways resulting in the formation of L-glutamate, the L enantiomer anion of 2-aminopentanedioic acid \n",
      "\tis: 0.2663598656654358\n",
      "Similarity between: \n",
      "\tLactose galactohydrolase activity \n",
      "\tL-glutamate anabolism \n",
      "\tis: 0.7775448560714722\n",
      "Similarity between: \n",
      "\tThe chemical reactions and pathways resulting in the formation of L-glutamate, the L enantiomer anion of 2-aminopentanedioic acid \n",
      "\tL-glutamate anabolism \n",
      "\tis: 0.3664991557598114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'\\x16\\x9c\\rN\\xf5`\\x18f\\xf4Ji\\x15l`\\xc2]\\xc4\\xf2 K\\xd6\\xd4\\x88\\xfeb\\x03\\x81\\xc6D\\xa6\\xbf_)}d\\xbaR|J\\xdc\\xf5 T\\xeb\\xcd\\x11y\\x1b\\x11k\\xc5\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0']\n",
      "Bad pipe message: %s [b'.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x00\\x1e\\x00\\x1c\\x04']\n",
      "Bad pipe message: %s [b'\\x03\\x06', b'\\x07\\x08']\n",
      "Bad pipe message: %s [b'\\t\\x08\\n\\x08\\x0b\\x08\\x04']\n",
      "Bad pipe message: %s [b'\\x08\\x06\\x04\\x01\\x05\\x01\\x06', b'']\n",
      "Bad pipe message: %s [b\"\\x03\\x02\\x03\\x04\\x00-\\x00\\x02\\x01\\x01\\x003\\x00&\\x00$\\x00\\x1d\\x00 d\\xbe'n\\xdd\\xef\\xbc\\xb9Z\\x1b\\xb9A\\x81\\xde\\xae\\xf9\\x9f\\xe7\\xe4i\\xfd\\xb1\"]\n",
      "Bad pipe message: %s [b'\\xff\\x9ae!h\\x90J+\\x9f\\x83\\x15W\\xbd\\xb6L\\xacR=\\x00\\x00', b\",\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\"]\n",
      "Bad pipe message: %s [b'']\n",
      "Bad pipe message: %s [b'M\\x11$\\xa6\\x92\\xc1C\\x00\\xc1\\xcd\\x9e\\xdb\\xf3\\x90\\xb0Q\\x88\\xde\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S']\n",
      "Bad pipe message: %s [b'\\x93\\xed\\x97\\xca\\xc6\\xbcM\\xa6\\xd3\\xaew\\xc8^\\xf5u\\xb4\\xe9[\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0', b'\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00']\n",
      "Bad pipe message: %s [b'D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x1a\\x00\\t\\x00\\x14\\x00\\x11\\x00\\x19\\x00\\x08\\x00']\n",
      "Bad pipe message: %s [b'\\x17\\x00\\x03\\xc0\\x10']\n",
      "Bad pipe message: %s [b'g\\x1bZ\\x11\\xd9\\x9f\\xa6\\xfa\\x06BlA\\x01:\\xd1\\xc9\\xdc\\xb8\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b']\n",
      "Bad pipe message: %s [b'\\x19\\x08\\xde\\xce\\xd6_\\xf1\\x8fB\\xcb\\xf5\\x1a\\xa6\\xa3\\x92g\\x18\\x8a\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0', b'\\x0c\\xc0\\x02\\x00\\x05\\x00']\n",
      "Bad pipe message: %s [b'\\xff\\x02\\x01']\n",
      "Bad pipe message: %s [b'S\\xd6\\xc7\\x99\\xadO\\x0c\\xaeN\\x1bDLf[\\xb4\\x83\\x88\\xc9\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12\\x00\\x0f\\x00\\x0c\\x00\\x1a\\x00\\t\\x00\\x14']\n",
      "Bad pipe message: %s [b\"\\xbf?dqM\\x03\\xe7h\\xca)\\xaa\\xac\\x17\\x1c\\xd4\\xb2q\\xfe\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\"]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import torch \n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.models import get_aggregated_embeddings_from_tokens\n",
    "\n",
    "protein_descriptions = [\n",
    "    \"Catalysis of the reaction: lactose + H2O = D-glucose + D-galactose\",\n",
    "    \"Lactose galactohydrolase activity\",\n",
    "    \"The chemical reactions and pathways resulting in the formation of L-glutamate, the L enantiomer anion of 2-aminopentanedioic acid\",\n",
    "    \"L-glutamate anabolism\",\n",
    "]\n",
    "\n",
    "# Load the tokenizer\n",
    "checkpoint = \"microsoft/biogpt\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Load the model\n",
    "model = AutoModel.from_pretrained(checkpoint)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Tokenize the protein descriptions\n",
    "    tokens = tokenizer(protein_descriptions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**tokens)\n",
    "\n",
    "    # Get embeddings for the tokenized descriptions\n",
    "    embeddings = outputs['last_hidden_state']\n",
    "\n",
    "    # Take the last token\n",
    "    averaged_embeddings = embeddings.mean(dim=1)\n",
    "\n",
    "    print(averaged_embeddings.shape)  # This should print torch.Size([4, 1024])\n",
    "\n",
    "    # Convert the averaged embeddings to numpy for cosine similarity computation\n",
    "    embeddings_matrix = averaged_embeddings.cpu().numpy()\n",
    "\n",
    "# Compute the cosine similarity between the embeddings\n",
    "cosine_sim = cosine_similarity(embeddings_matrix)\n",
    "\n",
    "# Print all the cosine similarities and their corresponding protein descriptions\n",
    "for i in range(len(protein_descriptions)):\n",
    "    for j in range(i + 1, len(protein_descriptions)):\n",
    "        print(\"Similarity between: \\n\\t{} \\n\\t{} \\n\\tis: {}\".format(protein_descriptions[i],\n",
    "                                                                        protein_descriptions[j],\n",
    "                                                                        cosine_sim[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BioGptModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "model = BioGptModel.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-22.7061, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states.shape\n",
    "# Compute the average embedding\n",
    "average_embedding = last_hidden_states.mean(dim=1)\n",
    "average_embedding.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-22.7061, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_embedding.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"microsoft/biogpt\")\n",
    "\n",
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations = read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>numeric_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The distribution of mitochondria, including th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The maintenance of the structure and integrity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The production of new individuals that contain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OBSOLETE. Assists in the correct assembly of r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Enables the transfer of zinc ions (Zn2+) from ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        label\n",
       "numeric_id                                                   \n",
       "1           The distribution of mitochondria, including th...\n",
       "2           The maintenance of the structure and integrity...\n",
       "3           The production of new individuals that contain...\n",
       "5           OBSOLETE. Assists in the correct assembly of r...\n",
       "6           Enables the transfer of zinc ions (Zn2+) from ..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of the first 1000 labels and their corresponding tokenized outputs\n",
    "labels = annotations['label'].tolist()[:100]\n",
    "tokenized_outputs = tokenizer(labels, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Now compute the embeddings for the tokenized outputs\n",
    "with torch.set_grad_enabled(True), autocast():\n",
    "    outputs = model(**tokenized_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(1), tensor([0, 1, 2]), tensor(2)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define the label_multihots tensor\n",
    "label_multihots = torch.tensor([[0, 1, 0],\n",
    "                                [1, 1, 1],\n",
    "                                [0, 0, 1]])\n",
    "\n",
    "label_indices_list = [torch.nonzero(row).squeeze() for row in label_multihots]\n",
    "print(label_indices_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2]])\n",
      "tensor(2)\n"
     ]
    }
   ],
   "source": [
    "print(torch.nonzero(label_multihots[2]))\n",
    "print(torch.nonzero(label_multihots[2]).squeeze())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
