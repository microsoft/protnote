{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from src.utils.data import read_pickle\n",
    "from src.utils.losses import contrastive_loss\n",
    "from src.data.collators import collate_variable_sequence_length\n",
    "from src.data.datasets import ProteinDataset\n",
    "from src.models.ProTCL import ProTCL\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "\n",
    "# Data paths\n",
    "TRAIN_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/train_GO.fasta'\n",
    "VAL_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/dev_GO.fasta'\n",
    "TEST_DATA_PATH = '/home/ncorley/protein/ProteinFunctions/data/swissprot/proteinfer_splits/random/test_GO.fasta'\n",
    "AMINO_ACID_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/amino_acid_vocab.json'\n",
    "GO_LABEL_VOCAB_PATH = '/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json'\n",
    "\n",
    "# Embedding paths\n",
    "LABEL_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/label_embeddings.pk1\"\n",
    "SEQUENCE_EMBEDDING_PATH = \"/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pk1\"\n",
    "\n",
    "# Load datasets\n",
    "train_dataset, val_dataset, test_dataset = ProteinDataset\\\n",
    "    .create_multiple_datasets(data_paths=[TRAIN_DATA_PATH, VAL_DATA_PATH, TEST_DATA_PATH],\n",
    "                              sequence_vocabulary_path=AMINO_ACID_VOCAB_PATH)\n",
    "\n",
    "# Create label voculabary by merging sets from train_loader, val_loader, and test_loader\n",
    "master_label_vocabulary = list(set(train_dataset.label_vocabulary) | set(val_dataset.label_vocabulary) | set(test_dataset.label_vocabulary))\n",
    "\n",
    "# Save master_label_vocabulary to JSON file\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'w') as f:\n",
    "    json.dump(master_label_vocabulary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "522607\n",
      "['A0A023GPI8', 'A0A023IWD9', 'A0A023IWE0', 'A0A023IWE1', 'A0A023IWE2', 'A0A023IWE3', 'A0A023IWG1', 'A0A023IWG2', 'A0A023IWG3', 'A0A023IWG4']\n",
      "True\n",
      "522607\n",
      "522607\n",
      "439540\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "# Import read_pickle\n",
    "from src.utils.data import read_pickle\n",
    "\n",
    "# Load /home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl\n",
    "sequence_id_map = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_id_map.pkl')\n",
    "\n",
    "# Print the shape and column names\n",
    "print(len(sequence_id_map))\n",
    "\n",
    "# Print the first 10 keys\n",
    "print(list(sequence_id_map.keys())[:10])\n",
    "\n",
    "# Check if the keys are unique\n",
    "print(len(sequence_id_map) == len(set(sequence_id_map.keys())))\n",
    "\n",
    "# Load sequence embeddings \n",
    "sequence_embeddings = read_pickle('/home/ncorley/protein/ProteinFunctions/data/embeddings/proteinfer_sequence_embeddings.pkl')\n",
    "print(len(sequence_embeddings))\n",
    "\n",
    "# Load old embeddings from /home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth with pytorch\n",
    "old_sequence_embeddings = torch.load('/home/ncorley/protein/ProteinFunctions/data/embeddings/sequence_embeddings.pth')\n",
    "\n",
    "# Print number of old embeddings\n",
    "print(len(old_sequence_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GO:0000001', 'GO:0000002', 'GO:0000003', 'GO:0000006', 'GO:0000007', 'GO:0000009', 'GO:0000010', 'GO:0000011', 'GO:0000012', 'GO:0000014']\n"
     ]
    }
   ],
   "source": [
    "# Load vocab from /home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json\n",
    "import json\n",
    "with open('/home/ncorley/protein/ProteinFunctions/data/vocabularies/GO_label_vocab.json', 'r') as f:\n",
    "    label_vocab = json.load(f)\n",
    "\n",
    "# Print the first 10 labels\n",
    "print(label_vocab[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from /home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl\n",
    "from src.utils.data import read_pickle\n",
    "annotations = read_pickle('/home/ncorley/protein/ProteinFunctions/data/annotations/go_annotations_2019_07_01.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "len(annotations)\n",
    "annotations.shape\n",
    "annotations.columns\n",
    "\n",
    "# Check if index is unique\n",
    "print(len(annotations) == len(set(annotations.index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# check for duplicates in label_vocab\n",
    "print(len(label_vocab) == len(set(label_vocab)))\n",
    "\n",
    "# Convert to set\n",
    "label_vocab = set(label_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label'], dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the annotations df to be only the labels in label_vocab. In annotations, the go id is the index\n",
    "annotations = annotations[annotations.index.isin(label_vocab)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(len(annotations) == len(label_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load PubMedBERT model\n",
    "from src.utils.models import load_PubMedBERT\n",
    "tokenizer, model = load_PubMedBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PubMedBERT tokenizer\n",
    "inputs = tokenizer([\"Test text This is what it is like to live! I will run forever!\", \"BLAH\"], return_tensors=\"pt\",\n",
    "                       truncation=True, padding=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label'], dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    }
   ],
   "source": [
    "# Create a new column in annotations that is the tokenized label text\n",
    "annotations['tokenized_label_text'] = annotations['label'].apply(lambda x: tokenizer(x, return_tensors=\"pt\",\n",
    "                          truncation=True, padding=True, max_length=512))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
