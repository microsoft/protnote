{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz'\n",
    "filename = 'uniprot_sprot.dat.gz'\n",
    "unzipped_filename = 'uniprot_sprot.dat'\n",
    "\n",
    "# Download the file from the web\n",
    "wget.download(link, filename)\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(unzipped_filename, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "print(f\"File {filename} has been downloaded and unzipped to {unzipped_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SwissProt\n",
    "\n",
    "# Extract data from SwissProt records\n",
    "data = []\n",
    "# See https://biopython.org/docs/1.75/api/Bio.SwissProt.html and https://web.expasy.org/docs/userman.html\n",
    "with open('../data/swissprot/uniprot_sprot.dat', 'r') as f:\n",
    "    records = SwissProt.parse(f)\n",
    "    for record in records:\n",
    "        # Extract sequence ID\n",
    "        seq_id = record.accessions[0]\n",
    "        \n",
    "        # Extract sequence\n",
    "        sequence = record.sequence\n",
    "\n",
    "        # Extract GO ids\n",
    "        go_ids = [ref[1] for ref in record.cross_references if ref[0] == \"GO\" and len(ref) > 0]\n",
    "        \n",
    "        # Extract free-text description\n",
    "        description = record.description\n",
    "\n",
    "        # Extract organism and organism classification\n",
    "        organism = record.organism\n",
    "        organism_classification = record.organism_classification\n",
    "\n",
    "        # Extract organelle\n",
    "        organelle = record.organelle\n",
    "        \n",
    "        # Extract CC line as a dictionary\n",
    "        cc = {}\n",
    "        for comment in record.comments:\n",
    "            key, value = comment.split(\": \", 1)\n",
    "            cc[key] = value\n",
    "        \n",
    "        data.append([seq_id, sequence, go_ids, description, organism, organism_classification, organelle, cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into a pandas DataFrame\n",
    "df_2023 = pd.DataFrame(data, columns=[\"seq_id\", \"sequence\", \"go_ids\", \"description\", \"organism\", \"organism_classification\", \"organelle\", \"cc\"])\n",
    "\n",
    "# Create a new column with the subcellular location\n",
    "df_2023['subcellular_location'] = df_2023.cc.apply(lambda x: x['SUBCELLULAR LOCATION'] if 'SUBCELLULAR LOCATION' in x else None)\n",
    "\n",
    "# import sequence embeddings from ../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the sequence embeddings from the file\n",
    "with open('../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl', 'rb') as f:\n",
    "    sequence_embeddings = pickle.load(f)\n",
    "\n",
    "# Make a set of the sequence strings\n",
    "sequence_strings_2019 = set(sequence_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Q6GZX4\n",
      "1    Q6GZX3\n",
      "2    Q197F8\n",
      "3    Q197F7\n",
      "4    Q6GZX2\n",
      "Name: seq_id, dtype: object\n",
      "['Q8SS29', 'A4QKE2', 'B0BVP3', 'Q55724', 'Q7W2N9']\n",
      "Number of sequences in df_2023 but not in ProteInfer dataset: 47493\n",
      "Number of sequences in df_2023: 569793\n",
      "Number of sequences in ProteInfer dataset: 522607\n"
     ]
    }
   ],
   "source": [
    "# Find sequence ids  that are in df but not in sequence_strings\n",
    "df_2023['in_ProteInfer_dataset'] = df_2023.seq_id.apply(lambda x: x in sequence_strings_2019)\n",
    "\n",
    "# Print 5 example sequences from df.sequence\n",
    "print(df_2023.seq_id.head())\n",
    "\n",
    "# Print 5 example sequences from sequence_strings\n",
    "print(list(sequence_strings_2019)[:5])\n",
    "\n",
    "# Count the number of sequences that are in df but not in sequence_strings\n",
    "print(f\"Number of sequences in df_2023 but not in ProteInfer dataset: {df_2023.in_ProteInfer_dataset.value_counts()[False]}\")\n",
    "print(f\"Number of sequences in df_2023: {len(df_2023)}\")\n",
    "print(f\"Number of sequences in ProteInfer dataset: {len(sequence_strings_2019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47401\n",
      "29283\n"
     ]
    }
   ],
   "source": [
    "# Import label embeddings from ../data/embeddings/frozen_proteinfer_label_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the label embeddings from the file\n",
    "with open('../data/embeddings/frozen_PubMedBERT_label_embeddings.pkl', 'rb') as f:\n",
    "    label_embeddings_2019 = pickle.load(f)\n",
    "\n",
    "# Make a set of the GO labels from the label embeddings\n",
    "label_ids_2019 = set(label_embeddings_2019.keys())\n",
    "print(len(label_ids_2019))\n",
    "\n",
    "# Make a set from all the GO labels that occur in the data\n",
    "label_ids_2023 = set([item for sublist in df_2023.go_ids for item in sublist])\n",
    "print(len(label_ids_2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GO labels in go_label_strings but not in label_strings: 666\n",
      "['GO:0140752', 'GO:0140499', 'GO:0140947', 'GO:0140961', 'GO:0140831', 'GO:0106223', 'GO:0140455', 'GO:0106370', 'GO:0120216', 'GO:0120283']\n"
     ]
    }
   ],
   "source": [
    "# Find GO labels that are in go_label_strings but not in label_strings\n",
    "print(f\"Number of GO labels in go_label_strings but not in label_strings: {len(label_ids_2023 - label_ids_2019)}\")\n",
    "\n",
    "# Print out 10 examples of GO labels that are in go_label_strings but not in label_strings\n",
    "print(list(label_ids_2023 - label_ids_2019)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'in_ProteInfer_dataset' == False: 47493\n",
      "Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): 917\n"
     ]
    }
   ],
   "source": [
    "# Find added labels\n",
    "new_go_labels = label_ids_2023 - label_ids_2019\n",
    "\n",
    "# Find protein sequences with added labels\n",
    "df_2023['new_labels'] = df_2023.go_ids.apply(lambda x: set(x) & new_go_labels)\n",
    "\n",
    "# Count how many rows have 'in_Proteinfer_dataset' == False\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False: {len(df_2023[df_2023.in_ProteInfer_dataset == False])}\")\n",
    "\n",
    "# Count how many rows have 'in_Proteinfer_dataset' == False and 'new_labels' != set()\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): {len(df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())])}\")\n",
    "\n",
    "# Create a new dataframe out of those that meet that criteria\n",
    "df_2023_new_sequences_and_labels = df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1345222/3313185098.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['non_common_amino_acids'] = filtered_df.sequence.apply(lambda x: set(x) - common_amino_acids)\n",
      "/tmp/ipykernel_1345222/3313185098.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SwissProt_2023_unseen_sequences_and_labels.rename(columns={'new_labels': 'go_ids'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe containiners seq_id, sequence, and go_ids\n",
    "filtered_df = df_2023_new_sequences_and_labels[['seq_id', 'sequence', 'new_labels']]\n",
    "\n",
    "# Set of 20 common amino acids\n",
    "common_amino_acids = set(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "# Check which rows contain amino acids other than the 20 common ones\n",
    "filtered_df['non_common_amino_acids'] = filtered_df.sequence.apply(lambda x: set(x) - common_amino_acids)\n",
    "\n",
    "# Filter to only contain rows that contain non-common amino acids\n",
    "SwissProt_2023_unseen_sequences_and_labels = filtered_df[filtered_df.non_common_amino_acids == set()]\n",
    "\n",
    "# Rename \"new_ids\" to \"go_ids\"\n",
    "SwissProt_2023_unseen_sequences_and_labels.rename(columns={'new_labels': 'go_ids'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a pickle file\n",
    "SwissProt_2023_unseen_sequences_and_labels.to_pickle('../data/zero_shot/SwissProt_2023_unseen_sequences_and_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "df = SwissProt_2023_unseen_sequences_and_labels\n",
    "\n",
    "# Convert dataframe to FASTA format and save to a file\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    seq_record = SeqRecord(Seq(row['sequence']),\n",
    "                           id=row['seq_id'],\n",
    "                           description=\" \".join(row['go_ids']))\n",
    "    records.append(seq_record)\n",
    "\n",
    "# Save to FASTA file\n",
    "fasta_file = \"../data/zero_shot/SwissProt_2023_unseen_sequences_and_labels.fasta\"\n",
    "SeqIO.write(records, fasta_file, \"fasta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import ProteinDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Paths for the dataset\n",
    "paths = {\n",
    "    \"data_path\": fasta_file,\n",
    "}\n",
    "\n",
    "# Create the dataset\n",
    "protein_dataset = ProteinDataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ID 1:  A0A443HJY8\n",
      "Sequence IDs: tensor([89])\n",
      "Sequence onehots: tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Label multihots: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Sequence lengths: tensor([433])\n"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import collate_variable_sequence_length\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 1  # You can adjust this value as needed\n",
    "protein_dataloader = DataLoader(protein_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_variable_sequence_length)\n",
    "\n",
    "# Now you can iterate over the DataLoader to get batches of data\n",
    "for batch in protein_dataloader:\n",
    "    # Unpack the batch\n",
    "    sequence_ids, sequence_onehots, label_multihots, sequence_lengths = batch\n",
    "    print(\"Original ID 1: \", protein_dataset.int2sequence_id[sequence_ids[0].item()])\n",
    "    print(f\"Sequence IDs: {sequence_ids}\")\n",
    "    print(f\"Sequence onehots: {sequence_onehots}\")\n",
    "    print(f\"Label multihots: {label_multihots}\")\n",
    "    print(f\"Sequence lengths: {sequence_lengths}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
