{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz'\n",
    "filename = 'uniprot_sprot.dat.gz'\n",
    "unzipped_filename = 'uniprot_sprot.dat'\n",
    "\n",
    "# Download the file from the web\n",
    "wget.download(link, filename)\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(unzipped_filename, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "print(f\"File {filename} has been downloaded and unzipped to {unzipped_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b'l\\xa5L\\xb7\\xef\\xae\\x18\\x07pJ\\xf0\\x87\\xbe\\xa0\\xb6\\r', b' ~q\\xe3*\\xc8\\n\\xb3v\\xa5&']\n",
      "Bad pipe message: %s [b'~\\xa8\\xe3\\xc2\\x81\\x1a\\xed>Ne\\x91n\\xfb\\x10F\\xc9\\x1c\\x1b \\xf0}\\xae\\xc2\\x98\\xe1U\\xe1\\xf2\\xcak\\xd9\\x1b\\xf0a\\xc8\\x91\\x88\\xbb\\x199\\x0e\\xf4\\xf7\\xd4o-\\x1cE\\xf6\\r\\xec\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\\x00\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x0c\\x00\\x00\\t127.0.0.1']\n",
      "Bad pipe message: %s [b'3)\\xb4\\x7f\\xe7\\x8f\\xb3S\\xb3\\xe9(\\xf2;\\x9d\\xf0\\xc0', b\"\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\\xc0'\\x00g\\x00@\\xc0\\n\\xc0\\x14\\x009\\x008\\xc0\\t\\xc0\\x13\\x003\\x00\", b'\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00<\\x005\\x00/\\x00\\x9a\\x00\\x99\\xc0\\x07\\xc0\\x11\\x00\\x96\\x00\\x05\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\x9c\\xbb\\xfc\\xae\\xb9\\x02\\xde\\xe1\\xf7-\\x1d\\x04l\\xb6@\\xc4\\x96m\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k']\n",
      "Bad pipe message: %s [b'\\x1e/\\xd9\\x1b\\xd4\\xe3\\xd1\\xfd\\xe3\\xbf=\\t l[W>[\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x96\\x00A\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x16\\x00\\x18\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\xc0\\x12\\xc0\\x08\\x00\\x16\\x00\\x13\\x00\\x10\\x00\\r\\xc0\\x17\\x00\\x1b\\xc0\\r\\xc0\\x03\\x00\\n\\x00\\x15\\x00\\x12']\n",
      "Bad pipe message: %s [b\"\\xfawr\\xe0\\xa3\\x01\\xbe\\x932\\x1d\\x12Y\\xe2F.\\xfdW\\xb4\\x00\\x00\\x86\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\\xc01\\xc0-\\xc0)\\xc0%\\xc0\\x0e\\xc0\\x04\\x00\\x9c\\x00<\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00g\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t\"]\n",
      "Bad pipe message: %s [b'7.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00']\n",
      "Bad pipe message: %s [b'\\x00\\x00\\r\\x00 \\x00\\x1e\\x06\\x01\\x06\\x02\\x06\\x03\\x05\\x01\\x05\\x02\\x05\\x03\\x04\\x01\\x04\\x02\\x04\\x03\\x03\\x01\\x03\\x02\\x03\\x03\\x02\\x01\\x02']\n",
      "Bad pipe message: %s [b'\\x03']\n",
      "Bad pipe message: %s [b\"\\x0b#\\xd6\\xcd\\x06I+J(\\x04\\xeeq6\\x7f\\xe4\\xd4\\xae\\xf4\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k\\x00j\\x00i\\x00h\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00\\xa7\\x00m\\x00:\\x00\\x89\\xc02\\xc0.\\xc0*\\xc0&\\xc0\\x0f\\xc0\\x05\\x00\\x9d\\x00=\\x005\\x00\\x84\\xc0/\\xc0+\\xc0'\\xc0#\\xc0\\x13\\xc0\\t\\x00\\xa4\\x00\\xa2\\x00\\xa0\\x00\\x9e\\x00g\\x00@\\x00?\\x00>\\x003\\x002\\x001\\x000\"]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from Bio import SwissProt\n",
    "\n",
    "# Extract data from SwissProt records\n",
    "data = []\n",
    "# See https://biopython.org/docs/1.75/api/Bio.SwissProt.html and https://web.expasy.org/docs/userman.html\n",
    "with open('../data/swissprot/uniprot_sprot.dat', 'r') as f:\n",
    "    records = SwissProt.parse(f)\n",
    "    for record in records:\n",
    "        # Extract sequence ID\n",
    "        seq_id = record.accessions[0]\n",
    "        \n",
    "        # Extract sequence\n",
    "        sequence = record.sequence\n",
    "\n",
    "        # Extract GO ids\n",
    "        go_ids = [ref[1] for ref in record.cross_references if ref[0] == \"GO\" and len(ref) > 0]\n",
    "        \n",
    "        # Extract free-text description\n",
    "        description = record.description\n",
    "\n",
    "        # Extract organism and organism classification\n",
    "        organism = record.organism\n",
    "        organism_classification = record.organism_classification\n",
    "\n",
    "        # Extract organelle\n",
    "        organelle = record.organelle\n",
    "        \n",
    "        # Extract CC line as a dictionary\n",
    "        cc = {}\n",
    "        for comment in record.comments:\n",
    "            key, value = comment.split(\": \", 1)\n",
    "            cc[key] = value\n",
    "        \n",
    "        data.append([seq_id, sequence, go_ids, description, organism, organism_classification, organelle, cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q6GZX4', 'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL', ['GO:0046782'], 'RecName: Full=Putative transcription factor 001R;', 'Frog virus 3 (isolate Goorha) (FV-3).', ['Viruses', 'Varidnaviria', 'Bamfordvirae', 'Nucleocytoviricota', 'Megaviricetes', 'Pimascovirales', 'Iridoviridae', 'Alphairidovirinae', 'Ranavirus'], '', {'FUNCTION': 'Transcription activation. {ECO:0000305}.'}]\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into a pandas DataFrame\n",
    "df_2023 = pd.DataFrame(data, columns=[\"seq_id\", \"sequence\", \"go_ids\", \"description\", \"organism\", \"organism_classification\", \"organelle\", \"cc\"])\n",
    "\n",
    "# Create a new column with the subcellular location\n",
    "df_2023['subcellular_location'] = df_2023.cc.apply(lambda x: x['SUBCELLULAR LOCATION'] if 'SUBCELLULAR LOCATION' in x else None)\n",
    "\n",
    "# import sequence embeddings from ../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the sequence embeddings from the file\n",
    "with open('../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl', 'rb') as f:\n",
    "    sequence_embeddings = pickle.load(f)\n",
    "\n",
    "# Make a set of the sequence strings\n",
    "sequence_strings_2019 = set(sequence_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Q6GZX4\n",
      "1    Q6GZX3\n",
      "2    Q197F8\n",
      "3    Q197F7\n",
      "4    Q6GZX2\n",
      "Name: seq_id, dtype: object\n",
      "['Q8SS29', 'A4QKE2', 'B0BVP3', 'Q55724', 'Q7W2N9']\n",
      "Number of sequences in df_2023 but not in ProteInfer dataset: 47493\n",
      "Number of sequences in df_2023: 569793\n",
      "Number of sequences in ProteInfer dataset: 522607\n"
     ]
    }
   ],
   "source": [
    "# Find sequence ids  that are in df but not in sequence_strings\n",
    "df_2023['in_ProteInfer_dataset'] = df_2023.seq_id.apply(lambda x: x in sequence_strings_2019)\n",
    "\n",
    "# Print 5 example sequences from df.sequence\n",
    "print(df_2023.seq_id.head())\n",
    "\n",
    "# Print 5 example sequences from sequence_strings\n",
    "print(list(sequence_strings_2019)[:5])\n",
    "\n",
    "# Count the number of sequences that are in df but not in sequence_strings\n",
    "print(f\"Number of sequences in df_2023 but not in ProteInfer dataset: {df_2023.in_ProteInfer_dataset.value_counts()[False]}\")\n",
    "print(f\"Number of sequences in df_2023: {len(df_2023)}\")\n",
    "print(f\"Number of sequences in ProteInfer dataset: {len(sequence_strings_2019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47401\n",
      "29283\n"
     ]
    }
   ],
   "source": [
    "# Import label embeddings from ../data/embeddings/frozen_proteinfer_label_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the label embeddings from the file\n",
    "with open('../data/embeddings/frozen_PubMedBERT_label_embeddings.pkl', 'rb') as f:\n",
    "    label_embeddings_2019 = pickle.load(f)\n",
    "\n",
    "# Make a set of the GO labels from the label embeddings\n",
    "label_ids_2019 = set(label_embeddings_2019.keys())\n",
    "print(len(label_ids_2019))\n",
    "\n",
    "# Make a set from all the GO labels that occur in the data\n",
    "label_ids_2023 = set([item for sublist in df_2023.go_ids for item in sublist])\n",
    "print(len(label_ids_2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GO labels in go_label_strings but not in label_strings: 666\n",
      "['GO:0140752', 'GO:0140499', 'GO:0140947', 'GO:0140961', 'GO:0140831', 'GO:0106223', 'GO:0140455', 'GO:0106370', 'GO:0120216', 'GO:0120283']\n"
     ]
    }
   ],
   "source": [
    "# Find GO labels that are in go_label_strings but not in label_strings\n",
    "print(f\"Number of GO labels in go_label_strings but not in label_strings: {len(label_ids_2023 - label_ids_2019)}\")\n",
    "\n",
    "# Print out 10 examples of GO labels that are in go_label_strings but not in label_strings\n",
    "print(list(label_ids_2023 - label_ids_2019)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'in_ProteInfer_dataset' == False: 47493\n",
      "Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): 917\n"
     ]
    }
   ],
   "source": [
    "# Find added labels\n",
    "new_go_labels = label_ids_2023 - label_ids_2019\n",
    "\n",
    "# Find protein sequences with added labels\n",
    "df_2023['new_labels'] = df_2023.go_ids.apply(lambda x: set(x) & new_go_labels)\n",
    "\n",
    "# Count how many rows have 'in_Proteinfer_dataset' == False\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False: {len(df_2023[df_2023.in_ProteInfer_dataset == False])}\")\n",
    "\n",
    "# Count how many rows have 'in_Proteinfer_dataset' == False and 'new_labels' != set()\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): {len(df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())])}\")\n",
    "\n",
    "# Create a new dataframe out of those that meet that criteria\n",
    "df_2023_new_sequences_and_labels = df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1345222/3313185098.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['non_common_amino_acids'] = filtered_df.sequence.apply(lambda x: set(x) - common_amino_acids)\n",
      "/tmp/ipykernel_1345222/3313185098.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SwissProt_2023_unseen_sequences_and_labels.rename(columns={'new_labels': 'go_ids'}, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe containiners seq_id, sequence, and go_ids\n",
    "filtered_df = df_2023_new_sequences_and_labels[['seq_id', 'sequence', 'new_labels']]\n",
    "\n",
    "# Set of 20 common amino acids\n",
    "common_amino_acids = set(['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y'])\n",
    "\n",
    "# Check which rows contain amino acids other than the 20 common ones\n",
    "filtered_df['non_common_amino_acids'] = filtered_df.sequence.apply(lambda x: set(x) - common_amino_acids)\n",
    "\n",
    "# Filter to only contain rows that contain common amino acids\n",
    "SwissProt_2023_unseen_sequences_and_labels = filtered_df[filtered_df.non_common_amino_acids == set()]\n",
    "\n",
    "# Rename \"new_ids\" to \"go_ids\"\n",
    "SwissProt_2023_unseen_sequences_and_labels.rename(columns={'new_labels': 'go_ids'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a pickle file\n",
    "SwissProt_2023_unseen_sequences_and_labels.to_pickle('../data/zero_shot/SwissProt_2023_unseen_sequences_and_labels.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio import SeqIO\n",
    "\n",
    "df = SwissProt_2023_unseen_sequences_and_labels\n",
    "\n",
    "# Convert dataframe to FASTA format and save to a file\n",
    "records = []\n",
    "for _, row in df.iterrows():\n",
    "    seq_record = SeqRecord(Seq(row['sequence']),\n",
    "                           id=row['seq_id'],\n",
    "                           description=\" \".join(row['go_ids']))\n",
    "    records.append(seq_record)\n",
    "\n",
    "# Save to FASTA file\n",
    "fasta_file = \"../data/zero_shot/SwissProt_2023_unseen_sequences_and_labels.fasta\"\n",
    "SeqIO.write(records, fasta_file, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import ProteinDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Paths for the dataset\n",
    "paths = {\n",
    "    \"data_path\": fasta_file,\n",
    "}\n",
    "\n",
    "# Create the dataset\n",
    "protein_dataset = ProteinDataset(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ID 1:  A0A443HJY8\n",
      "Sequence IDs: tensor([89])\n",
      "Sequence onehots: tensor([[[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Label multihots: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Sequence lengths: tensor([433])\n"
     ]
    }
   ],
   "source": [
    "from src.data.datasets import collate_variable_sequence_length\n",
    "\n",
    "# Create the DataLoader\n",
    "batch_size = 1  # You can adjust this value as needed\n",
    "protein_dataloader = DataLoader(protein_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_variable_sequence_length)\n",
    "\n",
    "# Now you can iterate over the DataLoader to get batches of data\n",
    "for batch in protein_dataloader:\n",
    "    # Unpack the batch\n",
    "    sequence_ids, sequence_onehots, label_multihots, sequence_lengths = batch\n",
    "    print(\"Original ID 1: \", protein_dataset.int2sequence_id[sequence_ids[0].item()])\n",
    "    print(f\"Sequence IDs: {sequence_ids}\")\n",
    "    print(f\"Sequence onehots: {sequence_onehots}\")\n",
    "    print(f\"Label multihots: {label_multihots}\")\n",
    "    print(f\"Sequence lengths: {sequence_lengths}\")\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
