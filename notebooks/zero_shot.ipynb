{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Bad pipe message: %s [b\"b\\xc7\\x86(\\x1cX\\xc7'\\xd8\\xeaA$o\\xe2\\xe1e\\x97K V}-\\x87\\xa9q\\x9e\\x8fUr\\xb7t\\xf8\\xe4\\x855\\x89Uy9\\xc5\\x13\\xffiW\\xeb\\xad\\x90dUH\\xbb\\x00\\x08\\x13\\x02\\x13\\x03\\x13\\x01\\x00\\xff\\x01\\x00\\x00\\x8f\"]\n",
      "Bad pipe message: %s [b\"9(\\x85C\\x85F\\xb2|\\xb01\\xe0qg\\xc8\\xb2,'\\xd0\\x00\\x00|\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\\\xc0`\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0#\"]\n",
      "Bad pipe message: %s [b'\\xaf\\xb8\\xfd\\x16\\x16\\xf3u\\r&\\x16c\\x99\\xaa\\rKi\\xa5\\x8d\\x00\\x00\\xa6\\xc0,\\xc00\\x00\\xa3\\x00\\x9f\\xcc\\xa9\\xcc\\xa8\\xcc\\xaa\\xc0\\xaf\\xc0\\xad\\xc0\\xa3\\xc0\\x9f\\xc0]\\xc0a\\xc0W\\xc0S\\xc0+\\xc0/\\x00\\xa2\\x00\\x9e\\xc0\\xae\\xc0\\xac\\xc0\\xa2\\xc0\\x9e\\xc0\\\\']\n",
      "Bad pipe message: %s [b\"\\xc0V\\xc0R\\xc0$\\xc0(\\x00k\\x00j\\xc0s\\xc0w\\x00\\xc4\\x00\\xc3\\xc0#\\xc0'\\x00g\\x00@\\xc0r\\xc0v\\x00\\xbe\\x00\\xbd\\xc0\\n\\xc0\\x14\\x009\\x008\\x00\\x88\\x00\\x87\\xc0\\t\\xc0\\x13\\x003\\x002\\x00\\x9a\\x00\\x99\\x00E\\x00D\\xc0\\x07\\xc0\\x11\\xc0\\x08\\xc0\\x12\\x00\\x16\\x00\\x13\\x00\\x9d\\xc0\\xa1\\xc0\\x9d\\xc0Q\\x00\\x9c\\xc0\\xa0\\xc0\\x9c\\xc0P\\x00=\\x00\\xc0\\x00<\\x00\\xba\\x005\\x00\\x84\\x00/\\x00\\x96\\x00A\\x00\\x05\\x00\\n\\x00\\xff\\x01\\x00\\x00j\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.0.0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x0c\\x00\\n\\x00\\x1d\\x00\\x17\\x00\\x1e\\x00\\x19\\x00\\x18\\x00#\\x00\\x00\\x00\\x16\\x00\\x00\\x00\\x17\\x00\\x00\\x00\\r\\x000\\x00.\\x04\\x03\\x05\\x03\\x06\\x03\\x08\\x07\\x08\\x08\\x08\"]\n",
      "Bad pipe message: %s [b'\\n\\x08\\x0b\\x08\\x04\\x08\\x05\\x08']\n",
      "Bad pipe message: %s [b'\\x01\\x05\\x01\\x06\\x01']\n",
      "Bad pipe message: %s [b'\\x02\\x03', b'\\x02\\x01', b'\\x02\\x02']\n",
      "Bad pipe message: %s [b'\\x05\\x02\\x06']\n",
      "Bad pipe message: %s [b'\\xd3\\x98w\\x84\\xcb3l\\xe3/\\xdb\\xa4\\x17+\\xed\\x03u\\x96\\xcd\\x00\\x00\\xa2\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\x00\\x88\\x00\\x87\\x00\\x86\\x00\\x85\\xc0\\x19\\x00:\\x00\\x89\\xc0\\x0f\\xc0\\x05\\x005\\x00\\x84\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00E\\x00D\\x00C\\x00B\\xc0\\x18\\x004\\x00\\x9b\\x00F']\n",
      "Bad pipe message: %s [b'a\\x1e\\x9a\\x9bq@H\\x00N\\xdc,\\xad\\xdf[<\\x8f\\x10Y\\x00\\x00>\\xc0\\x14\\xc0\\n\\x009\\x008\\x007\\x006\\xc0\\x0f\\xc0\\x05\\x005\\xc0\\x13\\xc0\\t\\x003\\x002\\x001\\x000\\xc0\\x0e\\xc0\\x04\\x00/\\x00\\x9a\\x00\\x99\\x00\\x98\\x00\\x97\\x00\\x96\\x00\\x07\\xc0\\x11\\xc0\\x07\\xc0\\x0c\\xc0\\x02\\x00\\x05\\x00\\x04\\x00\\xff\\x02\\x01\\x00\\x00C\\x00\\x00\\x00\\x0e\\x00\\x0c\\x00\\x00\\t127.']\n",
      "Bad pipe message: %s [b'0.1\\x00\\x0b\\x00\\x04\\x03\\x00\\x01\\x02\\x00\\n\\x00\\x1c\\x00\\x1a\\x00\\x17\\x00\\x19\\x00\\x1c\\x00\\x1b\\x00\\x18\\x00\\x1a\\x00\\x16\\x00\\x0e\\x00\\r\\x00\\x0b\\x00\\x0c\\x00\\t\\x00\\n\\x00#\\x00\\x00']\n",
      "Bad pipe message: %s [b'\\xb7\\x8d\\xb0\\xf2`\\xc2\\x00\\x96\\xa2']\n",
      "Bad pipe message: %s [b'Q\\xc6']\n",
      "Bad pipe message: %s [b'\\x9d\\xfd\\xd7Q\\xe2\\x83\\xd46\\x99\\xbb{\\xaf/\\xf0]\\x87\\xea\\xde\\x00\\x00\\xf4\\xc00\\xc0,\\xc0(\\xc0$\\xc0\\x14\\xc0\\n\\x00\\xa5\\x00\\xa3\\x00\\xa1\\x00\\x9f\\x00k']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import wget\n",
    "\n",
    "curdir = Path(os.getcwd())\n",
    "sys.path.append(str(curdir.parent.absolute()))\n",
    "\n",
    "from src.utils.data import read_fasta\n",
    "\n",
    "link = 'https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz'\n",
    "filename = 'uniprot_sprot.dat.gz'\n",
    "unzipped_filename = 'uniprot_sprot.dat'\n",
    "\n",
    "# Download the file from the web\n",
    "wget.download(link, filename)\n",
    "\n",
    "# Unzip the downloaded file\n",
    "with gzip.open(filename, 'rb') as f_in:\n",
    "    with open(unzipped_filename, 'wb') as f_out:\n",
    "        f_out.write(f_in.read())\n",
    "\n",
    "print(f\"File {filename} has been downloaded and unzipped to {unzipped_filename}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from Bio import SwissProt\n",
    "\n",
    "# Extract data from SwissProt records\n",
    "data = []\n",
    "# See https://biopython.org/docs/1.75/api/Bio.SwissProt.html and https://web.expasy.org/docs/userman.html\n",
    "with open('../data/swissprot/uniprot_sprot.dat', 'r') as f:\n",
    "    records = SwissProt.parse(f)\n",
    "    for record in records:\n",
    "        # Extract sequence ID\n",
    "        seq_id = record.accessions[0]\n",
    "        \n",
    "        # Extract sequence\n",
    "        sequence = record.sequence\n",
    "\n",
    "        # Extract GO ids\n",
    "        go_ids = [ref[1] for ref in record.cross_references if ref[0] == \"GO\" and len(ref) > 0]\n",
    "        \n",
    "        # Extract free-text description\n",
    "        description = record.description\n",
    "\n",
    "        # Extract organism and organism classification\n",
    "        organism = record.organism\n",
    "        organism_classification = record.organism_classification\n",
    "\n",
    "        # Extract organelle\n",
    "        organelle = record.organelle\n",
    "        \n",
    "        # Extract CC line as a dictionary\n",
    "        cc = {}\n",
    "        for comment in record.comments:\n",
    "            key, value = comment.split(\": \", 1)\n",
    "            cc[key] = value\n",
    "        \n",
    "        data.append([seq_id, sequence, go_ids, description, organism, organism_classification, organelle, cc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q6GZX4', 'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL', ['GO:0046782'], 'RecName: Full=Putative transcription factor 001R;', 'Frog virus 3 (isolate Goorha) (FV-3).', ['Viruses', 'Varidnaviria', 'Bamfordvirae', 'Nucleocytoviricota', 'Megaviricetes', 'Pimascovirales', 'Iridoviridae', 'Alphairidovirinae', 'Ranavirus'], '', {'FUNCTION': 'Transcription activation. {ECO:0000305}.'}]\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(data[0])\n",
    "print(len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data into a pandas DataFrame\n",
    "df_2023 = pd.DataFrame(data, columns=[\"seq_id\", \"sequence\", \"go_ids\", \"description\", \"organism\", \"organism_classification\", \"organelle\", \"cc\"])\n",
    "\n",
    "# Create a new column with the subcellular location\n",
    "df_2023['subcellular_location'] = df_2023.cc.apply(lambda x: x['SUBCELLULAR LOCATION'] if 'SUBCELLULAR LOCATION' in x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sequence embeddings from ../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the sequence embeddings from the file\n",
    "with open('../data/embeddings/frozen_proteinfer_sequence_embeddings.pkl', 'rb') as f:\n",
    "    sequence_embeddings = pickle.load(f)\n",
    "\n",
    "# Make a set of the sequence strings\n",
    "sequence_strings_2019 = set(sequence_embeddings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Q6GZX4\n",
      "1    Q6GZX3\n",
      "2    Q197F8\n",
      "3    Q197F7\n",
      "4    Q6GZX2\n",
      "Name: seq_id, dtype: object\n",
      "['Q5RDG8', 'Q027V0', 'B2UBB1', 'F4I893', 'Q6BJH5']\n",
      "Number of sequences in df_2023 but not in ProteInfer dataset: 47493\n",
      "Number of sequences in df_2023: 569793\n",
      "Number of sequences in ProteInfer dataset: 522607\n"
     ]
    }
   ],
   "source": [
    "# Find sequence ids  that are in df but not in sequence_strings\n",
    "df_2023['in_ProteInfer_dataset'] = df_2023.seq_id.apply(lambda x: x in sequence_strings_2019)\n",
    "\n",
    "# Print 5 example sequences from df.sequence\n",
    "print(df_2023.seq_id.head())\n",
    "\n",
    "# Print 5 example sequences from sequence_strings\n",
    "print(list(sequence_strings_2019)[:5])\n",
    "\n",
    "# Count the number of sequences that are in df but not in sequence_strings\n",
    "print(f\"Number of sequences in df_2023 but not in ProteInfer dataset: {df_2023.in_ProteInfer_dataset.value_counts()[False]}\")\n",
    "print(f\"Number of sequences in df_2023: {len(df_2023)}\")\n",
    "print(f\"Number of sequences in ProteInfer dataset: {len(sequence_strings_2019)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import label embeddings from ../data/embeddings/frozen_proteinfer_label_embeddings.pkl\n",
    "import pickle\n",
    "\n",
    "# Load the label embeddings from the file\n",
    "with open('../data/embeddings/frozen_PubMedBERT_label_embeddings.pkl', 'rb') as f:\n",
    "    label_embeddings_2019 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47401\n",
      "29283\n"
     ]
    }
   ],
   "source": [
    "# Make a set of the GO labels from the label embeddings\n",
    "label_ids_2019 = set(label_embeddings_2019.keys())\n",
    "print(len(label_ids_2019))\n",
    "\n",
    "# Make a set from all the GO labels that occur in the data\n",
    "label_ids_2023 = set([item for sublist in df_2023.go_ids for item in sublist])\n",
    "print(len(label_ids_2023))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GO labels in go_label_strings but not in label_strings: 666\n",
      "['GO:0140915', 'GO:0140823', 'GO:0160074', 'GO:0110162', 'GO:0106292', 'GO:0140926', 'GO:0062158', 'GO:0140900', 'GO:0120317', 'GO:0106283']\n"
     ]
    }
   ],
   "source": [
    "# Find GO labels that are in go_label_strings but not in label_strings\n",
    "print(f\"Number of GO labels in go_label_strings but not in label_strings: {len(label_ids_2023 - label_ids_2019)}\")\n",
    "\n",
    "# Print out 10 examples of GO labels that are in go_label_strings but not in label_strings\n",
    "print(list(label_ids_2023 - label_ids_2019)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find added labels\n",
    "new_go_labels = label_ids_2023 - label_ids_2019\n",
    "\n",
    "# Find protein sequences with added labels\n",
    "df_2023['new_labels'] = df_2023.go_ids.apply(lambda x: set(x) & new_go_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with 'in_ProteInfer_dataset' == False: 47493\n",
      "Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): 917\n"
     ]
    }
   ],
   "source": [
    "# Count how many rows have 'in_Proteinfer_dataset' == False\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False: {len(df_2023[df_2023.in_ProteInfer_dataset == False])}\")\n",
    "\n",
    "# Count how many rows have 'in_Proteinfer_dataset' == False and 'new_labels' != set()\n",
    "print(f\"Number of rows with 'in_ProteInfer_dataset' == False and 'new_labels' != set(): {len(df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())])}\")\n",
    "\n",
    "# Create a new dataframe out of those that meet that criteria\n",
    "df_2023_new_sequences_and_labels = df_2023[(df_2023.in_ProteInfer_dataset == False) & (df_2023.new_labels != set())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df_2023_new_sequences_and_labels as a pickle file\n",
    "# Save to here: ../data/zero_shot, with the name \"SwissProt_2023_unseen_sequences_and_labels.pkl\"\n",
    "df_2023_new_sequences_and_labels.to_pickle('../data/zero_shot/SwissProt_2023_unseen_sequences_and_labels.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "protein_functions",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
