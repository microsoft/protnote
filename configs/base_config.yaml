---
# Parameters for model training
#TODO: Optimizer can also be a hyperparameter
params:
  ############## PARAMS THAT NEED TO BE CHANGED FOR AMULET VS. SANDBOX A100 ##############
  ### A100 ###
  # Batch sizes
  TRAIN_BATCH_SIZE: 110 # 256 for A100
  VALIDATION_BATCH_SIZE: 100 # 256 for A100
  TEST_BATCH_SIZE: 100 # 16 for A100
  # # Sampling
  TRAIN_LABEL_SAMPLE_SIZE: null # 15K for both. If training label encoder, must be 100. Set to Null to turn off sampling
  VALIDATION_LABEL_SAMPLE_SIZE: null # 15K for both. Set to Null to turn off sampling
  # # Inference
  LABEL_BATCH_SIZE_LIMIT_NO_GRAD: 1500 # 1500 for A100
  SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD: 128 # 128 for A100 (TO BE VERIFIED)

  ### V100 ###
  # Batch sizes
  #TRAIN_BATCH_SIZE: 64 # 64 for V100
  #VALIDATION_BATCH_SIZE: 64 # 64 for V100
  #TEST_BATCH_SIZE: 4 # 4 for V100
  # Sampling
  #TRAIN_LABEL_SAMPLE_SIZE: 15000 # 15K for both
  #VALIDATION_LABEL_SAMPLE_SIZE: 15000 # 15K for both
  # Inference
  #LABEL_BATCH_SIZE_LIMIT_NO_GRAD: 750 # 1K for V100
  #SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD: 64 # 64 for V100 

  ############## PARAMS THAT DO NOT IMPACT MEMORY ##############
  #@Nate the DIM params do impact memory
  # General
  LEARNING_RATE: 0.0003
  PROTEIN_EMBEDDING_DIM: 1100
  LABEL_EMBEDDING_DIM: 1024
  LATENT_EMBEDDING_DIM: 1024
  OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR: 1 #scale with respect to LATENT_EMBEDDING_DIM
  OUTPUT_MLP_NUM_LAYERS: 2
  NUM_EPOCHS: 50
  GRADIENT_ACCUMULATION_STEPS: 1 # 1 = no gradient accumulation
  CLIP_VALUE: 10 # Gradient clipping
  OUTPUT_NEURON_PROBABILITY_BIAS: null
  OPTIMIZATION_METRIC_NAME: map_micro #only micro metrics are supported if sampling labels in validation
  DECISION_TH_METRIC_NAME: f1_micro
  
  #Losses. Only the parameters for selected loss will be used
  LOSS_FN: FocalLoss # Currently supported: BCE, FocalLoss, BatchWeightedBCE
  FOCAL_LOSS_GAMMA: 1.5
  FOCAL_LOSS_ALPHA: 0.9
  BCE_POS_WEIGHT: 25 # 671.7130737304688
  RGDBCE_TEMP: 0.11 # search from [1,1/3,1/5,1/7,1/9] according to paper

  # Sequence and label encoders
  TRAIN_SEQUENCE_ENCODER: False
  TRAIN_LABEL_ENCODER: False
  TRAIN_PROJECTION_HEAD: True
  LABEL_ENCODER_CHECKPOINT: microsoft/biogpt

  # Data processing and metrics
  DEDUPLICATE: True
  NORMALIZE_PROBABILITIES: False
  OPTIMIZATION_METRIC_NAME: map_micro #only micro metrics are supported if sampling labels in validation
  DECISION_TH_METRIC_NAME: f1_micro

  # Constants
  SEED: 42
  VALIDATIONS_PER_EPOCH: 1
  NUM_WORKERS: 4
  DECISION_TH: null # Set to null if you want to use the best threshold from validation

  # Subset fractions (for rapid prototyping; set to 1 for final model)
  TRAIN_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
  VALIDATION_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
  TEST_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data

# Constants for protein encoder model (e.g. ProteInfer). Not really params since 
# we are not going to change these.
embed_sequences_params:
  INPUT_CHANNELS: 20
  OUTPUT_CHANNELS: 1100
  KERNEL_SIZE: 9
  DILATION_BASE: 3
  NUM_RESNET_BLOCKS: 5
  BOTTLENECK_FACTOR: 0.5
  PROTEINFER_NUM_LABELS: 32102

# Paths to data, vocabularies, embeddings, and models
paths:
  # Paths referenced relative to DATA_PATH (will have DATA_PATH prepended)
  data_paths: 
    # ProteInfer data paths
    TRAIN_DATA_PATH: swissprot/proteinfer_splits/random/train_GO.fasta
    VAL_DATA_PATH: swissprot/proteinfer_splits/random/dev_GO.fasta
    TEST_DATA_PATH: swissprot/proteinfer_splits/random/test_GO.fasta
    FULL_DATA_PATH: swissprot/proteinfer_splits/random/full_GO.fasta # used to generate vocabularies

    # Zero shot data paths
    ZERO_SHOT_DATA_PATH: zero_shot/SwissProt_2023_unseen_sequences_and_labels.fasta

    # Vocabulary paths
    VOCABULARIES_DIR: vocabularies/proteinfer
    GO_ANNOTATIONS_PATH: annotations/go_annotations_2019_07_01.pkl

    # Embeddings paths (if using frozen pre-trained models)
    LABEL_EMBEDDING_PATH: embeddings/frozen_BioGPT_label_embeddings.pkl
    SEQUENCE_EMBEDDING_PATH: embeddings/frozen_proteinfer_sequence_embeddings.pkl

    PARENTHOOD_LIB_PATH: vocabularies/parenthood.json.gz
    PROTEINFER_WEIGHTS_PATH: models/proteinfer/GO_model_weights.pkl
    
  # Paths referenced relative to OUTPUT_PATH (will have OUTPUT_PATH prepended)
  output_paths:
    # Where to save the model
    OUTPUT_MODEL_DIR: checkpoints/

    # Where to save results
    RESULTS_DIR: results/

    # Where to log
    LOG_DIR: logs/