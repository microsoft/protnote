---
# Parameters for model training
# TODO: Optimizer can also be a hyperparameter
params:
  ############## PARAMS THAT NEED TO BE CHANGED FOR AMULET VS. SANDBOX A100 ##############
  ## A100 ###
  # Batch sizes
  # TRAIN_BATCH_SIZE: 4 # 256 for A100
  # VALIDATION_BATCH_SIZE: 4 # 256 for A100
  # TEST_BATCH_SIZE: 4 # 16 for A100
  # # Label Sampling: can only use one of grid sampler, in batch sampling or normal label sampling
  # GRID_SAMPLER: False
  
  # IN_BATCH_SAMPLING: False # Use either in batch sampling or lable samples, not both.
  
  # TRAIN_LABEL_SAMPLE_SIZE: null # 15K for both. If training label encoder, must be 100. Set to Null to turn off sampling
  # VALIDATION_LABEL_SAMPLE_SIZE: null # 15K for both. Set to Null to turn off sampling
  # # Inference
  # LABEL_BATCH_SIZE_LIMIT_NO_GRAD: 1500 # 1500 for A100
  # SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD: 128 # 128 for A100 (TO BE VERIFIED)

  ### V100 ###
  # Batch sizes
  TRAIN_BATCH_SIZE: 4 # 64 for V100
  VALIDATION_BATCH_SIZE: 64 # 64 for V100
  TEST_BATCH_SIZE: 64 # 4 for V100
  # Sampling
  GRID_SAMPLER: False

  IN_BATCH_SAMPLING: False # Use either in batch sampling or lable samples, not both.

  TRAIN_LABEL_SAMPLE_SIZE: null # 15K for both
  VALIDATION_LABEL_SAMPLE_SIZE: null # use all labels in validation
  # Inference
  LABEL_BATCH_SIZE_LIMIT_NO_GRAD: 500 # 1K for V100
  SEQUENCE_BATCH_SIZE_LIMIT_NO_GRAD: 64 # 64 for V100 

  ############## PARAMS DO NOT NEED TO VARY BETWEEN VIRTUAL MACHINES ##############
  # General
  LEARNING_RATE: 0.0003
  OPTIMIZER: Adam
  PROTEIN_EMBEDDING_DIM: 1100
  LABEL_EMBEDDING_DIM: 1024
  LATENT_EMBEDDING_DIM: 1024
  OUTPUT_MLP_HIDDEN_DIM_SCALE_FACTOR: 2 # Scale MLP hidden state with respect to LATENT_EMBEDDING_DIM
  OUTPUT_MLP_NUM_LAYERS: 2
  OUTPUT_NEURON_PROBABILITY_BIAS: null
  OUTPUT_MLP_BATCHNORM: True
  PROJECTION_HEAD_NUM_LAYERS: 2
  PROJECTION_HEAD_HIDDEN_DIM_SCALE_FACTOR: 1
  FEATURE_FUSION: concatenation #concatenation or similarity
  LABEL_EMBEDDING_POOLING_METHOD: 'mean' #select from mean, last_token, all
  OPTIMIZATION_METRIC_NAME: map_micro # Only micro metrics are supported if sampling labels in validation
  DECISION_TH_METRIC_NAME: f1_micro
  NUM_EPOCHS: 50

  # Memory optimizations
  GRADIENT_ACCUMULATION_STEPS: 1 # 1 = no gradient accumulation
  GRADIENT_CHECKPOINTING: False # True = gradient checkpointing
  LORA: False # True = use LORA
  LORA_RANK: 4
  CLIP_VALUE: 1 # Gradient clipping, set to "null" to turn off

  # Losses. Only the parameters for selected loss will be used
  LOSS_FN: FocalLoss # Currently supported: BCE, FocalLoss, BatchWeightedBCE, RGDBCE
  FOCAL_LOSS_GAMMA: 2
  FOCAL_LOSS_ALPHA: -1
  BCE_POS_WEIGHT: 1 # 671.7130737304688
  SUPCON_TEMP: 0.07
  RGDBCE_TEMP: 0.12 # search from [1,1/3,1/5,1/7,1/9] according to paper

  # Sequence and label encoders
  TRAIN_SEQUENCE_ENCODER: False
  LABEL_ENCODER_NUM_TRAINABLE_LAYERS: 0
  DISTRIBUTE_LABELS: False
  TRAIN_PROJECTION_HEAD: True
  LABEL_ENCODER_CHECKPOINT: microsoft/biogpt

  # Data processing and metrics
  DEDUPLICATE: True
  NORMALIZE_PROBABILITIES: False

  # Constants
  SEED: 42
  VALIDATIONS_PER_EPOCH: 1 #TODO: This really is epochs per validation
  NUM_WORKERS: 4
  DECISION_TH: null # Set to null if you want to use the best threshold from validation

  # Subset fractions (for rapid prototyping; set to 1 for final model)
  TRAIN_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
  VALIDATION_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
  TEST_SUBSET_FRACTION: 1 # Set to 1.0 if you want to use all data
  SHUFFLE_LABELS: False # Only has an effect if sample sizes are set and not using grid sampler. If False, simply select same labels in order

# Constants for protein encoder model (e.g. ProteInfer). Not really params since 
# we are not going to change these.
embed_sequences_params:
  INPUT_CHANNELS: 20
  OUTPUT_CHANNELS: 1100
  KERNEL_SIZE: 9
  DILATION_BASE: 3
  NUM_RESNET_BLOCKS: 5
  BOTTLENECK_FACTOR: 0.5
  PROTEINFER_NUM_LABELS: 32102

# Paths to data, vocabularies, embeddings, and models
paths:
  # Paths referenced relative to DATA_PATH (will have DATA_PATH prepended)
  data_paths: 
    # ProteInfer data paths
    TRAIN_DATA_PATH: swissprot/proteinfer_splits/random/train_GO.fasta
    VAL_DATA_PATH: swissprot/proteinfer_splits/random/dev_GO.fasta
    TEST_DATA_PATH: swissprot/proteinfer_splits/random/test_GO.fasta
    FULL_DATA_PATH: swissprot/proteinfer_splits/random/full_GO.fasta # used to generate vocabularies

    # Zero shot data paths
    ZERO_SHOT_DATA_PATH: zero_shot/SwissProt_2023_unseen_sequences_and_labels.fasta

    # Vocabulary paths
    VOCABULARIES_DIR: vocabularies/proteinfer
    GO_ANNOTATIONS_PATH: annotations/go_annotations_2019_07_01.pkl

    # Embeddings paths (if using frozen pre-trained models)
    LABEL_EMBEDDING_PATH: embeddings/frozen_BioGPT_label_embeddings.pkl #this is the base path which is modified by POOLING_METHOD
    SEQUENCE_EMBEDDING_PATH: embeddings/frozen_proteinfer_sequence_embeddings.pkl

    PARENTHOOD_LIB_PATH: vocabularies/parenthood.json.gz
    PROTEINFER_WEIGHTS_PATH: models/proteinfer/GO_model_weights.pkl
    
  # Paths referenced relative to OUTPUT_PATH (will have OUTPUT_PATH prepended)
  output_paths:
    # Where to save the model
    OUTPUT_MODEL_DIR: checkpoints/

    # Where to save results
    RESULTS_DIR: results/

    # Where to log
    LOG_DIR: logs/